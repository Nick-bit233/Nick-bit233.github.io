<!DOCTYPE html>
<html lang='zh-CN'>

<head>
  <meta name="generator" content="Hexo 7.0.0">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.19.0">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://gcore.jsdelivr.net'>
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>LangChain速成文档 - Nickbit's Blog</title>

  
    <meta name="description" content="注意，本篇基于langchain官方文档编写，众所周知LangChain的更新频率很高，因此本文不保证具有时效性 最后更新时间：2024-02-03， LangChain版本0.1.2   LangChain基础：使用链和LLM对话  安装langchain 使用pip： 1234pip install langchainpip install langchain-core # 正常来说，上面一">
<meta property="og:type" content="article">
<meta property="og:title" content="LangChain速成文档">
<meta property="og:url" content="https://nick-bit233.github.io/2024/01/23/LangChain%E9%80%9F%E6%88%90%E6%96%87%E6%A1%A3/index.html">
<meta property="og:site_name" content="Nickbit&#39;s Blog">
<meta property="og:description" content="注意，本篇基于langchain官方文档编写，众所周知LangChain的更新频率很高，因此本文不保证具有时效性 最后更新时间：2024-02-03， LangChain版本0.1.2   LangChain基础：使用链和LLM对话  安装langchain 使用pip： 1234pip install langchainpip install langchain-core # 正常来说，上面一">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-01-23T09:37:30.000Z">
<meta property="article:modified_time" content="2024-02-03T15:11:26.859Z">
<meta property="article:author" content="Nickbit aka. Sciencstine">
<meta name="twitter:card" content="summary">
  
  
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  

  
    <link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
    <script defer src="https://gcore.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
    <script defer src="https://gcore.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"onload="renderMathInElement(document.body);"></script>
  


  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    

  

<header class="header"><div class="logo-wrap"><a class="title" href="/"><div class="main" ff="title">Nickbit's Blog</div><div class="sub normal cap">Yes, this is a subtitle</div><div class="sub hover cap" style="opacity:0"> 没错，这就是副标题</div></a></div>

<nav class="menu dis-select"><a class="nav-item active" href="/">文章</a><a class="nav-item" href="/notes/">笔记</a><a class="nav-item" href="/friends/">友链</a><a class="nav-item" href="/about/">关于</a></nav>
</header>


<div class="widgets">
<widget class="widget-wrapper search"><div class="widget-body"><div class="search-wrapper" id="search"><form class="search-form"><input type="text" class="search-input" id="search-input" data-filter="/blog/" placeholder="文章搜索"><svg t="1670596976048" class="icon search-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2676" width="200" height="200"><path d="M938.2 832.6L723.8 618.1c-2.5-2.5-5.3-4.4-7.9-6.4 36.2-55.6 57.3-121.8 57.3-193.1C773.3 222.8 614.6 64 418.7 64S64 222.8 64 418.6c0 195.9 158.8 354.6 354.6 354.6 71.3 0 137.5-21.2 193.2-57.4 2 2.7 3.9 5.4 6.3 7.8L832.5 938c14.6 14.6 33.7 21.9 52.8 21.9 19.1 0 38.2-7.3 52.8-21.8 29.2-29.1 29.2-76.4 0.1-105.5M418.7 661.3C284.9 661.3 176 552.4 176 418.6 176 284.9 284.9 176 418.7 176c133.8 0 242.6 108.9 242.6 242.7 0 133.7-108.9 242.6-242.6 242.6" p-id="2677"></path></svg></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div></div></widget>


<widget class="widget-wrapper toc single" id="data-toc"><div class="widget-header cap dis-select"><span class="name">LangChain速成文档</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#langchain%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%E9%93%BE%E5%92%8Cllm%E5%AF%B9%E8%AF%9D"><span class="toc-text"> LangChain基础：使用链和LLM对话</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85langchain"><span class="toc-text"> 安装langchain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-text"> 基本使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80llm%E6%9F%A5%E8%AF%A2"><span class="toc-text"> 基础LLM查询</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8prompt%E6%A8%A1%E6%9D%BF"><span class="toc-text"> 使用Prompt模板</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E8%BF%9E%E6%8E%A5%E6%A8%A1%E6%9D%BF-%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%A7%A3%E6%9E%90%E8%BE%93%E5%87%BA"><span class="toc-text"> 使用链式调用连接模板、模型和解析输出</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lcel%E5%92%8C%E5%8F%AF%E8%BF%90%E8%A1%8C%E7%AE%A1%E9%81%93"><span class="toc-text"> LCEL和可运行管道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%96%E9%83%A8%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A"><span class="toc-text"> 外部参数绑定</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%84%E5%8A%A0%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E5%8A%9F%E8%83%BD"><span class="toc-text"> 附加函数调用功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text"> </span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9"><span class="toc-text"> 模型运行时配置修改</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E5%85%A5%E5%AF%B9%E8%AF%9D%E8%AE%B0%E5%BF%86"><span class="toc-text"> 加入对话记忆</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#langchain%E5%AE%9E%E7%8E%B0%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BArag"><span class="toc-text"> LangChain实现检索增强（RAG）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%96%87%E6%A1%A3"><span class="toc-text"> 加载文档</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#csv%E8%A1%A8%E6%A0%BC"><span class="toc-text"> CSV表格</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#html%E7%BD%91%E9%A1%B5"><span class="toc-text"> HTML网页</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#json"><span class="toc-text"> JSON</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#markdown"><span class="toc-text"> Markdown</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pdf"><span class="toc-text"> PDF</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#python%E6%BA%90%E7%A0%81"><span class="toc-text"> Python源码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%8A%A0%E8%BD%BD%E6%96%87%E6%A1%A3"><span class="toc-text"> 批量加载文档</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%89%B2%E6%96%87%E6%A1%A3"><span class="toc-text"> 分割文档</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%89%E7%85%A7%E5%AD%97%E7%AC%A6%E6%8B%86%E5%88%86%E6%96%87%E6%A1%A3"><span class="toc-text"> 按照字符拆分文档</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%89%E7%85%A7%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86"><span class="toc-text"> 按照单词拆分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E8%AF%AD%E4%B9%89%E6%8B%86%E5%88%86"><span class="toc-text"> 根据语义拆分</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E6%96%87%E6%A1%A3%E4%B8%BA%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5"><span class="toc-text"> 编码文档为向量嵌入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%90%91%E9%87%8F%E5%AD%98%E5%82%A8%E5%99%A8"><span class="toc-text"> 创建向量存储器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%90%9C%E7%B4%A2"><span class="toc-text"> 相似性搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E7%BC%93%E5%AD%98"><span class="toc-text"> 嵌入缓存</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2%E5%99%A8"><span class="toc-text"> 检索器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A3%80%E7%B4%A2%E5%99%A8-%E5%9F%BA%E4%BA%8E%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text"> 基础检索器 - 基于向量数据库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E6%9F%A5%E8%AF%A2%E6%A3%80%E7%B4%A2%E5%99%A8-multiqueryretriever"><span class="toc-text"> 多查询检索器 MultiQueryRetriever</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8E%8B%E7%BC%A9%E6%A3%80%E7%B4%A2%E5%99%A8"><span class="toc-text"> 上下文压缩检索器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E5%99%A8-multivector-retriever"><span class="toc-text"> 多向量检索器 MultiVector Retriever</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%A6%E6%9C%89%E6%97%B6%E9%97%B4%E5%8A%A0%E6%9D%83%E7%9A%84%E6%A3%80%E7%B4%A2%E5%99%A8"><span class="toc-text"> 带有时间加权的检索器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E6%A3%80%E7%B4%A2%E5%99%A8%E5%8C%85%E8%A3%85%E4%B8%BAllm-chain"><span class="toc-text"> 将检索器包装为LLM Chain</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E6%A3%80%E7%B4%A2%E5%99%A8%E5%8C%85%E8%A3%85%E4%B8%BA%E6%99%BA%E8%83%BD%E4%BD%93%E5%B7%A5%E5%85%B7"><span class="toc-text"> 将检索器包装为智能体工具</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#langchain-agent"><span class="toc-text"> LangChain Agent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-text"> 核心概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E9%93%BE%E5%9F%BA%E7%B1%BBagent"><span class="toc-text"> 智能体链基类：Agent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E6%89%A7%E8%A1%8C%E5%99%A8%E7%B1%BBagentexecutor"><span class="toc-text"> 智能体执行器类：AgentExecutor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E7%BD%AE%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text"> 预置智能体的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%9C%9F%E7%B1%BB%E5%9E%8B%E6%98%AFchat-models%E7%9A%84%E5%86%85%E7%BD%AE%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-text"> 预期类型是Chat Models的内置智能体</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%9C%9F%E7%B1%BB%E5%9E%8B%E6%98%AF%E7%BA%AFllm%E7%9A%84%E5%86%85%E7%BD%AE%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-text"> 预期类型是纯LLM的内置智能体</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7%E7%B1%BBtool"><span class="toc-text"> 工具类：Tool</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E5%AE%9E%E7%8E%B0"><span class="toc-text"> 工具的使用和实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#langchain%E5%86%85%E7%BD%AE%E5%B7%A5%E5%85%B7"><span class="toc-text"> Langchain内置工具</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B7%A5%E5%85%B7"><span class="toc-text"> 自定义工具</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86"><span class="toc-text"> 错误处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8openai%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%B7%A5%E5%85%B7"><span class="toc-text"> 在OPENAI聊天模型中使用工具</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-text"> 自定义智能体</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#agentexecutor-%E7%9A%84%E9%A2%9D%E5%A4%96%E5%8F%82%E6%95%B0"><span class="toc-text"> AgentExecutor 的额外参数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E6%89%A7%E8%A1%8C%E7%9A%84%E7%9B%91%E7%9D%A3%E5%92%8C%E6%A3%80%E6%9F%A5"><span class="toc-text"> 智能体执行的监督和检查</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%89%E6%AD%A5%E9%AA%A4%E6%89%A7%E8%A1%8C"><span class="toc-text"> 按步骤执行</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%BE%97%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E7%9A%84%E6%97%A5%E5%BF%97"><span class="toc-text"> 获得中间步骤的日志</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%88%91%E6%A3%80%E6%9F%A5%E9%94%99%E8%AF%AF"><span class="toc-text"> 自我检查错误</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E%E7%BB%93%E6%9E%84%E5%8C%96%E8%BE%93%E5%87%BA"><span class="toc-text"> 返回结构化输出</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7%E5%8C%85toolkits"><span class="toc-text"> 工具包：Toolkits</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#langchain%E5%86%85%E7%BD%AE%E5%B7%A5%E5%85%B7%E5%8C%85"><span class="toc-text"> Langchain内置工具包</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cot-tot-%E5%92%8C-%E6%80%9D%E7%BB%B4%E5%9B%BE%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text"> COT TOT 和 思维图的实现</span></a></li></ol></li></ol></div></div></widget>




</div>


    </aside>
    <div class='l_main'>
      

      



<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a></div><div id="post-meta">发布于&nbsp;<time datetime="2024-01-23T09:37:30.000Z">2024-01-23</time></div></div>

<article class='md-text content post'>
<h1 class="article-title"><span>LangChain速成文档</span></h1>
<blockquote>
<p>注意，本篇基于langchain官方文档编写，众所周知LangChain的更新频率很高，因此本文不保证具有时效性</p>
<p>最后更新时间：2024-02-03， LangChain版本0.1.2</p>
</blockquote>
<h2 id="langchain基础使用链和llm对话"><a class="markdownIt-Anchor" href="#langchain基础使用链和llm对话"></a> LangChain基础：使用链和LLM对话</h2>
<h3 id="安装langchain"><a class="markdownIt-Anchor" href="#安装langchain"></a> 安装langchain</h3>
<p>使用pip：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install langchain</span><br><span class="line">pip install langchain-core <span class="comment"># 正常来说，上面一行命令也会同时安装langchain-core</span></span><br><span class="line">pip install langchain-community <span class="comment"># 安装第三方贡献的langchain库</span></span><br><span class="line">pip install langchain-openai <span class="comment"># 使用OPENAI的模型和api需要的集成包</span></span><br></pre></td></tr></table></figure>
<p>如果你使用OPENAI 的 api，可能需要按照下面的方法配置api：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;配置部分：采用环境变量，但所有变量在实例化类对象时均可以传入参数的形式覆盖&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># API_KEY</span></span><br><span class="line">api_key = <span class="string">&quot;sk-xxx&quot;</span> </span><br><span class="line"><span class="comment"># 默认调用模型名称</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-3.5-turbo&quot;</span></span><br><span class="line"><span class="comment"># 默认调用模型的最大token数</span></span><br><span class="line">max_tokens = <span class="number">4096</span></span><br><span class="line"><span class="comment"># 默认调用模型的温度</span></span><br><span class="line">temperature = <span class="number">0</span></span><br><span class="line"><span class="comment"># 其他配置</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">如果想把 OPENAI API 的请求根路由修改成自己或第三方的代理地址，可以通过设置环境变量 “OPENAI_API_BASE” 来进行修改。</span></span><br><span class="line"><span class="string">12/14备注：实测openai的python库无法正确使用http规则代理（我不知道为什么，也许要去看源码），</span></span><br><span class="line"><span class="string">          如果使用官方接口，建议开启全局代理，然后不再设置代理环境变量</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># set openai api key</span></span><br><span class="line">os.environ[<span class="string">&quot;OPENAI_API_KEY&quot;</span>] = api_key</span><br><span class="line"><span class="comment"># set openai proxy</span></span><br><span class="line"><span class="comment"># os.environ[&quot;OPENAI_PROXY&quot;] = &quot;http://127.0.0.1:7890&quot;</span></span><br><span class="line"><span class="comment"># set openai base url</span></span><br><span class="line">os.environ[<span class="string">&quot;OPENAI_API_BASE&quot;</span>] = <span class="string">&quot;https://aigptx.top/v1&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="基本使用"><a class="markdownIt-Anchor" href="#基本使用"></a> 基本使用</h3>
<h4 id="基础llm查询"><a class="markdownIt-Anchor" href="#基础llm查询"></a> 基础LLM查询</h4>
<p>LangChain包括两种使用语言模型的方法：</p>
<ul>
<li>LangChain LLMs 类（接收字符串、输出字符串）</li>
<li>LangChain 聊天模型（接收消息、输出消息，其中消息是一个langChain定义的类），类名通常以Chat开头</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="comment"># 导入消息类型</span></span><br><span class="line"><span class="keyword">from</span> langchain.schema <span class="keyword">import</span> (</span><br><span class="line">    AIMessage,</span><br><span class="line">    HumanMessage,</span><br><span class="line">    SystemMessage</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#####</span></span><br><span class="line"><span class="comment"># 直接invoke llm 对象</span></span><br><span class="line">llm = OpenAI(model_name=model_name)</span><br><span class="line">result = llm.invoke(<span class="string">&quot;怎么评价人工智能&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment">#####</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#####</span></span><br><span class="line">chat = ChatOpenAI(model_name=model_name)  <span class="comment"># 实例化一个chat对象</span></span><br><span class="line"><span class="comment"># 消息类型：AIMessage, HumanMessage, SystemMessage（分别对应AI回复，用户输入，系统提示）</span></span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=<span class="string">&quot;You are a helpful assistant that translates English to French.&quot;</span>),</span><br><span class="line">    HumanMessage(content=<span class="string">&quot;Translate this sentence from English to French. I love programming.&quot;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># invoke chat 对象，返回“AIMessage”类型的对象</span></span><br><span class="line">result = chat.invoke(messages)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(result))  <span class="comment"># -&gt; AIMessage</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="comment">#####</span></span><br></pre></td></tr></table></figure>
<h4 id="使用prompt模板"><a class="markdownIt-Anchor" href="#使用prompt模板"></a> 使用Prompt模板</h4>
<p>帮助你通过简单输入构建提示模板：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> chat_prompt:</span><br><span class="line">    chat_prompt = ChatPromptTemplate.from_template(<span class="string">&quot;What is a good name for a company that makes &#123;product&#125;?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(chat_prompt.<span class="built_in">format</span>(product=<span class="string">&quot;colorful socks&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> chat_prompt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 PromptTemplate 类来创建提示模板</span></span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;product&quot;</span>],  <span class="comment"># 这是一个可以修改的参数，会被填入下面的template字符串中</span></span><br><span class="line">    template=<span class="string">&quot;What is a good name for a company that makes &#123;product&#125;?&quot;</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 类似string.format()的用法，传入参数填充模板中的变量</span></span><br><span class="line"><span class="built_in">print</span>(prompt.<span class="built_in">format</span>(product=<span class="string">&quot;colorful socks&quot;</span>))</span><br><span class="line"><span class="keyword">return</span> prompt</span><br></pre></td></tr></table></figure>
<h4 id="使用链式调用连接模板-模型和解析输出"><a class="markdownIt-Anchor" href="#使用链式调用连接模板-模型和解析输出"></a> 使用链式调用连接模板、模型和解析输出</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">basic_chain_usage_test</span>(<span class="params">use_chain_pipe=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 基本结构：prompt + model + output parser</span></span><br><span class="line">    prompt = ChatPromptTemplate.from_template(</span><br><span class="line">        <span class="string">&quot;tell me a short joke about &#123;topic&#125;, use &#123;language&#125;&quot;</span></span><br><span class="line">    )</span><br><span class="line">    model = ChatOpenAI(model_name=model_name, temperature=<span class="number">0.9</span>)</span><br><span class="line">    output_parser = StrOutputParser()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> use_chain_pipe:</span><br><span class="line">        <span class="comment"># langchain的各种类型都具有invoke函数</span></span><br><span class="line">        <span class="comment"># prompt类型的invoke函数会将输入的参数填充到prompt模板中，返回一个带有message后的prompt对象</span></span><br><span class="line">        prompt_value = prompt.invoke(&#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;ice cream&quot;</span>,</span><br><span class="line">                                      <span class="string">&quot;language&quot;</span>: <span class="string">&quot;Chinese&quot;</span>&#125;)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;prompt: <span class="subst">&#123;prompt_value&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># model类型的invoke函数会将prompt value传递给已实例化的model对象，这里model是一个LLM聊天模型，返回一个message对象</span></span><br><span class="line">        message = model.invoke(prompt_value)</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">type</span>(message))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;message: <span class="subst">&#123;message&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output parser类型的invoke函数会处理message对象中返回的信息。这里的StrOutputParser是一个基础输出器，将结果转换为字符串</span></span><br><span class="line">        result = output_parser.invoke(message.content)</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">type</span>(result))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;output: <span class="subst">&#123;result&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># piece together then different components into a single chain using LCEL</span></span><br><span class="line">    <span class="comment"># use | (unix pipe operator) as the operator to connect the components</span></span><br><span class="line">    <span class="comment"># 使用 | 作为连接符号，将不同的组件连接起来，以形成一个链式管道</span></span><br><span class="line">    chain = prompt | model | output_parser</span><br><span class="line"></span><br><span class="line">    result = chain.invoke(&#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;pigeons&quot;</span>,</span><br><span class="line">                           <span class="string">&quot;language&quot;</span>: <span class="string">&quot;Chinese&quot;</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;reply: <span class="subst">&#123;result&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="lcel和可运行管道"><a class="markdownIt-Anchor" href="#lcel和可运行管道"></a> LCEL和可运行管道</h3>
<p>上述例子中，类似<code>chain = prompt | model | output_parser</code>的语法糖被称为LCEL（<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/expression_language/">LangChain Expression Language</a>）。<br />
每个支持LCEL语法糖的对象，底层都被抽象为了一个“可运行”类：<code>Runnable</code>，多个可运行对象的输入输出相连接，组成“可运行管道”，即<code>RunnableParallel</code>。它允许接受来自<code>invoke</code>函数得到的输入（通常是一个字典类型），然后根据其上子类的实现不同，处理数据并得到输出。<br />
记住下面的几点：</p>
<ul>
<li>使用<code>RunnableParallel()</code>可以实例化一个可运行管道对象，该对象存储的数据结构被视为一个字典，可以通过参数赋值来设置字典的键值对，或是直接传入一个字典。
<ul>
<li>使用<code>RunnablePassthrough()</code>函数可以指代<code>invoke</code>函数的输入，返回类型是字典</li>
<li>使用<code>RunnablePassthrough.assign()</code>函数可以修改<code>invoke</code>函数的输入的字典，包括添加新的键值对，返回类型是字典</li>
<li>使用任何<strong>lambda表达式</strong>可以<strong>直接处理</strong>invoke函数的输入，返回的则是lambad表达式的结果</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableParallel, RunnablePassthrough</span><br><span class="line"></span><br><span class="line">runnable = RunnableParallel(</span><br><span class="line">    passed=RunnablePassthrough(),</span><br><span class="line">    extra=RunnablePassthrough.assign(mult=<span class="keyword">lambda</span> x: x[<span class="string">&quot;num&quot;</span>] * <span class="number">3</span>),</span><br><span class="line">    modified=<span class="keyword">lambda</span> x: x[<span class="string">&quot;num&quot;</span>] + <span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">runnable.invoke(&#123;<span class="string">&quot;num&quot;</span>: <span class="number">1</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&#x27;passed&#x27;</span>: &#123;<span class="string">&#x27;num&#x27;</span>: <span class="number">1</span>&#125;, </span><br><span class="line">    <span class="string">&#x27;extra&#x27;</span>: &#123;<span class="string">&#x27;num&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;mult&#x27;</span>: <span class="number">3</span>&#125;, </span><br><span class="line">    <span class="string">&#x27;modified&#x27;</span>: <span class="number">2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>在LCEL的链式语法中，如果两个对象之间以<code>|</code>连接了，实例化<code>RunnableParallel()</code>的步骤可以直接简写为定义一个字典：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">retrieval_chain = (</span><br><span class="line">    <span class="comment"># 下面的三种写法等价：</span></span><br><span class="line">    <span class="comment"># RunnableParallel(&#123;&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()&#125;)</span></span><br><span class="line">    <span class="comment"># RunnableParallel(context=retriever, question=RunnablePassthrough())</span></span><br><span class="line">    &#123;<span class="string">&quot;context&quot;</span>: retriever, <span class="string">&quot;question&quot;</span>: RunnablePassthrough()&#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | model</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>可以使用<code>itemgetter</code>函数快速检索一个特定的输入键值（需要线导入名称）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"></span><br><span class="line">chain = (</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;context&quot;</span>: itemgetter(<span class="string">&quot;question&quot;</span>) | retriever,</span><br><span class="line">        <span class="string">&quot;question&quot;</span>: itemgetter(<span class="string">&quot;question&quot;</span>),</span><br><span class="line">        <span class="string">&quot;language&quot;</span>: itemgetter(<span class="string">&quot;language&quot;</span>),</span><br><span class="line">    &#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | model</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain.invoke(&#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;where did harrison work&quot;</span>, <span class="string">&quot;language&quot;</span>: <span class="string">&quot;italian&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>可以使用<code>RunnableLambda()</code>函数，允许调用任何已定义的外部函数来处理输入数据，并返回你自定义的结果。但是，这些函数都必须有且只有一个类型为字典的参数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multiple_length_function</span>(<span class="params">_<span class="built_in">dict</span></span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(_<span class="built_in">dict</span>[<span class="string">&quot;text1&quot;</span>]) * <span class="built_in">len</span>(_<span class="built_in">dict</span>[<span class="string">&quot;text2&quot;</span>])</span><br><span class="line">    </span><br><span class="line">chain = (</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;x&quot;</span>: &#123;<span class="string">&quot;text1&quot;</span>: itemgetter(<span class="string">&quot;foo&quot;</span>), <span class="string">&quot;text2&quot;</span>: itemgetter(<span class="string">&quot;bar&quot;</span>)&#125;</span><br><span class="line">        | RunnableLambda(multiple_length_function),</span><br><span class="line">    &#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | model</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain.invoke(&#123;<span class="string">&quot;foo&quot;</span>: <span class="string">&quot;abc&quot;</span>, <span class="string">&quot;bar&quot;</span>: <span class="string">&quot;gah&quot;</span>&#125;)</span><br><span class="line"><span class="comment"># x的内容是9 (3*3=9)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>可运行管道组成的链是可以随意嵌套的</strong>，你可以将其视为一个带有特定功能的字典，允许多层套娃：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">prompt1 = ChatPromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;generate a &#123;attribute&#125; color. Return the name of the color and nothing else:&quot;</span></span><br><span class="line">)</span><br><span class="line">prompt2 = ChatPromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;what is a fruit of color: &#123;color&#125;. Return the name of the fruit and nothing else:&quot;</span></span><br><span class="line">)</span><br><span class="line">prompt3 = ChatPromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;what is a country with a flag that has the color: &#123;color&#125;. Return the name of the country and nothing else:&quot;</span></span><br><span class="line">)</span><br><span class="line">prompt4 = ChatPromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;What is the color of &#123;fruit&#125; and the flag of &#123;country&#125;?&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">color_generator = (</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;attribute&quot;</span>: RunnablePassthrough()&#125; </span><br><span class="line">    	| prompt1 </span><br><span class="line">    	| &#123;</span><br><span class="line">            <span class="string">&quot;color&quot;</span>: model | StrOutputParser()</span><br><span class="line">        &#125;</span><br><span class="line">)</span><br><span class="line">color_to_fruit = prompt2 | model | StrOutputParser()</span><br><span class="line">color_to_country = prompt3 | model | StrOutputParser()</span><br><span class="line">question_generator = (</span><br><span class="line">    color_generator </span><br><span class="line">    | &#123;</span><br><span class="line">        <span class="string">&quot;fruit&quot;</span>: color_to_fruit, </span><br><span class="line">        <span class="string">&quot;country&quot;</span>: color_to_country</span><br><span class="line">    &#125; </span><br><span class="line">    | prompt4</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="外部参数绑定"><a class="markdownIt-Anchor" href="#外部参数绑定"></a> 外部参数绑定</h3>
<p><code>Runnable.bind()</code>的函数允许你在外部添加可运行管道对象的参数。这里我们以为模型对象绑定参数为例。<br />
最常用的功能是，为模型绑定<code>stop</code>参数，可以指定LangChain在匹配到到LLM的某些输出后终止LLM的后续输出，直接返回已输出的内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (</span><br><span class="line">            <span class="string">&quot;system&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Write out the following equation using algebraic symbols then solve it. Use the format\n\nEQUATION:...\nSOLUTION:...\n\n&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;equation_statement&#125;&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">runnable = (</span><br><span class="line">    &#123;<span class="string">&quot;equation_statement&quot;</span>: RunnablePassthrough()&#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | model.bind(stop=<span class="string">&quot;SOLUTION&quot;</span>)</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(runnable.invoke(<span class="string">&quot;x raised to the third plus seven equals 12&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">### 不加bind(stop=&quot;SOLUTION&quot;)的结果：</span></span><br><span class="line">EQUATION: x^<span class="number">3</span> + <span class="number">7</span> = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">SOLUTION:</span><br><span class="line">Subtracting <span class="number">7</span> <span class="keyword">from</span> both sides of the equation, we get:</span><br><span class="line">x^<span class="number">3</span> = <span class="number">12</span> - <span class="number">7</span></span><br><span class="line">x^<span class="number">3</span> = <span class="number">5</span></span><br><span class="line"><span class="comment">### 加上bind(stop=&quot;SOLUTION&quot;)的结果：</span></span><br><span class="line">EQUATION: x^<span class="number">3</span> + <span class="number">7</span> = <span class="number">12</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="附加函数调用功能"><a class="markdownIt-Anchor" href="#附加函数调用功能"></a> 附加函数调用功能</h4>
<p>如果你使用的是OPENAI等支持函数调用的模型，你还可以定义函数（这要求LLM以固定的格式输出结果），并使用<code>bind</code>函数将其绑定到LLM对象或聊天模型对象中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain.output_parsers.openai_functions <span class="keyword">import</span> JsonOutputFunctionsParser</span><br><span class="line"><span class="keyword">from</span> langchain.output_parsers.openai_functions <span class="keyword">import</span> JsonKeyOutputFunctionsParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableParallel, RunnablePassthrough</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prompt_chain_with_input_and_output_handle</span>():</span><br><span class="line"></span><br><span class="line">    prompt = ChatPromptTemplate.from_template(<span class="string">&quot;tell me a joke about &#123;foo&#125;&quot;</span>)</span><br><span class="line">    model = ChatOpenAI(model_name=model_name, temperature=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为model定义函数调用，这回要求model返回一个符合该function定义的，json格式的结果</span></span><br><span class="line">    functions = [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;joke&quot;</span>,</span><br><span class="line">            <span class="string">&quot;description&quot;</span>: <span class="string">&quot;A joke&quot;</span>,</span><br><span class="line">            <span class="string">&quot;parameters&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">                <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;setup&quot;</span>: &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>, <span class="string">&quot;description&quot;</span>: <span class="string">&quot;The setup for the joke&quot;</span>&#125;,</span><br><span class="line">                    <span class="string">&quot;punchline&quot;</span>: &#123;</span><br><span class="line">                        <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;The punchline for the joke&quot;</span>,</span><br><span class="line">                    &#125;,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;required&quot;</span>: [<span class="string">&quot;setup&quot;</span>, <span class="string">&quot;punchline&quot;</span>],</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 这个管道的定义使得当用户输入&quot;ears&quot;时，map_对象中实际得到的数据是&#123;&quot;foo&quot;: &quot;ears&quot;&#125;</span></span><br><span class="line">    map_ = RunnableParallel(foo=RunnablePassthrough())</span><br><span class="line">    <span class="comment"># chain = (</span></span><br><span class="line">    <span class="comment">#         map_</span></span><br><span class="line">    <span class="comment">#         | prompt</span></span><br><span class="line">    <span class="comment">#         | model.bind(function_call=&#123;&quot;name&quot;: &quot;joke&quot;&#125;, functions=functions)</span></span><br><span class="line">    <span class="comment">#         | JsonKeyOutputFunctionsParser(key_name=&quot;setup&quot;)  # 使用Parser从json中提取想要输出key（如：setup）字段</span></span><br><span class="line">    <span class="comment"># )</span></span><br><span class="line">    chain = (</span><br><span class="line">            map_</span><br><span class="line">            | prompt</span><br><span class="line">            | model.bind(function_call=&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;joke&quot;</span>&#125;, functions=functions)  <span class="comment"># 绑定函数和指定函数调用</span></span><br><span class="line">            | JsonOutputFunctionsParser()  <span class="comment"># 使用Parser提取整个json</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    result = chain.invoke(<span class="string">&quot;ears&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<h4 id=""><a class="markdownIt-Anchor" href="#"></a> </h4>
<h3 id="模型运行时配置修改"><a class="markdownIt-Anchor" href="#模型运行时配置修改"></a> 模型运行时配置修改</h3>
<p>如果你不想在某些模型运行时使用全局的语言模型配置参数（模型名称，温度，最大token限制等），可以使用以下方法：<br />
首先，在实例化model时指定可以覆盖修改的参数，使用<code>ConfigurableField</code>，其中id字段要与你使用的模型后端匹配，而名称和描述可以自定义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> ConfigurableField</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(temperature=<span class="number">0</span>).configurable_fields(</span><br><span class="line">    temperature=ConfigurableField(</span><br><span class="line">        <span class="built_in">id</span>=<span class="string">&quot;llm_temperature&quot;</span>,</span><br><span class="line">        name=<span class="string">&quot;LLM Temperature&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;The temperature of the LLM&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>使用模型时（包括在LCEL中使用时），用函数即可覆盖配置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.with_config(</span><br><span class="line">    configurable=&#123;</span><br><span class="line">        <span class="string">&quot;llm_temperature&quot;</span>: <span class="number">0.9</span></span><br><span class="line">    &#125;</span><br><span class="line">).invoke(<span class="string">&quot;pick a random number&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>还可以使用<code>configurable_alternatives</code>指定一套可替代的配置方案，详细请参考：<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/expression_language/how_to/configure#configurable-alternatives">Configure chain internals at runtime | 🦜️🔗 Langchain</a></p>
<h3 id="加入对话记忆"><a class="markdownIt-Anchor" href="#加入对话记忆"></a> 加入对话记忆</h3>
<p>如果需要进行多轮对话，则有必要将每次对话的内容记录下来，并在下一次提示时提供历史消息。<br />
可以通过在定义prompt时添加关于历史的<code>MessagesPlaceholder</code>占位符来实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_memory_chain</span>():</span><br><span class="line">    <span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line">    <span class="keyword">from</span> langchain.memory <span class="keyword">import</span> ConversationBufferMemory</span><br><span class="line">    <span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder</span><br><span class="line">    <span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableLambda, RunnablePassthrough</span><br><span class="line"></span><br><span class="line">    model = ChatOpenAI(model_name=model_name)  <span class="comment"># 使用聊天LLM模型和聊天提示模板</span></span><br><span class="line">    prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">        [</span><br><span class="line">            (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;You are a helpful chatbot&quot;</span>),  <span class="comment"># 这是系统提示</span></span><br><span class="line">            MessagesPlaceholder(variable_name=<span class="string">&quot;history&quot;</span>),  <span class="comment"># 这是一个占位符，用于将对话历史填充到模板中</span></span><br><span class="line">            (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>),  <span class="comment"># 这是用户输入</span></span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    memory = ConversationBufferMemory(return_messages=<span class="literal">True</span>)  <span class="comment"># 添加一个ConversationBufferMemory对象，用于存储对话历史</span></span><br><span class="line">    mem = memory.load_memory_variables(&#123;&#125;)  <span class="comment"># 初始化memory，返回值为这样的字典：&#123;&quot;history&quot;: []&#125;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;init memory: <span class="subst">&#123;mem&#125;</span>&quot;</span>)</span><br><span class="line">    chain = (</span><br><span class="line">            RunnablePassthrough.assign(</span><br><span class="line">                history=RunnableLambda(memory.load_memory_variables) | itemgetter(<span class="string">&quot;history&quot;</span>)</span><br><span class="line">            )  <span class="comment"># 从memory中加载键值为&quot;history&quot;的变量，然后将其通过管道输入给下一步的prompt</span></span><br><span class="line">            | prompt</span><br><span class="line">            | model</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    inputs = &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;hi im bob&quot;</span>&#125;</span><br><span class="line">    response = chain.invoke(inputs)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;first response: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    memory.save_context(inputs, &#123;<span class="string">&quot;output&quot;</span>: response.content&#125;)  <span class="comment"># 一次chain invoke后，需要更新memory中的context</span></span><br><span class="line">    mem = memory.load_memory_variables(&#123;&#125;)  <span class="comment"># 重新加载memory，返回值中会有已经存储的对话历史</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;memory after first invoke: <span class="subst">&#123;mem&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 再次进行对话，这次会将对话历史填充到prompt中</span></span><br><span class="line">    inputs = &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;can you tell what is my name?&quot;</span>&#125;</span><br><span class="line">    response = chain.invoke(inputs)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;second response: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>记住，<code>MessagesPlaceholder</code>占位符不止用于记忆历史，可以记录任何外部内容，当你需要从外部添加信息时也可以使用。</p>
<h2 id="langchain实现检索增强rag"><a class="markdownIt-Anchor" href="#langchain实现检索增强rag"></a> LangChain实现检索增强（RAG）</h2>
<p>检索增强的含义是，LLM可能不知道用户需要的特定数据，因此需要一个程序检索外部数据，然后在执行生成步骤时将其传递给LLM。<br />
检索增强的步骤通常是将外部源数据（通常是各种文档）进行加载、拆分处理，然后转化为向量（被称为“嵌入”），这将保存数据中的语义信息，去除干扰，以方便语言模型理解，最后存储在一种介质中（如向量数据库），当模型需要时，通过一种方法从介质中“检索”数据。</p>
<h3 id="加载文档"><a class="markdownIt-Anchor" href="#加载文档"></a> 加载文档</h3>
<p>LangChain支持各种文档格式的读取，如果你的数据只是简单的文本格式（如markdown和txt），可以直接使用<code>TextLoader</code>获得一份存储在内存的文档对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"></span><br><span class="line">loader = TextLoader(<span class="string">&quot;./index.md&quot;</span>)</span><br><span class="line">loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文档对象的数据结构示例：</span></span><br><span class="line">[</span><br><span class="line">    Document(</span><br><span class="line">        page_content=<span class="string">&#x27;---\nsidebar_position: 0\n---\n# Document loader ... &#x27;</span>,  </span><br><span class="line">        metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;../docs/docs/modules/data_connection/document_loaders/index.md&#x27;</span>&#125;</span><br><span class="line">    )</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 每个文档包含两个对象，page_content是文本形式存储的内容，metadata包含文本的元数据，比如存储路径source</span></span><br></pre></td></tr></table></figure>
<p>关于所有支持的文档格式和相关库，参考：<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/document_loaders/">https://python.langchain.com/docs/integrations/document_loaders/</a><br />
下面我们会介绍其他针对常用格式的Loader：</p>
<h4 id="csv表格"><a class="markdownIt-Anchor" href="#csv表格"></a> CSV表格</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders.csv_loader <span class="keyword">import</span> CSVLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同的CSV文档可能有不同的分隔符，读取时可以指定，并且可以指定表头的名称</span></span><br><span class="line">loader = CSVLoader(file_path=<span class="string">&#x27;./example_data/mlb_teams_2012.csv&#x27;</span>, csv_args=&#123;</span><br><span class="line">    <span class="string">&#x27;delimiter&#x27;</span>: <span class="string">&#x27;,&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;quotechar&#x27;</span>: <span class="string">&#x27;&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;fieldnames&#x27;</span>: [<span class="string">&#x27;MLB Team&#x27;</span>, <span class="string">&#x27;Payroll in millions&#x27;</span>, <span class="string">&#x27;Wins&#x27;</span>]</span><br><span class="line">&#125;)</span><br><span class="line">data = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取CSV文件时，可以指定source_column参数将文本的元数据从默认的“文件路径”转化为每一列的表头名称</span></span><br><span class="line"><span class="comment"># 在文本-问答任务中这种处理也许有用。</span></span><br><span class="line">loader = CSVLoader(file_path=<span class="string">&#x27;./example_data/mlb_teams_2012.csv&#x27;</span>, source_column=<span class="string">&quot;Team&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="html网页"><a class="markdownIt-Anchor" href="#html网页"></a> HTML网页</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredHTMLLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种方法是使用UnstructuredHTMLLoader，这将直接提取html中的纯文本内容到page_content中，而忽略其他的所有内容。</span></span><br><span class="line">loader = UnstructuredHTMLLoader(<span class="string">&quot;example_data/fake-content.html&quot;</span>)</span><br><span class="line">data = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另一种方法是使用BeautifulSoup4</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> BSHTMLLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这将使得html中的纯文本内容被提取到page_content中，而网页标题（如果有）被写入元数据中的&#x27;title&#x27;字段</span></span><br><span class="line">loader = BSHTMLLoader(<span class="string">&quot;example_data/fake-content.html&quot;</span>)</span><br><span class="line">data = loader.load()</span><br><span class="line">data</span><br></pre></td></tr></table></figure>
<h4 id="json"><a class="markdownIt-Anchor" href="#json"></a> JSON</h4>
<p><code>JSONLoader</code>类定义了加载JSON格式文件的方法。LangChain允许你从一个json文件中读取出多个不同的文档，并且可以指定读取哪些键对应的值的内容。这是通过一个名为<code>jq</code>的python库实现的。<br />
在实例化<code>JSONLoader</code>的时候，可以传入<code>jq_schema</code>参数，其中指定了要从json文件的哪个键中提取对象。<code>jq_schema</code>的语法与lua字典类似，直接使用<code>.</code>可以访问子级键值对，如果对象是一个列表，使用<code>[]</code>可以提取整个列表中的每个对象，这会让<code>JSONLoader</code>返回多个<code>Document</code>对象组成的列表，其在列表中的索引序号将被写入元数据中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">loader = JSONLoader(</span><br><span class="line">    file_path=<span class="string">&#x27;./example_data/facebook_chat.json&#x27;</span>,</span><br><span class="line">    jq_schema=<span class="string">&#x27;.messages[].content&#x27;</span>,</span><br><span class="line">    text_content=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">data = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：如果json长这这样：</span></span><br><span class="line">    &#123;</span><br><span class="line">     <span class="string">&#x27;joinable_mode&#x27;</span>: &#123;<span class="string">&#x27;link&#x27;</span>: <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;mode&#x27;</span>: <span class="number">1</span>&#125;,</span><br><span class="line">     <span class="string">&#x27;messages&#x27;</span>: [&#123;<span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;Bye!&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sender_name&#x27;</span>: <span class="string">&#x27;User 2&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;timestamp_ms&#x27;</span>: <span class="number">1675597571851</span>&#125;,</span><br><span class="line">                  ...</span><br><span class="line">                  &#123;<span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;Goodmorning! $50 is too low.&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sender_name&#x27;</span>: <span class="string">&#x27;User 2&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;timestamp_ms&#x27;</span>: <span class="number">1675577876645</span>&#125;],</span><br><span class="line">     <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;User 1 and User 2 chat&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment"># data中的内容将会是：</span></span><br><span class="line">    [</span><br><span class="line">        Document(</span><br><span class="line">            page_content=<span class="string">&#x27;Bye!&#x27;</span>,</span><br><span class="line">            metadata=&#123;</span><br><span class="line">                <span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json&#x27;</span>, </span><br><span class="line">                <span class="string">&#x27;seq_num&#x27;</span>: <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">        ),</span><br><span class="line">        ...</span><br><span class="line">        Document(</span><br><span class="line">            page_content=<span class="string">&#x27;Goodmorning! $50 is too low.&#x27;</span>, </span><br><span class="line">            metadata=&#123;</span><br><span class="line">                <span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json&#x27;</span>, </span><br><span class="line">                <span class="string">&#x27;seq_num&#x27;</span>: <span class="number">10</span></span><br><span class="line">            &#125;</span><br><span class="line">        ),</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>
<p>常见的json格式对应的<code>jq_schema</code>语法如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JSON        -&gt; <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> ...<span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> ...<span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> ...<span class="punctuation">&#125;</span><span class="punctuation">]</span></span><br><span class="line">jq_schema   -&gt; <span class="string">&quot;.[].text&quot;</span></span><br><span class="line"></span><br><span class="line">JSON        -&gt; <span class="punctuation">&#123;</span><span class="attr">&quot;key&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> ...<span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> ...<span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> ...<span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br><span class="line">jq_schema   -&gt; <span class="string">&quot;.key[].text&quot;</span></span><br><span class="line"></span><br><span class="line">JSON        -&gt; <span class="punctuation">[</span><span class="string">&quot;...&quot;</span><span class="punctuation">,</span> <span class="string">&quot;...&quot;</span><span class="punctuation">,</span> <span class="string">&quot;...&quot;</span><span class="punctuation">]</span></span><br><span class="line">jq_schema   -&gt; <span class="string">&quot;.[]&quot;</span></span><br></pre></td></tr></table></figure>
<p>如果你的json文档格式是一个后缀名为<code>.jsonl</code>的 JSON Lines文件，这种文件中的每一行是一个符合json格式的字符串，可以在一个文件中存储多个json对象。对于这种文件，在加载时<code>json_lines=True</code>参数即可。指定的<code>jq_schema</code>索引会对每一行的json对象起作用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loader = JSONLoader(</span><br><span class="line">    file_path=<span class="string">&#x27;./example_data/facebook_chat_messages.jsonl&#x27;</span>,</span><br><span class="line">    jq_schema=<span class="string">&#x27;.content&#x27;</span>,</span><br><span class="line">    text_content=<span class="literal">False</span>,</span><br><span class="line">    json_lines=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">data = loader.load()</span><br></pre></td></tr></table></figure>
<p>你还可以自定义要将哪些json文件的内容写入文档的metadata，即元数据字段中，这需要通过传入一个<code>metadata_func</code>来实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># metadata_func有两个参数，第一个参数是源数据的json对象，第二个参数对应其在文档中的元数据对象。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">metadata_func</span>(<span class="params">record: <span class="built_in">dict</span>, metadata: <span class="built_in">dict</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将原json的&quot;sender_name&quot;字段的值写入文档元数据的&quot;sender_name&quot;字段</span></span><br><span class="line">    metadata[<span class="string">&quot;sender_name&quot;</span>] = record.get(<span class="string">&quot;sender_name&quot;</span>)</span><br><span class="line">    <span class="comment"># 你也可以修改已有元数据的内容，如这里修改了所有文档中元数据的路径，</span></span><br><span class="line">    <span class="comment"># 将它们路径中 /langchain 前的部分都删除了。</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;source&quot;</span> <span class="keyword">in</span> metadata:</span><br><span class="line">        source = metadata[<span class="string">&quot;source&quot;</span>].split(<span class="string">&quot;/&quot;</span>)</span><br><span class="line">        source = source[source.index(<span class="string">&quot;langchain&quot;</span>):]</span><br><span class="line">        metadata[<span class="string">&quot;source&quot;</span>] = <span class="string">&quot;/&quot;</span>.join(source)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> metadata</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loader = JSONLoader(</span><br><span class="line">    file_path=<span class="string">&#x27;./example_data/facebook_chat.json&#x27;</span>,</span><br><span class="line">    jq_schema=<span class="string">&#x27;.messages[]&#x27;</span>,</span><br><span class="line">    content_key=<span class="string">&quot;content&quot;</span>,</span><br><span class="line">    metadata_func=metadata_func</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">data = loader.load()</span><br></pre></td></tr></table></figure>
<h4 id="markdown"><a class="markdownIt-Anchor" href="#markdown"></a> Markdown</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredMarkdownLoader</span><br><span class="line"></span><br><span class="line">markdown_path = <span class="string">&quot;../README.md&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果加入参数mode=&quot;elements&quot;，将会保留所有markdown语法的符号（如```)，否则只会提取文本</span></span><br><span class="line">loader = UnstructuredMarkdownLoader(markdown_path, mode=<span class="string">&quot;elements&quot;</span>)</span><br><span class="line">data = loader.load()</span><br></pre></td></tr></table></figure>
<h4 id="pdf"><a class="markdownIt-Anchor" href="#pdf"></a> PDF</h4>
<p>有多种方法提取PDF文件的内容（基于底层的不同），详情参见：<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf">https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf</a><br />
这里介绍使用<code>PyPDFLoader</code>读取文件，它返回一个<code>Document</code>的列表，每个对象都是pdf中一页的内容对应的文档，页码信息会被存储到元数据中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确保先安装了pypdf</span></span><br><span class="line"><span class="comment"># pip install pypdf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> PyPDFLoader</span><br><span class="line"></span><br><span class="line">loader = PyPDFLoader(<span class="string">&quot;example_data/layout-parser-paper.pdf&quot;</span>)</span><br><span class="line">pages = loader.load_and_split()</span><br></pre></td></tr></table></figure>
<p>如果想从读取图像中的文字，可以使用 <code>rapidocr-onnxruntime</code> ,这是一个OCR包，可以识别图像中的文本并存储到文档中。安装完成后加入参数<code>extract_images=True</code>即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install rapidocr-onnxruntime</span></span><br><span class="line"></span><br><span class="line">loader = PyPDFLoader(<span class="string">&quot;https://arxiv.org/pdf/2103.15348.pdf&quot;</span>, extract_images=<span class="literal">True</span>)</span><br><span class="line">pages = loader.load()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果对pdf文档的结构无所谓，使用非结构化数据可以只提取所有的纯编码数据到文档。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredPDFLoader</span><br><span class="line"></span><br><span class="line">loader = UnstructuredPDFLoader(<span class="string">&quot;example_data/layout-parser-paper.pdf&quot;</span>) <span class="comment"># 同样，加入mode=&quot;elements&quot;可以保留元素</span></span><br><span class="line">data = loader.load()</span><br></pre></td></tr></table></figure>
<h4 id="python源码"><a class="markdownIt-Anchor" href="#python源码"></a> Python源码</h4>
<p>可以使用<code>PythonLoader</code>类直接加载python代码为文档。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> PythonLoader</span><br></pre></td></tr></table></figure>
<h4 id="批量加载文档"><a class="markdownIt-Anchor" href="#批量加载文档"></a> 批量加载文档</h4>
<p>从一个目录下加载目录中的所有文件。默认情况下，所有文件都以非结构化的方式（直接读取文本）加载到文档对象中，返回一个列表。<br />
<code>DirectoryLoader</code>相关参数：</p>
<ul>
<li><code>glob</code> : 用来控制加载符合哪些路径名和扩展名的文件</li>
<li><code>show_progress=True</code>：显示进度条，需要先安装<code>tqdm</code>库</li>
<li><code>use_multithreading=True</code>：使用多线程加快速度</li>
<li><code>loader_cls=TextLoader</code>：指定所有文件使用某个特定的文档加载器类</li>
<li><code>silent_errors=True</code>：如果加载过程中出现错误（如编码问题），忽略错误并继续加载（默认将会终止加载，并不会返回已加载的内容到内存）</li>
<li><code>loader_kwargs=&#123;&#125;</code> ：其他加载时的参数，以字典形式传入
<ul>
<li>要在加载过程中自动检测文件编码，传入<code>&#123;'autodetect_encoding':True&#125;</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> DirectoryLoader</span><br><span class="line"></span><br><span class="line">text_loader_kwargs=&#123;<span class="string">&#x27;autodetect_encoding&#x27;</span>: <span class="literal">True</span>&#125;</span><br><span class="line">loader = DirectoryLoader(path, glob=<span class="string">&quot;**/*.txt&quot;</span>, loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)</span><br><span class="line">docs = loader.load()</span><br></pre></td></tr></table></figure>
<h3 id="分割文档"><a class="markdownIt-Anchor" href="#分割文档"></a> 分割文档</h3>
<p>处理长文本时，有必要将文本分割成块。尽管这听起来很简单，但这里有很多潜在的复杂性。理想情况下，您希望将语义相关的文本片段放在一起。“语义相关”的含义可能取决于文本的类型。<br />
关于LangChain支持的所有分割器类型，参考：<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/data_connection/document_transformers/">Text Splitters | 🦜️🔗 Langchain</a><br />
这里只介绍两种：基于字符（单词）拆分和基于语义拆分。</p>
<h4 id="按照字符拆分文档"><a class="markdownIt-Anchor" href="#按照字符拆分文档"></a> 按照字符拆分文档</h4>
<p>最简单的方法，按照字符数切分文档，一般只适用于西文。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(</span><br><span class="line">    separator=<span class="string">&quot;\n\n&quot;</span>,</span><br><span class="line">    chunk_size=<span class="number">1000</span>,</span><br><span class="line">    chunk_overlap=<span class="number">200</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span>,</span><br><span class="line">    is_separator_regex=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里传入的是纯文本的列表对象，将会分别拆分每个对象。</span></span><br><span class="line"><span class="comment"># metadata是可选的，会一一对应地写入被拆分的文档中的元数据。</span></span><br><span class="line"><span class="comment"># 返回值是Document类型</span></span><br><span class="line">documents = text_splitter.create_documents(</span><br><span class="line">    [doc1, doc2, ...], </span><br><span class="line">    metadatas=[meta1, meta2, ...]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<h4 id="按照单词拆分"><a class="markdownIt-Anchor" href="#按照单词拆分"></a> 按照单词拆分</h4>
<p>推荐普遍情况下有一堆长文本时使用，希望尽可能的将相近的段落和语句放在一个分块中，适用于各种语言的编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    <span class="comment"># Set a really small chunk size, just to show.</span></span><br><span class="line">    chunk_size=<span class="number">100</span>,</span><br><span class="line">    chunk_overlap=<span class="number">20</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span>,</span><br><span class="line">    is_separator_regex=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">texts = text_splitter.create_documents([doc])</span><br><span class="line"><span class="built_in">print</span>(texts[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(texts[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h4 id="根据语义拆分"><a class="markdownIt-Anchor" href="#根据语义拆分"></a> 根据语义拆分</h4>
<p>这将根据文本中的语义相似性拆分文本，语义更相似的，更有可能放在一起。需要文本嵌入编码模型的介入。<br />
这里使用OPENAI的文本嵌入api作为语义相似度检查的例子，使用前需要先配置好相关api环境变量:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">!pip install --quiet langchain_experimental langchain_openai</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_experimental.text_splitter <span class="keyword">import</span> SemanticChunker</span><br><span class="line"><span class="keyword">from</span> langchain_openai.embeddings <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"></span><br><span class="line">text_splitter = SemanticChunker(OpenAIEmbeddings())</span><br><span class="line"></span><br><span class="line">docs = text_splitter.create_documents([doc])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(docs[<span class="number">0</span>].page_content)</span><br></pre></td></tr></table></figure>
<h3 id="编码文档为向量嵌入"><a class="markdownIt-Anchor" href="#编码文档为向量嵌入"></a> 编码文档为向量嵌入</h3>
<p>编码文档到向量嵌入非常简单，只需制定好模型，然后传入字符串格式的文本即可。前面小节中加载的文档对象，可以从<code>page_content</code>字段中取出字符串值。<br />
可以使用的文本-向量编码模型及说明文档参考：<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/text_embedding/">Text embedding models | 🦜️🔗 Langchain</a><br />
下面会使用在线调用OPENAI的文档编码模型为例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你在全局设置了环境变量，则无需传入api key的参数</span></span><br><span class="line">embeddings_model = OpenAIEmbeddings(openai_api_key=<span class="string">&quot;...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里直接使用字符串作为例子</span></span><br><span class="line">embeddings = embeddings_model.embed_documents(</span><br><span class="line">    [</span><br><span class="line">        <span class="string">&quot;Hi there!&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Oh, hello!&quot;</span>,</span><br><span class="line">        <span class="string">&quot;What&#x27;s your name?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;My friends call me World&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Hello World!&quot;</span></span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">len</span>(embeddings), <span class="built_in">len</span>(embeddings[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>可以使用文本-向量编码模型快速比较不同文本直接的相似度，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">embedded_query = embeddings_model.embed_query(<span class="string">&quot;What was the name mentioned in the conversation?&quot;</span>)</span><br><span class="line">embedded_query[:<span class="number">5</span>] <span class="comment"># 输出embed_query传入的文本与embeddings_model中已编码的5个文本之间（各个）的相似度。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出举例：</span></span><br><span class="line">[<span class="number">0.0053587136790156364</span>,</span><br><span class="line"> -<span class="number">0.0004999046213924885</span>,</span><br><span class="line"> <span class="number">0.038883671164512634</span>,</span><br><span class="line"> -<span class="number">0.003001077566295862</span>,</span><br><span class="line"> -<span class="number">0.00900818221271038</span>]</span><br></pre></td></tr></table></figure>
<h3 id="创建向量存储器"><a class="markdownIt-Anchor" href="#创建向量存储器"></a> 创建向量存储器</h3>
<p>下面的例子使用 FAISS 向量数据库，该数据库利用了Facebook AI相似性搜索（FAISS）库。<br />
也可以选择其他不同的向量存储器库，参考：<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/vectorstores">Vector stores | 🦜️🔗 Langchain</a><br />
先安装相关依赖：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install faiss-cpu</span><br></pre></td></tr></table></figure>
<p>相关步骤如下：</p>
<ul>
<li>加载一个示例文档</li>
<li>使用分割器将文档分割为固定长度的字符串</li>
<li>对于每个字符串切片，进行向量嵌入编码（使用<code>OpenAIEmbeddings</code>），然后使用<code>FAISS.from_documents</code>存储到数据库对象。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取源文件</span></span><br><span class="line">raw_documents = TextLoader(<span class="string">&#x27;../../../state_of_the_union.txt&#x27;</span>).load()</span><br><span class="line"><span class="comment"># 分割文本为若干块，并转换为对应的文档对象</span></span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">documents = text_splitter.split_documents(raw_documents)</span><br><span class="line"><span class="comment"># 创建向量数据库，传入两个参数：文档对象列表 和 嵌入模型的实例化对象</span></span><br><span class="line">db = FAISS.from_documents(documents, OpenAIEmbeddings())</span><br></pre></td></tr></table></figure>
<h4 id="相似性搜索"><a class="markdownIt-Anchor" href="#相似性搜索"></a> 相似性搜索</h4>
<p>创建好向量数据库后，就可以根据相似性，搜索出需要的文本内容，然后解码为可读的文本。<br />
<code>similarity_search</code>函数将会查询输入文本，找出最相关的多个文档，并返回其解码后的文档内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">&quot;What did the president say about Ketanji Brown Jackson&quot;</span></span><br><span class="line">docs = db.similarity_search(query)</span><br><span class="line"><span class="built_in">print</span>(docs[<span class="number">0</span>].page_content)</span><br></pre></td></tr></table></figure>
<p>如果输入不是可读文本，而是一个已经编码好的向量，使用函数<code>similarity_search_by_vector </code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embedding_vector = OpenAIEmbeddings().embed_query(query)</span><br><span class="line">docs = db.similarity_search_by_vector(embedding_vector)</span><br><span class="line"><span class="built_in">print</span>(docs[<span class="number">0</span>].page_content)</span><br></pre></td></tr></table></figure>
<h4 id="嵌入缓存"><a class="markdownIt-Anchor" href="#嵌入缓存"></a> 嵌入缓存</h4>
<p>使用带有缓存的文本嵌入类，可以使得从重复的文本创建向量数据库时，不再调用模型编码而直接查询缓存中的结果。这可能可以减少模型的调用量。<br />
通常，嵌入缓存和向量数据库一起使用，当查询缓存命中时，LangChain直接从缓存中构建向量数据库，而不是从原始文本编码开始。下面是一个例子：</p>
<ul>
<li>为了使用缓存，首先需要引入名称<code>CacheBackedEmbeddings</code>，并指定一个用于底层编码嵌入向量的模型，这里仍然使用<code>OpenAIEmbeddings</code></li>
<li>创建一个缓存的存储位置，下面的例子中，缓存位于本地文件目录<code>&quot;./cache/&quot;</code>中。
<ul>
<li>你也可以将缓存设为存储在内存中（如果你的内存足够），使用如下的语句：</li>
<li><code>from langchain.storage import InMemoryByteStore</code></li>
<li><code>store = InMemoryByteStore()</code></li>
</ul>
</li>
<li>指定一个命名空间参数<code>namespace</code>，命名空间比较重要，它保证了你使用 不同的嵌入模型 编码 同一文本 时不会触发缓存命中（否则这容易导致不同模型间的冲突）。一般来说，将其指定为嵌入模型的名称即可。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> CacheBackedEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.storage <span class="keyword">import</span> LocalFileStore</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"></span><br><span class="line">underlying_embeddings = OpenAIEmbeddings()</span><br><span class="line"></span><br><span class="line">store = LocalFileStore(<span class="string">&quot;./cache/&quot;</span>)</span><br><span class="line"></span><br><span class="line">cached_embedder = CacheBackedEmbeddings.from_bytes_store(</span><br><span class="line">    underlying_embeddings, store, namespace=underlying_embeddings.model</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>完成后，你可以使用<code>cached_embedder</code>代替<code>OpenAIEmbeddings()</code>等底层嵌入模型创建向量数据库，它们会自动在需要时使用缓存。<br />
使用<code>list(store.yield_keys())</code>可以查看缓存中的主键。</p>
<h3 id="检索器"><a class="markdownIt-Anchor" href="#检索器"></a> 检索器</h3>
<p>上面的内容介绍了简单的向量数据库语义搜索，它们直接返回相应的文本。但实际使用时，往往需要结构化的数据，因此需要包装检索器。<br />
LangChain提供了许多内置的检索器对象，可以方便地完成多查询、排序、时间加权等功能。关于所有的检索器类，参考：<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/data_connection/retrievers/">Retrievers | 🦜️🔗 Langchain</a></p>
<h4 id="基础检索器-基于向量数据库"><a class="markdownIt-Anchor" href="#基础检索器-基于向量数据库"></a> 基础检索器 - 基于向量数据库</h4>
<p>可以直接从向量数据库对象构建检索器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 省略了前置步骤……</span></span><br><span class="line">db = FAISS.from_documents(docs, embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建检索器</span></span><br><span class="line">retriever = db.as_retriever()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询相关的文档(默认以相似性搜索)</span></span><br><span class="line">docs = retriever.get_relevant_documents(<span class="string">&quot;what did he say about ketanji brown jackson&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在创建检索器时，可以指定其使用的检索方法， 如最大边缘相关性搜索（MMR）,前提是对应的向量数据库对象支持这种检索。<br />
在<code>search_kwargs</code>参数中可以使用字典传入其他参数，如相似性分数阈值（只返回分数高于该阈值的文档），或在指定top k（只返回k个最相关的文档）等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用MMR检索</span></span><br><span class="line">retriever = db.as_retriever(</span><br><span class="line">    search_type=<span class="string">&quot;mmr&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 使用带有阈值的相似性搜索</span></span><br><span class="line">retriever = db.as_retriever(</span><br><span class="line">    search_type=<span class="string">&quot;similarity_score_threshold&quot;</span>, search_kwargs=&#123;<span class="string">&quot;score_threshold&quot;</span>: <span class="number">0.5</span>&#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 指定Top k</span></span><br><span class="line">retriever = db.as_retriever(</span><br><span class="line">    search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">3</span>&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="多查询检索器-multiqueryretriever"><a class="markdownIt-Anchor" href="#多查询检索器-multiqueryretriever"></a> 多查询检索器 MultiQueryRetriever</h4>
<p>这种查询器的目标时为了提高语义检索的鲁棒性。由于自然语言存在一定的冗余性和歧义，当查询措辞发生细微变化，或者嵌入不能很好地捕捉数据的语义时，检索可能会产生不同的结果。这通常会使得检索到的文档范围缩小（即同样的问题，只是改了一个措辞，就查不到相关文档了）。为了解决这样的问题，可以使用多查询：即每次查询时，使用多个语义相近的字符串代替单一字符串，进行多次查询，并统合最终的结果。<br />
在LangChain中，自然地使用LLM来生成“多个语义相近的查询”，这样用户只需像往常一样输入一个查询语句，但可以达到多查询的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.retrievers.multi_query <span class="keyword">import</span> MultiQueryRetriever</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbedding</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个向量数据库（这里使用的存储后端是Chroma）</span></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">vectordb = Chroma.from_documents(documents=docs, embedding=embeddings)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>) <span class="comment"># 实例化一个LLM对象，作为多查询生成器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建多查询检索器</span></span><br><span class="line">retriever_from_llm = MultiQueryRetriever.from_llm(</span><br><span class="line">    retriever=vectordb.as_retriever(), llm=llm</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="comment"># 增加一个logging对象，以打印生成出的多查询内容</span></span><br><span class="line">logging.basicConfig()</span><br><span class="line">logging.getLogger(<span class="string">&quot;langchain.retrievers.multi_query&quot;</span>).setLevel(logging.INFO)</span><br><span class="line"></span><br><span class="line">question = <span class="string">&quot;What are the approaches to Task Decomposition?&quot;</span></span><br><span class="line">unique_docs = retriever_from_llm.get_relevant_documents(query=question)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出示例：这里是log输出，即LLM为查询question生成的相近语义问题</span></span><br><span class="line">INFO:langchain.retrievers.multi_query:Generated queries: </span><br><span class="line">[</span><br><span class="line"> <span class="string">&#x27;1. How can Task Decomposition be approached?&#x27;</span>, </span><br><span class="line"> <span class="string">&#x27;2. What are the different methods for Task Decomposition?&#x27;</span>, </span><br><span class="line"> <span class="string">&#x27;3. What are the various approaches to decomposing tasks?&#x27;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>作为生成多查询的基础，你可以修改底层LLM的配置。上述例子默认使用一个基本的LLM对话。你可以使用基本的LangChain链来将其自定义为带有特定prompt的LLM对话：</p>
<blockquote>
<p>注意：为了使得检索器能正确找到LLM输出的多查询内容，在自定义LLM chain时需要将其输出格式化解析为键值对的形式，参考下面的例子。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个LLM输出格式解析器，将输出安装换行符分割，存储到一个lines的字典里</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LineList</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="comment"># &quot;lines&quot; is the key (attribute name) of the parsed output</span></span><br><span class="line">    lines: <span class="type">List</span>[<span class="built_in">str</span>] = Field(description=<span class="string">&quot;Lines of text&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LineListOutputParser</span>(<span class="title class_ inherited__">PydanticOutputParser</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(pydantic_object=LineList)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; LineList:</span><br><span class="line">        lines = text.strip().split(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> LineList(lines=lines)</span><br><span class="line"></span><br><span class="line">output_parser = LineListOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义撰写提示，要求LLM输出5个多查询语句</span></span><br><span class="line">QUERY_PROMPT = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;question&quot;</span>],</span><br><span class="line">    template=<span class="string">&quot;&quot;&quot;You are an AI language model assistant. Your task is to generate five </span></span><br><span class="line"><span class="string">    different versions of the given user question to retrieve relevant documents from a vector </span></span><br><span class="line"><span class="string">    database. By generating multiple perspectives on the user question, your goal is to help</span></span><br><span class="line"><span class="string">    the user overcome some of the limitations of the distance-based similarity search. </span></span><br><span class="line"><span class="string">    Provide these alternative questions separated by newlines.</span></span><br><span class="line"><span class="string">    Original question: &#123;question&#125;&quot;&quot;&quot;</span>,</span><br><span class="line">)</span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 构造LLM Chain</span></span><br><span class="line">llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用自定义的LLM Chain 创建多查询检索器，其中，parser_key函数指定了</span></span><br><span class="line"><span class="comment"># 检索器从输出数据的哪个key中获取最终的多查询语句列表</span></span><br><span class="line">retriever = MultiQueryRetriever(</span><br><span class="line">    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=<span class="string">&quot;lines&quot;</span></span><br><span class="line">) </span><br></pre></td></tr></table></figure>
<h4 id="上下文压缩检索器"><a class="markdownIt-Anchor" href="#上下文压缩检索器"></a> 上下文压缩检索器</h4>
<p>有的时候，知识库中的每个文档都是长文本，而当检索时，用户只需要文档的概括内容，或是其中与查询最相关的内容，而非整个文档。通常这需要对查询结果进行后处理来实现，不过LangChain为此包装了压缩检索器，可以一并完成查询结果过滤的过程。<br />
上下文压缩检索器基于基本检索器，在使用前，先从向量数据库构建一个基本检索器。</p>
<ul>
<li>可以使用一个LLM实例来决定过滤掉结果文档中的哪些内容，这被称为<code>LLMChainFilter</code>：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.retrievers.document_compressors <span class="keyword">import</span> LLMChainFilter</span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="comment"># 省略知识库数据处理的过程</span></span><br><span class="line"><span class="comment"># 创建基本检索器</span></span><br><span class="line">retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要一个LLM作为过滤器的后端</span></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 创建LLM过滤器</span></span><br><span class="line">_<span class="built_in">filter</span> = LLMChainFilter.from_llm(llm)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建上下文压缩检索器</span></span><br><span class="line">compression_retriever = ContextualCompressionRetriever(</span><br><span class="line">    base_compressor=_<span class="built_in">filter</span>, </span><br><span class="line">    base_retriever=retriever</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行查询，并返回压缩后的查询结果</span></span><br><span class="line">compressed_docs = compression_retriever.get_relevant_documents(</span><br><span class="line">    <span class="string">&quot;What did the president say about Ketanji Jackson Brown&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>但是，使用LLM作为压缩和过滤的后端，会让每次查询时都增加许多额外的LLM调用，如果想要节约成本，可以直接使用嵌入模型，查询结果文档内部语句的相似度，从而返回结果。这种情况下，使用<code>EmbeddingsFilter</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.retrievers.document_compressors <span class="keyword">import</span> EmbeddingsFilter</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="comment"># 省略知识库数据处理的过程</span></span><br><span class="line"><span class="comment"># 创建基本检索器</span></span><br><span class="line">retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line"><span class="comment"># 构建向量相似度过滤器</span></span><br><span class="line">embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=<span class="number">0.76</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建上下文压缩检索器</span></span><br><span class="line">compression_retriever = ContextualCompressionRetriever(</span><br><span class="line">    base_compressor=embeddings_filter, </span><br><span class="line">    base_retriever=retriever</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行查询，并返回压缩后的查询结果</span></span><br><span class="line">compressed_docs = compression_retriever.get_relevant_documents(</span><br><span class="line">    <span class="string">&quot;What did the president say about Ketanji Jackson Brown&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>LangChain还提供多种过滤器，如去除冗余的向量嵌入等。并且，分割文本的工具类也可以视为过滤器。<br />
当你需要对输出文本进行一些流水线处理时（如先分割，在去除冗余，最后提取高相似度文本），可以使用<code>ocumentCompressorPipeline</code>构建一个<strong>流水线压缩器</strong>，LangChain将会自动按步骤处理中间文档，只输出最终结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.retrievers.document_compressors <span class="keyword">import</span> DocumentCompressorPipeline</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_transformers <span class="keyword">import</span> EmbeddingsRedundantFilter</span><br><span class="line"></span><br><span class="line">splitter = CharacterTextSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">0</span>, separator=<span class="string">&quot;. &quot;</span>) <span class="comment"># 文本分割</span></span><br><span class="line">redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings) <span class="comment"># 去除冗余</span></span><br><span class="line">relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=<span class="number">0.76</span>) <span class="comment"># 相似度比较</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建流水线压缩器</span></span><br><span class="line">pipeline_compressor = DocumentCompressorPipeline(</span><br><span class="line">    transformers=[splitter, redundant_filter, relevant_filter]</span><br><span class="line"><span class="comment"># 构建上下文压缩检索器</span></span><br><span class="line">)compression_retriever = ContextualCompressionRetriever(</span><br><span class="line">    base_compressor=pipeline_compressor, base_retriever=retriever</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">compressed_docs = compression_retriever.get_relevant_documents(</span><br><span class="line">    <span class="string">&quot;What did the president say about Ketanji Jackson Brown&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="多向量检索器-multivector-retriever"><a class="markdownIt-Anchor" href="#多向量检索器-multivector-retriever"></a> 多向量检索器 MultiVector Retriever</h4>
<p>注意：这与多查询不同，多向量的意思时，每个源文档在存入知识库（向量数据库）时，具有多个对应的向量嵌入，而它们的特性不同（如分别描述了摘要，重点问题。解决方案等文档内容的不同方面）。然后检索时，按照查询的语义偏好查找同一文档的不同向量嵌入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TODO</span><br></pre></td></tr></table></figure>
<h4 id="带有时间加权的检索器"><a class="markdownIt-Anchor" href="#带有时间加权的检索器"></a> 带有时间加权的检索器</h4>
<p>在检索语义时，也考虑同样的文档或知识内容 <em>上一次被访问的时间</em>。<br />
<code>语义相似度 + (1.0 - decay_rate) ^ 距离上次检索过去的时间（小时）</code><br />
<code>decay_rate</code>被设定为0-1之间的值，越接近0，意味着在检索时，新鲜时间对文档检索结果的影响越大，反之亦然。因此<code>decay_rate</code>为1时，时间加权相当于不存在；<code>decay_rate</code>为0时，则无论文档上一次访问的时间距离多久，文档都有相同的新鲜度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">from</span> langchain.docstore <span class="keyword">import</span> InMemoryDocstore</span><br><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> TimeWeightedVectorStoreRetriever</span><br><span class="line"><span class="keyword">from</span> langchain.schema <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义嵌入模型</span></span><br><span class="line">embeddings_model = OpenAIEmbeddings()</span><br><span class="line"><span class="comment"># 初始化一个空的向量数据库</span></span><br><span class="line">embedding_size = <span class="number">1536</span></span><br><span class="line">index = faiss.IndexFlatL2(embedding_size)</span><br><span class="line">vectorstore = FAISS(embeddings_model, index, InMemoryDocstore(&#123;&#125;), &#123;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改decay_rate来测试时间加权的检索效果</span></span><br><span class="line">retriever = TimeWeightedVectorStoreRetriever(</span><br><span class="line">    vectorstore=vectorstore, decay_rate=<span class="number">0.01</span>, k=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">yesterday = datetime.now() - timedelta(days=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 添加一个字符串作为文档，设置上次检索时间为昨天</span></span><br><span class="line">retriever.add_documents(</span><br><span class="line">    [Document(page_content=<span class="string">&quot;hello world&quot;</span>, metadata=&#123;<span class="string">&quot;last_accessed_at&quot;</span>: yesterday&#125;)]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 再添加一个字符串，不设定上次检索时间</span></span><br><span class="line">retriever.add_documents([Document(page_content=<span class="string">&quot;hello foo&quot;</span>)])</span><br><span class="line"></span><br><span class="line">retriever.get_relevant_documents(<span class="string">&quot;hello world&quot;</span>)</span><br><span class="line"><span class="comment"># 在decay_rate接近0的时候会返回&quot;hello world&quot;，而接近1时会返回&quot;hello foo&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="将检索器包装为llm-chain"><a class="markdownIt-Anchor" href="#将检索器包装为llm-chain"></a> 将检索器包装为LLM Chain</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> create_retrieval_chain</span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="comment"># 省略知识库数据处理的过程</span></span><br><span class="line">vector = FAISS.from_documents(documents, OpenAIEmbeddings())</span><br><span class="line">retriever = vector.as_retriever()</span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="comment"># 有两种方式构建带有检索器的链，一是在已有链的基础上使用create_retrieval相关函数</span></span><br><span class="line">chain = prompt | llm | output_parser</span><br><span class="line">retrieval_chain = create_retrieval_chain(retriever, chain)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二是直接在链的RunnableParalle管道中传入retriever对象（也是Runnable的）</span></span><br><span class="line">retrieval_chain = (</span><br><span class="line">    &#123;<span class="string">&quot;context&quot;</span>: retriever, <span class="string">&quot;question&quot;</span>: RunnablePassthrough()&#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | llm</span><br><span class="line">    | output_parser</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="将检索器包装为智能体工具"><a class="markdownIt-Anchor" href="#将检索器包装为智能体工具"></a> 将检索器包装为智能体工具</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.tools.retriever <span class="keyword">import</span> create_retriever_tool</span><br><span class="line"></span><br><span class="line"><span class="meta">... </span><span class="comment"># 省略知识库数据处理的过程</span></span><br><span class="line">vector = FAISS.from_documents(documents, OpenAIEmbeddings())</span><br><span class="line">retriever = vector.as_retriever()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建检索器工具</span></span><br><span class="line">retriever_tool = create_retriever_tool(</span><br><span class="line">    retriever,</span><br><span class="line">    <span class="string">&quot;langsmith_search&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Search for information about LangSmith. For any questions about LangSmith, you must use this tool!&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="langchain-agent"><a class="markdownIt-Anchor" href="#langchain-agent"></a> LangChain Agent</h2>
<h3 id="核心概念"><a class="markdownIt-Anchor" href="#核心概念"></a> 核心概念</h3>
<ul>
<li><code>AgentAction</code>：<code>dataclass</code>，表示Agent采取的一个动作，通常是使用工具</li>
</ul>
<blockquote>
<p>属性：</p>
<ul>
<li><code>tool</code> 应该调用的工具的名称</li>
<li><code>tool_input</code> 该工具的输入</li>
</ul>
</blockquote>
<p>实际使用时通常为带Log的<code>AgentActionMessageLog</code></p>
<ul>
<li><code>AgentFinish</code>：智能体准备返回给用户时的最终结果</li>
</ul>
<blockquote>
<p>属性：</p>
<ul>
<li><code>return_values</code> 字典集，包含所有最终输出
<ul>
<li>通常有一个名为<code>output</code>的key，表示回复给用户的字符串</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>Intermediate Steps：记录所有智能体的历史动作，和与本次运行相关的输出
<ul>
<li>通常被保存为一个列表数据类型：<code>List[Tuple(AgentAction, Observation)]</code>
<ul>
<li>为了最大化框架的灵活性，<code>Observation</code>的类型可以是“任意”（Any），但通常是字符串</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="智能体链基类agent"><a class="markdownIt-Anchor" href="#智能体链基类agent"></a> 智能体链基类：Agent</h3>
<p>这是一个在LangChain的链 的基础上构建的类，功能是让大模型决策下一步的行动。</p>
<blockquote>
<p>再次提醒：记住 <em>LangChain的链</em> 的概念其实只是<strong>包装好</strong>一个提示大模型并解析输出的过程，并允许在这个过程中间插入额外的数据处理步骤。</p>
</blockquote>
<p>既然Agent基于Chain类型构建，因此底层上可能使用两种LangChain提示策略包装，即 <strong>纯LLM</strong> 或 <strong>聊天模型</strong>（Chat Models），根据该类型的不同，Agent具有不同的 “预期类型” （Intended Model Type）<br />
同时，LangChain为常用功能包装好了若干智能体类，它们支持的功能各不相同。</p>
<h3 id="智能体执行器类agentexecutor"><a class="markdownIt-Anchor" href="#智能体执行器类agentexecutor"></a> 智能体执行器类：AgentExecutor</h3>
<p>程序运行时，实际执行智能体功能的躯壳（运行时），进行实际调用智能体、执行它选择的动作、将动作输出传递回代理的操作，并重复这些操作。<br />
AgentExecutor在运行时可以处理的问题：</p>
<ul>
<li>如果Agent选择了不存在的工具</li>
<li>如果Agent的输出无法解析为合法的工具调用或最终输出</li>
<li>如果Agent选择的工具在运行时发生错误</li>
<li>记录Agent在所有级别上的运行日志（可以输出到控制台或使用<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/langsmith">LangSmith</a>记录）</li>
</ul>
<h3 id="预置智能体的使用"><a class="markdownIt-Anchor" href="#预置智能体的使用"></a> 预置智能体的使用</h3>
<h4 id="预期类型是chat-models的内置智能体"><a class="markdownIt-Anchor" href="#预期类型是chat-models的内置智能体"></a> 预期类型是Chat Models的内置智能体</h4>
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/agents/agent_types/openai_tools">OpenAI Tools</a>
<ul>
<li>使用OPENAI支持“工具调用”的模型，这些模型经过了微调，增强了检测何时调用函数，并输出 <strong>应该传递给该函数的参数</strong> 作为响应的能力。这使得LangChain可用利用这一特性调用多个函数，以此来选择AgentAction，进行动作规划
<ul>
<li>和下面的“函数调用”的区别是，“函数调用”只支持推理单个函数，而“工具调用”支持一个或多个函数。</li>
<li>只在OPENAI最新的模型（<code>gpt-3.5-turbo-1106</code>或<code>gpt-4-1106</code>）以后支持。</li>
<li>因为“工具调用”是“函数调用”的上位替代，因此OPENAI认为“函数调用”是废旧的功能，不建议再使用旧的OpenAI Functions，而是统一使用OpenAI Tools构建智能体。</li>
</ul>
</li>
<li>需要导入的引用：</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 支持OPENAI的agent</span></span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_openai_tools_agent</span><br><span class="line"><span class="comment"># 底层的聊天模型</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他适用于创建工具，网络通信等的包都省略了……</span></span><br></pre></td></tr></table></figure>
<ul>
<li>创建智能体</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义可用的工具列表，这里是一个示例，导入Langchain内置的其中一个网络搜索工具</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.tavily_search <span class="keyword">import</span> TavilySearchResults</span><br><span class="line">tools = [TavilySearchResults(max_results=<span class="number">1</span>)] <span class="comment"># 通常工具定义为全局变量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_openai_tools_agent</span>():</span><br><span class="line">    <span class="comment"># 定义一个提示词，可自定义，这里从一个langchain仓库中获取</span></span><br><span class="line">    <span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line">    prompt = hub.pull(<span class="string">&quot;hwchase17/openai-tools-agent&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化一个聊天模型对象</span></span><br><span class="line">    llm = ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-1106&quot;</span>, temperature=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化智能体对象</span></span><br><span class="line">    agent = create_openai_tools_agent(llm, tools, prompt)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> agent</span><br></pre></td></tr></table></figure>
<ul>
<li>实例化运行时并执行智能体</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">agent = new_openai_tools_agent()</span><br><span class="line"><span class="comment"># 创建智能体执行器</span></span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 输入内容</span></span><br><span class="line"><span class="built_in">input</span> = &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;what is LangChain?&quot;</span>&#125;</span><br><span class="line">agent_executor.invoke(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>默认Input只包含用户的一个字符串键值对，但鉴于OpenAI Tools智能体是基于聊天模型的，可再输入中添加任意的对话历史，这和聊天模型的链调用一样：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> AIMessage, HumanMessage</span><br><span class="line"></span><br><span class="line">agent_executor.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;what&#x27;s my name? Don&#x27;t use tools to look this up unless you NEED to&quot;</span>,</span><br><span class="line">        <span class="string">&quot;chat_history&quot;</span>: [</span><br><span class="line">            HumanMessage(content=<span class="string">&quot;hi! my name is bob&quot;</span>),</span><br><span class="line">            AIMessage(content=<span class="string">&quot;Hello Bob! How can I assist you today?&quot;</span>),</span><br><span class="line">        ],</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent">OpenAI Functions</a>
<ul>
<li>与OpenAI Tools一致，使用OPENAI模型内部的“函数调用”（Function calling）功能</li>
<li>使用时，只需修改上述导入的函数名<code>create_openai_tools_agent</code>为<code>create_openai_functions_agent</code>即可，其他配置完全相同。</li>
<li>也适用于其他开源模型提供的与OpenAI Functions兼容格式的API接口，因为这些模型通常没有随着OPENAI而更新api，所以使用时选择旧版的<code>create_openai_tools_agent</code></li>
<li>需要导入的引用：</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 支持OPENAI模型的agent</span></span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_openai_functions_agent</span><br><span class="line"><span class="comment"># 底层的聊天模型</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">prompt = hub.pull(<span class="string">&quot;hwchase17/openai-functions-agent&quot;</span>)</span><br><span class="line"><span class="comment"># 其他的部分省略，与OpenAI Tools 基本一致</span></span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/agents/agent_types/structured_chat">Structured Chat</a>
<ul>
<li>Structured Chat（结构化对话）Agent与OpenAI Tools的原理基本相同，唯一的区别是，因为不依赖模型自身经过微调而输出函数调用的能力，结构化对话Agent通过**_ 引导大模型输出有效的 格式化（或称序列化）函数参数_** 来解析并调用工具，再将结果反馈给大模型，由此完成一步的AgentAction。</li>
<li>默认的序列化方法是JSON。</li>
<li>需要导入的引用：</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_structured_chat_agent</span><br><span class="line"><span class="comment"># 底层的聊天模型仍然可用使用OPENAI模型</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br></pre></td></tr></table></figure>
<ul>
<li>实例化Agent</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义可用的工具列表，这里是一个示例，导入Langchain内置的其中一个网络搜索工具</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.tavily_search <span class="keyword">import</span> TavilySearchResults</span><br><span class="line">tools = [TavilySearchResults(max_results=<span class="number">1</span>)] <span class="comment"># 通常工具定义为全局变量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_structured_chat_agent</span>():</span><br><span class="line">    <span class="comment"># 定义一个提示词，可自定义，这里从一个langchain仓库中获取</span></span><br><span class="line">    <span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line">    prompt = hub.pull(<span class="string">&quot;hwchase17/structured-chat-agent&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化一个聊天模型对象</span></span><br><span class="line">    llm = ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>, temperature=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化智能体对象</span></span><br><span class="line">    agent = create_structured_chat_agent(llm, tools, prompt)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> agent</span><br></pre></td></tr></table></figure>
<ul>
<li>因为非“工具调用”的大模型可能不按照格式化要求输出，在实例化Agent时，可以通过参数控制是否自动处理可能的解析错误。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">agent = new_structured_chat_agent()</span><br><span class="line"><span class="comment"># 创建智能体执行器</span></span><br><span class="line">agent_executor = AgentExecutor(</span><br><span class="line">    agent=agent, tools=tools, verbose=<span class="literal">True</span>, handle_parsing_errors=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 输入内容</span></span><br><span class="line"><span class="built_in">input</span> = &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;what is LangChain?&quot;</span>&#125;</span><br><span class="line">agent_executor.invoke(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Structured Chat Agent内部将会以下面的方式输出函数调用的过程，但最后返回用户的结果output仍然是自然语言。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; Entering new AgentExecutor chain...</span><br><span class="line">Action:</span><br></pre></td></tr></table></figure>
<p>{<br />
“action”: “tavily_search_results_json”,<br />
“action_input”: {“query”: “LangChain”}<br />
}</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 这是工具调用返回的结果，这行注释是方便你理解内容的，并不在实际的输出中出现</span><br><span class="line">[&#123;&#x27;url&#x27;: &#x27;https://www.ibm.com/topics/langchain&#x27;, &#x27;content&#x27;: &#x27;LangChain is essentially a library of abstractions for Python and Javascript...&#x27;&#125;]</span><br><span class="line">Action:</span><br></pre></td></tr></table></figure>
<p>{<br />
“action”: “Final Answer”,<br />
“action_input”: “LangChain is an open source orchestration framework …”<br />
}</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br><span class="line"></span><br><span class="line">&#123;&#x27;input&#x27;: &#x27;what is LangChain?&#x27;,</span><br><span class="line"> &#x27;output&#x27;: &#x27;LangChain is an open source orchestration framework ....&#x27;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/agents/agent_types/json_agent">JSON Chat</a>
<ul>
<li>与Structured Chat Agent基本一致，唯一的区别是 <strong><em>JSON Chat Agent仅支持 一个输入参数 的工具调用情况</em></strong>，如果你的Agent需要调用<strong>含多个参数的</strong>工具，请使用Structured Chat Agent</li>
<li>强制模型使用JSON格式输出函数调用参数。适用于专门对JSON格式进行微调或引导的模型。</li>
<li>使用时，只需将上述步骤中导入的名称<code>create_structured_chat_agent</code>替换为<code>create_json_chat_agent</code></li>
<li>如果观察中间输出，会发现JSON Chat与Structured Chat的区别是在输出序列化文本时不再加入markdown标记代码段的符号<code>``` ```</code>，整个对话的全局都视文本为JSON格式。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_json_chat_agent</span><br><span class="line"><span class="comment"># 底层的聊天模型仍然可用使用OPENAI模型</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br></pre></td></tr></table></figure>
<blockquote>
<p>别忘了以上所有基于对话模型的Agent都可以在输入中简单地加入对话历史。</p>
</blockquote>
<h4 id="预期类型是纯llm的内置智能体"><a class="markdownIt-Anchor" href="#预期类型是纯llm的内置智能体"></a> 预期类型是纯LLM的内置智能体</h4>
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/agents/agent_types/xml_agent">XML</a>
<ul>
<li>对XML特攻，要求模型以XML格式输出函数调用信息，适合于擅长XML处理的模型。与**<em>JSON Chat Agent一样，仅支持单一参数的工具调用</em>**</li>
<li>需要导入的引用：</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_xml_agent</span><br><span class="line"><span class="comment"># 使用 Anthropic’s Claude 模型</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.chat_models <span class="keyword">import</span> ChatAnthropic</span><br></pre></td></tr></table></figure>
<ul>
<li>实例化并运行Agent</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义可用的工具列表，这里是一个示例，导入Langchain内置的其中一个网络搜索工具</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.tavily_search <span class="keyword">import</span> TavilySearchResults</span><br><span class="line">tools = [TavilySearchResults(max_results=<span class="number">1</span>)] <span class="comment"># 通常工具定义为全局变量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_xml_agent</span>():</span><br><span class="line">    <span class="comment"># 定义一个提示词，可自定义，这里从一个langchain仓库中获取</span></span><br><span class="line">    <span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line">    prompt = hub.pull(<span class="string">&quot;hwchase17/xml-agent-convo&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化一个claude-2模型查询对象</span></span><br><span class="line">	llm = ChatAnthropic(model=<span class="string">&quot;claude-2&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化智能体对象</span></span><br><span class="line">    aagent = create_xml_agent(llm, tools, prompt)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> agent</span><br><span class="line"></span><br><span class="line">agent = new_xml_agent()</span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line">agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;what is LangChain?&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>中间输出将会类似下面这样：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; Entering new AgentExecutor chain...</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">tool</span>&gt;</span>tavily_search_results_json<span class="tag">&lt;/<span class="name">tool</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">tool_input</span>&gt;</span>what is LangChain?</span><br><span class="line">  </span><br><span class="line"><span class="comment">&lt;!--这是工具调用返回的结果，这行注释是方便你理解内容的，并不在实际的输出中出现--&gt;</span></span><br><span class="line">[&#123;&#x27;url&#x27;: &#x27;https://aws.amazon.com/what-is/langchain/&#x27;, &#x27;content&#x27;: &#x27;What Is LangChain? ......&#x27;&#125;] </span><br><span class="line">  </span><br><span class="line"><span class="tag">&lt;<span class="name">final_answer</span>&gt;</span>LangChain is an open source framework .....<span class="tag">&lt;/<span class="name">final_answer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br></pre></td></tr></table></figure>
<ul>
<li>底层不是对话模型的接口，因此模型不会认为Agent的一次调用是一场对话的一部分。但是，你仍然可以加入“历史消息”，只不过需要将历史的输入 从使用对话模型的Message类 改为 输入一个简单的字符串：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">agent_executor.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;what&#x27;s my name? Only use a tool if needed, otherwise respond with Final Answer&quot;</span>,</span><br><span class="line">        <span class="comment"># Notice that chat_history is a string, since this prompt is aimed at LLMs, not chat models</span></span><br><span class="line">        <span class="string">&quot;chat_history&quot;</span>: <span class="string">&quot;Human: Hi! My name is Bob\nAI: Hello Bob! Nice to meet you&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/agents/agent_types/react">ReAct</a>
<ul>
<li>使用“ReAct&quot;策略，而不是某种格式化文本来引导Agent执行函数调用，具体参见：<a target="_blank" rel="noopener" href="https://react-lm.github.io/">ReAct论文</a></li>
<li>使用方法和JSON Chat Agent以及 XML Agent一致，同样，让ReAct**<em>调用的每个工具只能含有单一参数</em>**</li>
<li>需要导入的引用</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_react_agent</span><br><span class="line"><span class="comment"># 注意使用非聊天Chat的模型</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAI</span><br></pre></td></tr></table></figure>
<ul>
<li>实例化并运行Agent</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义可用的工具列表，这里是一个示例，导入Langchain内置的其中一个网络搜索工具</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.tavily_search <span class="keyword">import</span> TavilySearchResults</span><br><span class="line">tools = [TavilySearchResults(max_results=<span class="number">1</span>)] <span class="comment"># 通常工具定义为全局变量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_react_agent</span>():</span><br><span class="line">    <span class="comment"># 定义一个提示词，可自定义，这里从一个langchain仓库中获取</span></span><br><span class="line">    <span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line">    prompt = hub.pull(<span class="string">&quot;hwchase17/react&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化一个模型对象</span></span><br><span class="line">	llm = OpenAI()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化智能体对象</span></span><br><span class="line">    agent = create_react_agent(llm, tools, prompt)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> agent</span><br><span class="line"></span><br><span class="line">agent = new_react_agent()</span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line">agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;what is LangChain?&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>中间输出将会类似下面这样：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; Entering new AgentExecutor chain...</span><br><span class="line"></span><br><span class="line">I should research LangChain to learn more about it.</span><br><span class="line">Action: tavily_search_results_json</span><br><span class="line">Action Input: <span class="string">&quot;LangChain&quot;</span></span><br><span class="line"><span class="comment"># 这是工具调用返回的结果，这行注释是方便你理解内容的，并不在实际的输出中出现</span></span><br><span class="line">[&#123;<span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://www.ibm.com/topics/langchain&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;LangChain is essentially a library of .....&#x27;</span>&#125;] </span><br><span class="line"></span><br><span class="line">I should read the summary <span class="keyword">and</span> look at the different features <span class="keyword">and</span> integrations of LangChain.</span><br><span class="line">Action: tavily_search_results_json</span><br><span class="line">Action Input: <span class="string">&quot;LangChain features and integrations&quot;</span>[&#123;<span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://www.ibm.com/topics/langchain&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&quot;LangChain provides integrations for over 25 different embedding methods .....&quot;</span>&#125;] </span><br><span class="line"></span><br><span class="line">I should take note of the launch date <span class="keyword">and</span> popularity of LangChain.</span><br><span class="line">Action: tavily_search_results_json</span><br><span class="line">Action Input: <span class="string">&quot;LangChain launch date and popularity&quot;</span>[&#123;<span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://www.ibm.com/topics/langchain&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&quot;LangChain is an open source orchestration framework for .....&quot;</span>&#125;] </span><br><span class="line"></span><br><span class="line">I now know the final answer.</span><br><span class="line">Final Answer: LangChain <span class="keyword">is</span> an <span class="built_in">open</span> source orchestration framework <span class="keyword">for</span> building applications using large language models (LLMs) like chatbots <span class="keyword">and</span> virtual agents .....</span><br><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>以防你没有去看ReAct的论文，简单解释是，这里Agent的每个动作要求LLM输出三行文本：<br />
第一行：解释你要采取的行动<br />
第二行：选择一个要使用的工具函数的名称<br />
第三行：给出使用这个工具的输入参数</p>
</blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/agents/agent_types/self_ask_with_search">Self Ask With Search</a>
<ul>
<li>这是一个简单的包装，仅仅让模型执行一个简单的网络搜索步骤。</li>
<li>这个类用来帮助开发者熟悉如何自定义构建一个Agent，也适用于非常小的模型，执行简单和轻量化的搜索任务时使用。</li>
<li>使用此Agent时，<strong><em>只允许定义唯一的工具</em></strong>，该工具的名称（name字段）必须是&quot;Intermediate Answer&quot;，返回尽可能精简的搜索结果，Agent负责从中寻找和总结出一个最佳的答案。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_self_ask_with_search_agent</span><br><span class="line"><span class="keyword">from</span> langchain_community.llms <span class="keyword">import</span> Fireworks</span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.tavily_search <span class="keyword">import</span> TavilyAnswer</span><br><span class="line"></span><br><span class="line"><span class="comment"># TavilyAnswer工具类只会返回精简的搜索结果，记得将其命名为&quot;Intermediate Answer&quot;</span></span><br><span class="line"><span class="comment"># tools数组里只能有一个这样的工具</span></span><br><span class="line">tools = [TavilyAnswer(max_results=<span class="number">1</span>, name=<span class="string">&quot;Intermediate Answer&quot;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_self_ask_with_search_agent</span>():</span><br><span class="line">    <span class="comment"># 定义一个提示词，可自定义，这里从一个langchain仓库中获取</span></span><br><span class="line">    <span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line">    prompt = hub.pull(<span class="string">&quot;hwchase17/self-ask-with-search&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化一个模型对象</span></span><br><span class="line">	llm = Fireworks()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化智能体对象</span></span><br><span class="line">    agent = create_self_ask_with_search_agent(llm, tools, prompt)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> agent</span><br><span class="line"></span><br><span class="line">agent = new_self_ask_with_search_agent()</span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line">agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;what is LangChain?&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="工具类tool"><a class="markdownIt-Anchor" href="#工具类tool"></a> 工具类：Tool</h3>
<p>工具是智能体可以调用的函数。 Tool 被抽象为两个组件：</p>
<ul>
<li>input schema：告诉LLM调用该工具需要哪些参数，需要为每个参数定义合理的命名及描述，它们将被LangChain嵌入进Agent的prompt中。</li>
<li>function：实际要运行的函数，通常类型即为一个python函数。</li>
</ul>
<h4 id="工具的使用和实现"><a class="markdownIt-Anchor" href="#工具的使用和实现"></a> 工具的使用和实现</h4>
<h5 id="langchain内置工具"><a class="markdownIt-Anchor" href="#langchain内置工具"></a> Langchain内置工具</h5>
<p><strong>在这里查看目前的内置工具和它们的使用方法：</strong><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/tools"><strong>https://python.langchain.com/docs/integrations/tools</strong></a><br />
步骤：【以使用Wikipedia查询工具为例】</p>
<ol>
<li>首先导入Python名称，部分工具依赖于第三方python库或插件，需要先安装它们：
<ol>
<li><code>%pip install --upgrade --quiet  wikipedia</code></li>
</ol>
</li>
<li>导入名称时，请参考各工具的文档，确保从正确的库路径导入。</li>
</ol>
<blockquote>
<p>部分工具的使用可能还需要导入为了调用该工具而编写的额外模块（如下面的例子所示）<br />
更为繁琐的可能需要在本地计算机上配置服务，或前往第三方平台配置密钥等，请参考具体的内置工具使用文档</p>
</blockquote>
<ol>
<li><code>from langchain.tools import WikipediaQueryRun</code></li>
<li><code>from langchain_community.utilities import WikipediaAPIWrapper</code> 这里就需要一个包装维基百科API调用的模块</li>
<li>做好了这些以后，可以实例化生成工具对象
<ol>
<li><code>wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())</code></li>
</ol>
</li>
</ol>
<p>如果你不知道一个内置工具的具体定义，可以调试如下<strong>基本属性</strong>（下面以<code>tool</code>为例）：</p>
<ul>
<li><code>tool.name</code>名称：字符串，定义工具的名称，可以在实例化时指定</li>
<li><code>tool.description</code>描述：字符串，描述工具的功能，可以在实例化时指定</li>
<li><code>tool.args</code>参数：默认使用JSON格式（即python字典对象），每一条参数的形式为<code>&lt;参数名&gt;：&#123;'title': &lt;参数标题&gt;，'description':&lt;参数描述&gt;, 'type': &lt;参数类型&gt;&#125;</code>，其中参数名是唯一的key，参数标题和描述是可选的，提高开发时的可读性，例如：
<ul>
<li><code>&#123;'query': &#123;'title': 'Query', 'type': 'string'&#125;&#125;</code></li>
</ul>
</li>
<li><code>return_direct</code>：布尔变量，定义函数的输出是否是直接返回给用户的内容</li>
</ul>
<p>如果一个工具只有一个参数，可以直接使用<code>tool.run(&lt;参数&gt;)</code>调用工具，否则，调用工具时传入完整的参数字典，格式为<code>&#123;&lt;参数名&gt;: &lt;参数值&gt;&#125;</code>，例如：</p>
<ul>
<li><code>tool.run(&#123;&quot;query&quot;:&quot;langchain&quot;&#125;)</code></li>
</ul>
<h5 id="自定义工具"><a class="markdownIt-Anchor" href="#自定义工具"></a> 自定义工具</h5>
<p>创建自定义工具有多种方法，可能会需要导入如下的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.pydantic_v1 <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> BaseTool, StructuredTool, tool</span><br></pre></td></tr></table></figure>
<ul>
<li>使用@tool 装饰器
<ul>
<li>最简单的方法，只需要在函数定义前加上<code>@tool</code>修饰符</li>
<li>会默认使用函数名称作为工具名称，但可以通过传递字符串作为第一个参数来覆盖此名称。</li>
<li>函数需要具有一个python风格的docstring，这将成为<code>@tool</code>修饰符生成的工具的描述。</li>
<li>例如：</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply two numbers.&quot;&quot;&quot;</span>  <span class="comment"># 这个就是docstring</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(multiply.name)</span><br><span class="line"><span class="built_in">print</span>(multiply.description)</span><br><span class="line"><span class="built_in">print</span>(multiply.args)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出如下：</span></span><br><span class="line">multiply</span><br><span class="line">multiply(a: <span class="built_in">int</span>, b: <span class="built_in">int</span>) -&gt; <span class="built_in">int</span> - Multiply two numbers.</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: &#123;<span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;integer&#x27;</span>&#125;, <span class="string">&#x27;b&#x27;</span>: &#123;<span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;integer&#x27;</span>&#125;&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>覆盖定义工具属性的例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个类定义了一个参数输入的基本模式，用来覆盖原函数定义的参数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SearchInput</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    query: <span class="built_in">str</span> = Field(description=<span class="string">&quot;should be a search query&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 覆盖了工具的名称，参数（使用自定义的SearchInput）和return_direct属性</span></span><br><span class="line"><span class="meta">@tool(<span class="params"><span class="string">&quot;search-tool&quot;</span>, args_schema=SearchInput, return_direct=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">query: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Look up things online.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;LangChain&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(search.name)</span><br><span class="line"><span class="built_in">print</span>(search.description)</span><br><span class="line"><span class="built_in">print</span>(search.args)</span><br><span class="line"><span class="built_in">print</span>(search.return_direct)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出如下：</span></span><br><span class="line">search-tool</span><br><span class="line">search-tool(query: <span class="built_in">str</span>) -&gt; <span class="built_in">str</span> - Look up things online.</span><br><span class="line">&#123;<span class="string">&#x27;query&#x27;</span>: &#123;<span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;Query&#x27;</span>, <span class="string">&#x27;description&#x27;</span>: <span class="string">&#x27;should be a search query&#x27;</span>, <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;string&#x27;</span>&#125;&#125;</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<ul>
<li>继承BaseTool子类
<ul>
<li>子类化BaseTool类来显式定义自定义工具</li>
<li>自由度最高，但需要更多的代码量</li>
<li>需要先继承<code>BaseModel</code>定义工具需要传入的参数，然后继承<code>BaseTool</code>定义工具的其他属性</li>
<li>举个例子：还是乘法计算器</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Type</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.callbacks.manager <span class="keyword">import</span> (</span><br><span class="line">    AsyncCallbackManagerForToolRun,</span><br><span class="line">    CallbackManagerForToolRun,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Input Model时，参数名称为变量名，参数类型显示指定，使用一个Field对象定义可选的属性（标题，描述等）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CalculatorInput</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    a: <span class="built_in">int</span> = Field(description=<span class="string">&quot;first number&quot;</span>)</span><br><span class="line">    b: <span class="built_in">int</span> = Field(description=<span class="string">&quot;second number&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomCalculatorTool</span>(<span class="title class_ inherited__">BaseTool</span>):</span><br><span class="line">    <span class="comment"># 定义工具的基本属性</span></span><br><span class="line">    name = <span class="string">&quot;Calculator&quot;</span></span><br><span class="line">    description = <span class="string">&quot;useful for when you need to answer questions about math&quot;</span></span><br><span class="line">    args_schema: <span class="type">Type</span>[BaseModel] = CalculatorInput <span class="comment"># 设置参数时需要显示指定类型</span></span><br><span class="line">    return_direct: <span class="built_in">bool</span> = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写run函数，实现工具的执行</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_run</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, a: <span class="built_in">int</span>, b: <span class="built_in">int</span>, run_manager: <span class="type">Optional</span>[CallbackManagerForToolRun] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Use the tool.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写_arun函数，实现工具的异步执行</span></span><br><span class="line">    <span class="comment"># 如果工具不需要异步执行，可以直接返回异常</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">_arun</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        a: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        b: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        run_manager: <span class="type">Optional</span>[AsyncCallbackManagerForToolRun] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Use the tool asynchronously.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Calculator does not support async&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化一个自定义工具</span></span><br><span class="line">search = CustomSearchTool()</span><br><span class="line"><span class="built_in">print</span>(search.name)</span><br><span class="line"><span class="built_in">print</span>(search.args)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出如下：</span></span><br><span class="line">Calculator</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: &#123;<span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;description&#x27;</span>: <span class="string">&#x27;first number&#x27;</span>, <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;integer&#x27;</span>&#125;, <span class="string">&#x27;b&#x27;</span>: &#123;<span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;description&#x27;</span>: <span class="string">&#x27;second number&#x27;</span>, <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;integer&#x27;</span>&#125;&#125;</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<ul>
<li>继承 StructuredTool 数据类
<ul>
<li>相当于混合了前两种方法。</li>
<li>有一定的自由度，同时写法简单。</li>
<li>先定义好一个函数，然后使用<code>StructuredTool.from_function</code>方法，传入工具的属性即可。</li>
<li>举例：</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 工具要执行的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_function</span>(<span class="params">query: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;LangChain&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 工具参数会默认设为函数参数</span></span><br><span class="line">search = StructuredTool.from_function(</span><br><span class="line">    func=search_function,</span><br><span class="line">    name=<span class="string">&quot;Search&quot;</span>,</span><br><span class="line">    description=<span class="string">&quot;useful for when you need to answer questions about current events&quot;</span>,</span><br><span class="line">    <span class="comment"># coroutine= ... &lt;- 同样可以指定异步执行的方法</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>如果需要自定义参数覆盖函数的参数，可以指定<code>args_schema</code>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义参数类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CalculatorInput</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    a: <span class="built_in">int</span> = Field(description=<span class="string">&quot;first number&quot;</span>)</span><br><span class="line">    b: <span class="built_in">int</span> = Field(description=<span class="string">&quot;second number&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 工具要执行的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply two numbers.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line">calculator = StructuredTool.from_function(</span><br><span class="line">    func=multiply,</span><br><span class="line">    name=<span class="string">&quot;Calculator&quot;</span>,</span><br><span class="line">    description=<span class="string">&quot;multiply numbers&quot;</span>,</span><br><span class="line">    args_schema=CalculatorInput,</span><br><span class="line">    return_direct=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># coroutine= ... &lt;- 同样可以指定异步执行的方法</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="错误处理"><a class="markdownIt-Anchor" href="#错误处理"></a> 错误处理</h4>
<p>如果工具执行函数的过程种遇到异常并返回，<strong>正常情况下智能体会终止当前任务</strong>。<br />
如果希望智能体在遇到某些影响不大的异常后也可以继续执行，需要在函数执行体内抛出<code>ToolException</code> 并相应地设置 <code>handle_tool_error </code>。<br />
<strong>当工具抛出 <strong><code>ToolException</code></strong> 时，代理不会停止工作，而是根据工具的异常处理函数</strong><code>handle_tool_error</code><strong>处理异常，处理结果将作为观察返回给代理，并以红色打印。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.tools <span class="keyword">import</span> ToolException</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_tool1</span>(<span class="params">s: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="keyword">raise</span> ToolException(<span class="string">&quot;The search tool1 is not available.&quot;</span>)</span><br><span class="line"></span><br><span class="line">search = StructuredTool.from_function(</span><br><span class="line">    func=search_tool1,</span><br><span class="line">    name=<span class="string">&quot;Search_tool1&quot;</span>,</span><br><span class="line">    description=<span class="string">&quot;A bad tool&quot;</span>,</span><br><span class="line">    handle_tool_error=<span class="literal">True</span>, <span class="comment"># 添加异常处理属性标记</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">search.run(<span class="string">&quot;test&quot;</span>) </span><br><span class="line"><span class="comment">### 输出：</span></span><br><span class="line"><span class="string">&#x27;The search tool1 is not available.&#x27;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>如果只抛出了<code>ToolException</code> 异常而没有定义 <code>handle_tool_error</code>属性，则LangChain仍然不会让智能体处理异常。</li>
<li>当<code>handle_tool_error</code>属性仅定义为<code>True</code>而不是异常处理函数时，LangChain会将异常字符串作为处理结果返回给智能体。</li>
<li><code>handle_tool_error</code>属性定义为一个函数时，其需要接受<code>ToolException</code>类型的异常对象，并返回字符串。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_handle_error</span>(<span class="params">error: ToolException</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        <span class="string">&quot;The following errors occurred during tool execution:&quot;</span></span><br><span class="line">        + error.args[<span class="number">0</span>]</span><br><span class="line">        + <span class="string">&quot;Please try another tool.&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">search = StructuredTool.from_function(</span><br><span class="line">    func=search_tool1,</span><br><span class="line">    name=<span class="string">&quot;Search_tool1&quot;</span>,</span><br><span class="line">    description=<span class="string">&quot;A bad tool&quot;</span>,</span><br><span class="line">    handle_tool_error=_handle_error,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">search.run(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"><span class="comment">### 输出：</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">The following errors occurred during tool execution:</span></span><br><span class="line"><span class="string">The search tool1 is not available. Please try another tool.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="在openai聊天模型中使用工具"><a class="markdownIt-Anchor" href="#在openai聊天模型中使用工具"></a> 在OPENAI聊天模型中使用工具</h4>
<p>如果你只是需要一个工具函数，或是觉得LangChain中的内置工具很好用，但不需要一个智能体来决定工具的调用，可以将工具调用显式地嵌入ChatOpenAI聊天模型的提示中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.schema <span class="keyword">import</span> HumanMessage</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> MoveFileTool, format_tool_to_openai_function</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-0613&quot;</span>)</span><br><span class="line">tools = [MoveFileTool()]</span><br><span class="line">functions = [format_tool_to_openai_function(t) <span class="keyword">for</span> t <span class="keyword">in</span> tools]</span><br><span class="line"></span><br><span class="line">message = model.predict_messages(</span><br><span class="line">    [HumanMessage(content=<span class="string">&quot;move file foo to bar&quot;</span>)], functions=functions</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="自定义智能体"><a class="markdownIt-Anchor" href="#自定义智能体"></a> 自定义智能体</h3>
<p>智能体其实只是一个带有 工具调用和记忆的 LLM查询或对话系统，在底层完全可以使用LangChain的ICLE语法（即“链”）来实现。在下面的例子中，使用ChatOpenAI作为基础对话模型来实现自定义智能体。</p>
<ul>
<li>先定义好要使用的模型和工具</li>
<li>在对LLM提示的时候，记得加上<code>MessagesPlaceholder</code>,以实现和智能体的对话具有历史记忆能力。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> tool</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder</span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> AIMessage, HumanMessage</span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.convert_to_openai <span class="keyword">import</span> format_tool_to_openai_function</span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_word_length</span>(<span class="params">word: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns the length of a word.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(word)</span><br><span class="line"></span><br><span class="line">tools = [get_word_length] <span class="comment"># 定义工具</span></span><br><span class="line">chat_history = [] <span class="comment"># 记录历史</span></span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>, temperature=<span class="number">0</span>) <span class="comment"># 定义llm</span></span><br><span class="line"><span class="comment"># 将工具绑定到llm</span></span><br><span class="line">llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) <span class="keyword">for</span> t <span class="keyword">in</span> tools])</span><br><span class="line"></span><br><span class="line">MEMORY_KEY = <span class="string">&quot;chat_history&quot;</span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (</span><br><span class="line">            <span class="string">&quot;system&quot;</span>,</span><br><span class="line">            <span class="string">&quot;You are very powerful assistant, but bad at calculating lengths of words.&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">        MessagesPlaceholder(variable_name=MEMORY_KEY),</span><br><span class="line">        (<span class="string">&quot;user&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>),</span><br><span class="line">        MessagesPlaceholder(variable_name=<span class="string">&quot;agent_scratchpad&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>然后，从<code>langchain.agents.format_scratchpad</code>包中导入自定义创建智能体的基础类型。
<ul>
<li>这里，因为我们需要将函数调用转化为OPENAI api的形式，所以导入<code>format_to_openai_function_messages</code>函数，并同时导入<code>OpenAIFunctionsAgentOutputParser</code>来解析OPENAI 模型返回的输出。</li>
</ul>
</li>
<li>使用链式方法创建智能体，通常一个链条的构成是：
<ul>
<li>输入：即核心概念中的AgentAction，将“用户输入”，“中间步骤”和“历史”三个部分按照字典键值对的格式配置。
<ul>
<li>必要的时候（如使用OPENAI api的工具调用），要将输入转化为模型接口支持的格式。</li>
</ul>
</li>
<li>提示：按照已定义好的提示模板</li>
<li>已绑定好工具的LLM对象或聊天模型对象</li>
<li>解析输出的对象，根据你查询的模型决定。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents.format_scratchpad <span class="keyword">import</span> format_to_openai_function_messages</span><br><span class="line"><span class="keyword">from</span> langchain.agents.output_parsers <span class="keyword">import</span> OpenAIFunctionsAgentOutputParser</span><br><span class="line"></span><br><span class="line">agent = (</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="keyword">lambda</span> x: x[<span class="string">&quot;input&quot;</span>],</span><br><span class="line">        <span class="string">&quot;agent_scratchpad&quot;</span>: <span class="keyword">lambda</span> x: format_to_openai_function_messages(</span><br><span class="line">            x[<span class="string">&quot;intermediate_steps&quot;</span>]</span><br><span class="line">        ),</span><br><span class="line">        <span class="string">&quot;chat_history&quot;</span>: <span class="keyword">lambda</span> x: x[<span class="string">&quot;chat_history&quot;</span>],</span><br><span class="line">    &#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | llm_with_tools</span><br><span class="line">    | OpenAIFunctionsAgentOutputParser()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>最后，创建一个智能体执行器，配置好输入和历史即可执行：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor</span><br><span class="line"></span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">input1 = <span class="string">&quot;how many letters in the word educa?&quot;</span></span><br><span class="line">result = agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: input1, <span class="string">&quot;chat_history&quot;</span>: chat_history&#125;)</span><br><span class="line">chat_history.extend(</span><br><span class="line">    [</span><br><span class="line">        HumanMessage(content=input1),</span><br><span class="line">        AIMessage(content=result[<span class="string">&quot;output&quot;</span>]),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">input2 = <span class="string">&quot;is that a real word?&quot;</span></span><br><span class="line">agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: input2, <span class="string">&quot;chat_history&quot;</span>: chat_history&#125;)</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h4 id="agentexecutor-的额外参数"><a class="markdownIt-Anchor" href="#agentexecutor-的额外参数"></a> AgentExecutor 的额外参数</h4>
<ul>
<li><code>verbose=True</code>：是否输出详细执行信息</li>
<li><code>max_iterations=10</code>：限制最大迭代次数</li>
<li><code>max_execution_time=10</code>：限制最大执行时间（秒）</li>
<li><code>handle_parsing_errors=True</code>：是否自动处理解析错误（见下面的小节)</li>
<li><code>handle_parsing_errors=&quot;...&quot;</code>：处理解析错误，并使用自定义的字符串（见下面的小节)</li>
</ul>
<h3 id="智能体执行的监督和检查"><a class="markdownIt-Anchor" href="#智能体执行的监督和检查"></a> 智能体执行的监督和检查</h3>
<h4 id="按步骤执行"><a class="markdownIt-Anchor" href="#按步骤执行"></a> 按步骤执行</h4>
<p>调用<code>AgentExecutor</code>类对象的<code>iter</code>函数，获得每一个中间步骤的迭代器。由此，可以自由控制执行哪些步骤，并随时可以插入其他功能的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;What is the product of the 998th, 999th and 1000th prime numbers?&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> agent_executor.<span class="built_in">iter</span>(&#123;<span class="string">&quot;input&quot;</span>: question&#125;):</span><br><span class="line">    <span class="comment"># 这种写法等价于:</span></span><br><span class="line">    <span class="comment"># output = step.get(&quot;intermediate_step&quot;)</span></span><br><span class="line">    <span class="comment"># if output:</span></span><br><span class="line">    <span class="keyword">if</span> output := step.get(<span class="string">&quot;intermediate_step&quot;</span>):</span><br><span class="line">        action, value = output[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> action.tool == <span class="string">&quot;GetPrime&quot;</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Used Tool GetPrime，get number： <span class="subst">&#123;value&#125;</span> ...&quot;</span>)</span><br><span class="line">            <span class="keyword">assert</span> is_prime(<span class="built_in">int</span>(value))</span><br><span class="line">        <span class="comment"># 询问控制台用户是否继续Agent执行</span></span><br><span class="line">        _<span class="keyword">continue</span> = <span class="built_in">input</span>(<span class="string">&quot;Should the agent continue (Y/n)?:\n&quot;</span>) <span class="keyword">or</span> <span class="string">&quot;Y&quot;</span></span><br><span class="line">        <span class="keyword">if</span> _<span class="keyword">continue</span>.lower() != <span class="string">&quot;y&quot;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h4 id="获得中间步骤的日志"><a class="markdownIt-Anchor" href="#获得中间步骤的日志"></a> 获得中间步骤的日志</h4>
<p>智能体执行完毕后，<code>agent_executor.invoke</code>会返回一个对象，其中包含所有执行历史的情况，其结构如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">response = agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;What is Leo DiCaprio&#x27;s middle name?&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO</span></span><br></pre></td></tr></table></figure>
<p>要获得每一个中间步骤的详细情况，获取<code>response[&quot;intermediate_steps&quot;]</code>即可，这是一个列表，其中每个元素是一个元组，包含两个对象，分别是：</p>
<ul>
<li>一个<code>AgentActionMessageLog</code>对象，记录当前步骤调用工具的详细信息（包括工具名称，参数，日志字符串，实际嵌入LLM提示中的<code>AIMessage</code>对象等）
<ul>
<li>如果步骤是“输出结果返回给用户”，则这里会是一个<code>AgentFinish</code>对象。</li>
</ul>
</li>
<li>一个字符串，为调用工具返回的“Observation”</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[(</span><br><span class="line">    AgentActionMessageLog(</span><br><span class="line">        tool=<span class="string">&#x27;Wikipedia&#x27;</span>, </span><br><span class="line">        tool_input=<span class="string">&#x27;Leo DiCaprio&#x27;</span>, </span><br><span class="line">        log=<span class="string">&#x27;\nInvoking: `Wikipedia` with `Leo DiCaprio`\n\n\n&#x27;</span>, </span><br><span class="line">        message_log=[</span><br><span class="line">            AIMessage(</span><br><span class="line">                content=<span class="string">&#x27;&#x27;</span>, </span><br><span class="line">                additional_kwargs=&#123;</span><br><span class="line">                    <span class="string">&#x27;function_call&#x27;</span>: &#123;</span><br><span class="line">                        <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Wikipedia&#x27;</span>, </span><br><span class="line">                        <span class="string">&#x27;arguments&#x27;</span>: <span class="string">&#x27;&#123;\n  &quot;__arg1&quot;: &quot;Leo DiCaprio&quot;\n&#125;&#x27;</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">        ]</span><br><span class="line">    ), </span><br><span class="line">     <span class="string">&#x27;Page: Leonardo DiCaprio\nSummary: Leonardo Wilhelm DiCaprio (; Italian: [diˈkaːprjo]; born November 1&#x27;</span></span><br><span class="line">)]</span><br></pre></td></tr></table></figure>
<h4 id="自我检查错误"><a class="markdownIt-Anchor" href="#自我检查错误"></a> 自我检查错误</h4>
<blockquote>
<p>注意：这和之前介绍的工具类的错误处理不同，前者只是处理调用工具时，工具可能发生的错误。而这里是从智能体的层面上，防止LLM在输出对任务的推理时出现幻觉，产生不符合规定格式的字符串（如json格式，ReAct范式等）</p>
</blockquote>
<p>在实例化任意 <code>AgentExecutor</code>的时候加入参数<code>handle_parsing_errors=True</code>，可以使LangChain自动处理智能体的不当输出，这主要包括无效的格式或不完整的回应。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">agent_executor = AgentExecutor(</span><br><span class="line">    agent=agent, tools=tools, verbose=<span class="literal">True</span>, handle_parsing_errors=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>例如，如果Agent使用“ReAct&quot;策略，但LLM并未遵循提示输出“Action: ”字段时，LangChain将向LLM自动重发一条消息，告知错误格式的位置的应当的输出，以此尝试引导LLM纠正错误。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt; Entering new AgentExecutor chain...</span><br><span class="line"></span><br><span class="line"># LLM的返回，缺少 Action: 字段</span><br><span class="line">Thought: I should search for &quot;Leo DiCaprio&quot; on Wikipedia</span><br><span class="line">Action Input: Leo DiCaprio</span><br><span class="line"></span><br><span class="line"># LangChain生成的错误反馈，将作为下一步的Input自动输入给LLM</span><br><span class="line">Invalid Format: Missing &#x27;Action:&#x27; after ...</span><br><span class="line"></span><br><span class="line"># LLM的下一步返回，这次的格式正确</span><br><span class="line">Thought:I should search for &quot;Leonardo DiCaprio&quot; on Wikipedia</span><br><span class="line">Action: Wikipedia</span><br><span class="line">Action Input: Leonardo DiCaprio</span><br><span class="line"></span><br><span class="line"># 正确执行工具调用后，返回给LLM工具调用的输出结果</span><br><span class="line">Page: Leonardo DiCaprio </span><br><span class="line">Summary: Leonardo Wilhelm DiCaprio (; Italian: [diˈkaːprjo]; born November 1 ...</span><br></pre></td></tr></table></figure>
<p>如果要将自动生成的错误反馈修改为自定义的字符串，显示指定参数<code>handle_parsing_errors</code>时传入字符串而非布尔值即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">agent_executor = AgentExecutor(</span><br><span class="line">    agent=agent, tools=tools, verbose=<span class="literal">True</span>,</span><br><span class="line">    handle_parsing_errors=<span class="string">&quot;Check your output and make sure it conforms, use the Action/Action Input syntax&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="返回结构化输出"><a class="markdownIt-Anchor" href="#返回结构化输出"></a> 返回结构化输出</h4>
<p>上面的所有情况，智能体都以字符串输出结果，如果想要智能体也输出结构化的数据，以便结合到更大范围的系统中。<br />
我们可以将“返回给用户”这一步骤也视为一个工具调用，只不过调用的结果是输出结构化的文本。在LangChain种，可以定义Response类来实现这一功能。<br />
【注意：以下示例以基于OPENAI聊天模型的智能体为例】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Response</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Final response to the question being asked&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    answer: <span class="built_in">str</span> = Field(description=<span class="string">&quot;The final answer to respond to the user&quot;</span>)</span><br><span class="line">    sources: <span class="type">List</span>[<span class="built_in">int</span>] = Field(</span><br><span class="line">        description=<span class="string">&quot;List of page chunks that contain answer to the question. Only include a page chunk if it contains relevant information&quot;</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>然后，在定义工具时，使用函数<code>convert_pydantic_to_openai_function</code>将<code>Response</code>类定义为工具函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.convert_to_openai <span class="keyword">import</span> format_tool_to_openai_function</span><br><span class="line"><span class="keyword">from</span> langchain.utils.openai_functions <span class="keyword">import</span> convert_pydantic_to_openai_function</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">llm_with_tools = llm.bind(</span><br><span class="line">    functions=[</span><br><span class="line">        <span class="comment"># 其他工具，例如一个 retriever tool</span></span><br><span class="line">        format_tool_to_openai_function(retriever_tool),</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># Response 结构化返回工具</span></span><br><span class="line">        convert_pydantic_to_openai_function(Response),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>定义 解析Response工具输出的函数。这里我们没有使用预置智能体，因此引入名称<code>AgentFinish</code>和<code>AgentActionMessageLog</code>来包装LLM输出的中间步骤数据结构，以便得到可以被LangChain其他模块解析的对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> langchain_core.agents <span class="keyword">import</span> AgentActionMessageLog, AgentFinish</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">output</span>):</span><br><span class="line">    <span class="comment"># 如果LLM没有调用任何函数，将自定义回答结果直接包装为AgentFinish</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;function_call&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> output.additional_kwargs:</span><br><span class="line">        <span class="keyword">return</span> AgentFinish(return_values=&#123;<span class="string">&quot;output&quot;</span>: output.content&#125;, log=output.content)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果LLM要调用某个非Response类的函数，将其函数名和参数抽取出来</span></span><br><span class="line">    function_call = output.additional_kwargs[<span class="string">&quot;function_call&quot;</span>]</span><br><span class="line">    name = function_call[<span class="string">&quot;name&quot;</span>]</span><br><span class="line">    inputs = json.loads(function_call[<span class="string">&quot;arguments&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果LLM调用了Response函数准备返回结构化数据，将返回包装为AgentFinish。</span></span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">&quot;Response&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> AgentFinish(return_values=inputs, log=<span class="built_in">str</span>(function_call))</span><br><span class="line">    <span class="comment"># 否则，将返回包装为 agent action，这将使LangChain代替我们执行函数并继续下一步与智能体的交互。</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> AgentActionMessageLog(</span><br><span class="line">            tool=name, tool_input=inputs, log=<span class="string">&quot;&quot;</span>, message_log=[output]</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>最后，使用parse作为链步骤中解析智能体输出的动作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents.format_scratchpad <span class="keyword">import</span> format_to_openai_function_messages</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;You are a helpful assistant&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;user&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>),</span><br><span class="line">        MessagesPlaceholder(variable_name=<span class="string">&quot;agent_scratchpad&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">agent = (</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="keyword">lambda</span> x: x[<span class="string">&quot;input&quot;</span>],</span><br><span class="line">        <span class="comment"># Format agent scratchpad from intermediate steps</span></span><br><span class="line">        <span class="string">&quot;agent_scratchpad&quot;</span>: <span class="keyword">lambda</span> x: format_to_openai_function_messages(</span><br><span class="line">            x[<span class="string">&quot;intermediate_steps&quot;</span>]</span><br><span class="line">        ),</span><br><span class="line">    &#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | llm_with_tools</span><br><span class="line">    | parse</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">agent_executor = AgentExecutor(tools=[retriever_tool], agent=agent, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">agent_executor.invoke(</span><br><span class="line">    &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;...&quot;</span>&#125;, return_only_outputs=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="工具包toolkits"><a class="markdownIt-Anchor" href="#工具包toolkits"></a> 工具包：Toolkits</h3>
<p>对于许多常见任务，代理将需要一组相关的工具。为此，LangChain提供了工具包的概念——实现特定目标所需的大约3-5个工具的组合。<br />
使用工具包非常方便，只需导入工具包的名称，然后使用<code>get_tools()</code>方法即可返回一个列表，包含该工具包中的所有工具，你可以直接将该列表作为参数传入实例化Agent的方法中，轻松愉快！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize a toolkit</span></span><br><span class="line">toolkit = ExampleTookit(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get list of tools</span></span><br><span class="line">tools = toolkit.get_tools()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create agent</span></span><br><span class="line">agent = create_agent_method(llm, tools, prompt)</span><br></pre></td></tr></table></figure>
<h4 id="langchain内置工具包"><a class="markdownIt-Anchor" href="#langchain内置工具包"></a> Langchain内置工具包</h4>
<p><strong>所有内置工具包的使用文档：</strong><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/toolkits">https://python.langchain.com/docs/integrations/toolkits</a></p>
<h3 id="cot-tot-和-思维图的实现"><a class="markdownIt-Anchor" href="#cot-tot-和-思维图的实现"></a> COT TOT 和 思维图的实现</h3>



<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2024/12/29/Nickbit%202024%E5%BD%B1%E8%A7%86%E6%B8%B8%E8%89%BA%E6%80%BB%E7%BB%93/">Nickbit 2024影视游艺总结</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2023/12/31/Nickbit2023%E5%BD%B1%E8%A7%86%E6%B8%B8%E8%89%BA%E6%80%BB%E7%BB%93/">Nickbit 2023影视游艺总结</a></div></section></div>






  <div class='related-wrap md-text reveal' id="comments">
    <section class='header cmt-title cap theme'>
      快来参与讨论吧
    </section>
    <section class='body cmt-body waline'>
      

<div id="waline_container" class="waline_thread"><svg class="loading" style="vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg></div>

    </section>
  </div>



      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>本站由 <a href="/">@anonymity</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建。<br />
本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  const stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = stellar.config.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.version = '1.19.0';
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.19.0';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
    root : '/',
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://gcore.jsdelivr.net/npm/jquery@3.6.2/dist/jquery.min.js'
  };

  if ('local_search') {
    stellar.search = {};
    stellar.search.service = 'local_search';
    if (stellar.search.service == 'local_search') {
      let service_obj = Object.assign({}, {"field":"all","path":"/search.json","content":true,"codeblock":true,"sort":"-date"});
      stellar.search[stellar.search.service] = service_obj;
    }
  }

  // stellar js
  stellar.plugins.stellar = Object.assign({"sites":"/js/plugins/sites.js","friends":"/js/plugins/friends.js","ghinfo":"/js/plugins/ghinfo.js","timeline":"/js/plugins/timeline.js","linkcard":"/js/plugins/linkcard.js","fcircle":"/js/plugins/fcircle.js","weibo":"/js/plugins/weibo.js"});

  stellar.plugins.marked = Object.assign("https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js");
  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/vanilla-lazyload@17.8.3/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.css","js":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://gcore.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://gcore.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://gcore.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti@0.9.2/umd/heti.min.css","js":"https://unpkg.com/heti@0.9.2/umd/heti-addon.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.copycode = Object.assign({"enable":true,"js":"/js/plugins/copycode.js","default_text":"Copy","success_text":"Copied"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->

  <script>
  function load_comment(){
    if(!document.getElementById("waline_container"))return;
    stellar.loadCSS('https://unpkg.com/@waline/client@2.14.1/dist/waline.css');
    stellar.loadScript('https://unpkg.com/@waline/client@2.14.1/dist/waline.js', {defer:true}).then(function () {
      const el = document.getElementById("waline_container");
      var path = el.getAttribute('comment_id');
      if (!path) {
        path = decodeURI(window.location.pathname);
      }
      Waline.init(Object.assign({"js":"https://unpkg.com/@waline/client@2.14.1/dist/waline.js","css":"https://unpkg.com/@waline/client@2.14.1/dist/waline.css","serverURL":"https://www.nicenick.live","commentCount":true,"pageview":true,"emoji":["https://unpkg.com/@waline/emojis@1.1.0/bilibili"]}, {
        el: '#waline_container',
        path: path,
        
      }));
    });
  }
  window.addEventListener('DOMContentLoaded', (event) => {
    console.log('DOM fully loaded and parsed');
    load_comment();
  });

</script>




<!-- inject -->


  </div>
</body>
</html>
