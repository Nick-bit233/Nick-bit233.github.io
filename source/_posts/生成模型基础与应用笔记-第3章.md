---
title: 【笔记】生成模型基础与应用 - 第3章
date: 2023-11-27 14:54:53
year: 2023
tags: [生成模型, 笔记]
katex: true
---
<meta name="referrer" content="no-referrer" />

C3 主题模型：主题模型是一类用于文本分析的非监督学习方法，旨在从文本数据中发现隐藏的主题结构......

<!-- more -->

## C3 主题模型
**Topic models**：以非监督学习的方式对文本的隐含结构进行发现或生成的模型
> 主题模型是一类用于**文本分析**的非监督学习方法，旨在从文本数据中发现隐藏的主题结构。这些主题模型的目标是识别文本集合中的主题，而无需事先标记的主题标签或监督信息。主题模型最常见的应用之一是用于文本数据的主题建模，其中文档被看作是多个主题的混合，而每个主题又由一组词汇表示。

主题模型的发展：

- LSA
- PLSA
- LDA 2003
- HDP 2005
### 单词向量空间

- 将文档中每个单词的出现的频数（或加权的频数）表示为向量
   - 可以事先定义 **有效单词**的 **语料库**，只提取文档中有效单词的出现频率生成向量（也可以视为不在语料库中的单词其权值为0），所有的语料库中的有效单词用**集合W**表示
   - 一个文档就表示成：$d=(f_{w_1},f_{w_2},....,f_{w_N})$，每个$f_i$都是有效单词$i$出现的次数
- 一个文档集中的所有文档（设一共有N个文档组成了**集合D**）组成了一个向量集合，即 **单词向量空间**
- 对于两个不同的文档$d_i,\ d_j$，可以使用它们之间的数学度量，来表示文本之间的语义相似度：
   - 计算方法可以为 文档向量的 **内积** 或 **标准化内积**
- 单词向量空间的表示方法：
   - 1、单词-文本矩阵：将单词在每个文档的出现频率向量作为列向量，组成的矩阵。通常为稀疏矩阵

单词-文本矩阵的例子：![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698137498203-3dbd1471-0a04-4382-86fe-747431990568.png#averageHue=%23eeeeee&clientId=u282752da-b69c-4&from=paste&height=315&id=ubdb7149f&originHeight=520&originWidth=664&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=259496&status=done&style=none&taskId=ucf91a161-b004-4f5c-8a72-5af695055ce&title=&width=402.424219164749)

      - 计算机处理稀疏矩阵，非常浪费算力，因此，需要一个等价的数据结构来表示相同给信息，于是提出了下面这种表示方法。
   - 2、单词频率-逆文本频率（TF-IDF）： 统计单词$w_i$在文本$d_k$中出现的权值 ：

$TF-IDF(w_i,d_k) = \frac{f(w_i,d_k)}{len(d_k)}log\frac{N}{df(w_i)}$

         - tf(w,b)：单词w在文本b中的出现频数
         - len(b)：文本b中的单词总数
         - df(w)：整个文本集合中，含有单词w的文本数
         - N：整个文本集合的大小（含有的文本数量）

单词向量空间的优缺点优点：模型简单、计算效率高缺点：内积计算的相似度不一定能准确表达两个文档间的相似度
### 话题向量空间

- 定义话题：假设所有的文本中一共含有K个话题
- 假设每个话题都是一个M维度的向量$t$，$t$由语料库（**集合W**）中的M个单词组成。

$$t = (w_1,w_2,...,w_M)^T$$

   - 这样，一共K个话题就组成了一个话题向量空间，记为$T = [t_1,t_2,...,t_k]=[w_{11}\ w_{12} ... \ w_{MK}]$【单词-话题矩阵】
- 根据单词向量空间的定义，可以推导出：
   - 假设一篇文本$x$在单词向量空间中被表示为：$x = (f_{w_1},f_{w_2},....,f_{w_N})$，而在话题空间中被表示为：$$y = g(x) = (f_{t_1},f_{t_2},...,f_{t_k})$$
   - 为了方便，我们将话题的出现频率 记为$f_{t_1} = y_1, f_{t_2} = y_2...$这样，一个文档x，它可以表示为所有话题和单词的线性组合：

$$x = t_1y_1 + t_2y_2 + ... t_ky_k$$

   - 如果我们定义**话题-文本**矩阵Y，即将每个话题表示的文本$y$组合起来，则由上述的线性组合关系，对整个文档集合D，即可由话题向量空间的 **话题矩阵T** 来表示。
   - 因此，**单词-文本矩阵**、**单词-话题矩阵**和**话题-文本矩阵**具有如下的关系：

$$X_{MN} = T_{MK}Y_{KN}$$
**潜在语义分析**即是 将文本在 **单词向量空间的表示** 通过 **线性变换** 转换为** 在话题向量空间中的表示**的方法
### 期望最大化算法
在讲潜在语义分析之前，必须要了解一下什么是贝叶斯概率模型的潜变量，以及对潜变量模型的估计算法。期望最大化算法（Exception Maximization Algorithm，简称EM算法）是一种启发式的迭代算法，用于对**含有隐变量的概率模型** 的参数做 **极大似然估计** 
> 假设我们有这样一个概率模型：
> - 有三枚硬币$A,B,C$，抛掷它们正面朝上的概率不同，记为$r,p,q$
> - 重复执行以下试验（Ex(X)表示对事件X做一次实验，1表示正面向上，0表示反面向上）：
> $$Ex(Y)= Ex(B) \ if \ Ex(A)==1 \ else \ Ex(C)$$
> - 即抛硬币A，如果是正面抛硬币B，否则抛硬币C，**你只能观测到最后一次抛掷硬币的结果**。
> - 那么如何估计三个参数$r,p,q$呢？
> - 因为这个随机事件包含两个概率模型的 复合。因此我们无法直接对结果建模，数学家们由此提出，能否加入“**隐变量**”$Z$： 
> - - 用它来表示试验中抛掷A硬币的中间结果（虽然我们并不能直接观测到），假设共抛了n次，第i次观测结果的值记作$z_i$
> -假设我们只看最终结果Y，将参数表示为向量$\theta = (r,p,q)$，则实际要观测的事件Y的似然可以表示为：
> $$P(Y|\theta) = P(Y,Z|\theta)= \sum_{i=1}^{n}P(y_i,z_i|\theta) = \sum_{i=1}^{n}P(z_i|\theta)P(y_i|z_i,\theta) \\= rp^{\sum(y_i=1)}(1-p)^{\sum(y_i=0)}+(1-r)q^{\sum(y_i=1)}(1-q)^{\sum(y_i=0)}$$
> 直接使用MLE估计这个似然函数？不行，包含三个未知变量，且需要求log和的极大值，算不出解析解。

既然没有准确的解析解，数学家于是提出了用迭代法逼近最大值的求解办法，这就是EM算法。我们先定义好两种情况的似然函数，既然要求极大似然，我们对似然函数直接取log，定义**对数似然函数**：

- 不将隐变量Z视为随机变量的情况【称为：不完全数据】
$$L_Y(\theta) = log[\prod_{i=1}^nP(y_i|\theta)] = \sum_{i=1}^nlog[\sum_{z_i \in \ Ex(Z)}P(y_i,z_i|\theta)]$$
- 将隐变量Z视为随机变量的情况【称为：完全数据】
$$L_{Y,Z}(\theta) = log[\prod_{i=1}^nP(y_i,z_i|\theta)] = \sum_{i=1}^nlog[P(y_i,z_i|\theta)]$$

我们假设$\theta$从一个初始值$\theta_0$开始，每次迭代的时候更新（下标+1），那么我们希望 **在已知观测结果Y的情况下，让完全数据的对数似然函数**$L_{Y,Z}(\theta)$**尽可能取得最大的期望**，由此定义Q函数：
$$Q(\theta,\theta_t) = E[L_{Y,Z}(\theta)|Y,\theta_t] = \sum_{z_i \in \ Ex(Z)}{P(Z|Y,\theta_t)log[P(Y,Z|\theta)]}$$
其中，$\theta_t$是第t步更新的参数，Q是关于$\theta$的函数，让它最大，则需要对$\theta$求偏导，并取导数为0。此时$\theta_t$取一个新值，即：
$$
\theta_{t+1} = argmax_\theta{Q(\theta,\theta_t)}
$$【MLE】
这是采用极大似然估计的方法，如果你说要考虑先验概率（即MAP方法），那么每次更新是需要加上先验函数的对数值：
$$
\theta_{t+1} = argmax_\theta{Q(\theta,\theta_t)} + logP(\theta)
$$【MAP】

> 现在我们试着对刚才的问题应用EM算法：
> - 先假设$\theta^{(0)} = (0.5,0.5,0.5)$（为了下标不打架，我们用上括号标记步数t）
> - 则$P(Z|Y,\theta_t)$表示：
> - 假设第$i$次的最终**观测结果是**$y_i(=1 \ or\ 0)$**，这个结果是由 抛硬币B**$(z_i=1)$** 还是 抛硬币C**$(z_i=0)$** 得到的概率，将其记为**$p_i$**，**我们可用直接以伯努利实验的结果计算：
> $p_i = \frac{P(y_i,z_i=1)}{P(y_i,z_i=0)+P(y_i,z_i=1)}=\frac{rp}{rp+(1-r)q}(y_i=1) \ or \ \frac{r(1-p)}{r(1-p)+(1-r)(1-q)}(y_i=0)$**【对于每一步i】**
> - **然后求其关于Z的期望，即：**
> $$Q(\theta) = \sum_{i=1}^{n}p_i\log[P(y_i=1,Z|\theta)] + (1-p_i)\log[P(y_i=0,Z|\theta)]$$
> - 注意：$p_i$**与每一步要更新的**$\theta_t$**有关，因此其表示为的**$r,p,q$**也需要每一步更新。**而$log[P(Y,Z|\theta)]$则直接表示为上述的似然函数，注意它与要逼近的$\theta_t$无关，可以直接表示为$r,p,q$的式子，故：
> $$ Q(\theta,\theta_t)=\sum_{i=1}^{n}p_i^{(t-1)}\log[rp^{y_i}(1-p)^{1-y_i}] + (1-p_i^{(t-1)})\log[(1-r)q^{y_i}(1-q)^{1-y_i}] $$
> - 对该函数的$r,p,q$分别求偏导，即可得到：
> $$r^{(t+1)} = \frac{\sum_{i=1}^{n}p_i^{(t)}}{n}$$
> $$p^{(t+1)} = \frac{\sum_{i=1}^{n}p_i^{(t)}y_i}{\sum_{i=1}^{n}p_i^{(t)}}$$
> $$q^{(t+1)} = \frac{\sum_{i=1}^{n}(1-p_i^{(t)})y_i}{\sum_{i=1}^{n}(1-p_i^{(t)})}$$

问题： 为什么EM算法能近似实现对观测数据 的极大似然估计？  答：可以通过数学证明：可以通过$L(\theta)$函数构造Q函数，当作为中间函数时，可以证明在迭代过程中极大对数似然函数$L(\theta)$的下界单调增加，即：$L(\theta_{t+1})  \geq L(\theta_t)$
> 证明就省略啦~

### 概率潜在语义分析（PLSA）
 是一种利用概率生成模型 对 文本集合进行话题分析的 无监督学习方法  假设：

- 每个文本由一个话题的分布决定
- 每个话题由一个单词的分布决定
- 因此，从给定文本的表示到生成单词，就是一个概率模型，其中**话题是隐变量**
> 举个例子：假设一个离散空间中，【话题】和其对应的单词的概率分布如下：
> 【教育】 = {大学：0.5，老师：0.3，课程：0.2 }
> 【经济】 = {市场：0.4，企业：0.2，金融：0.4}  
> 【交通】 = {高铁：0.5，汽车：0.2，飞机：0.3}  
> 而对于一个挖了空位的文本，其每个位置对应话题的概率分布如下：
>  D(xx场景下如何看待xx和xx的关系) = {教育：0.5，经济：0.3，交通：0.2}  
> 那么，生成 “**大学**场景下如何看待**大学**与**企业**的关系”的概率  是：
>  $[P( t = 教育 )P(w = 大学|t = 教育) 
]^2× \\P(t = 经济) P(w = 企业 |t = 经济 )
=0.00375$

假设文本的集合是$D=\{d_1,d_2,...,d_N\}$单词的集合是：$W=\{w_1,w_2,...,w_M\}$话题的集合是：$Z=\{z_1,z_2,...,z_K\}$我们可以看到，在求文本的生成分布时，涉及到以下的条件概率：$P(z|d)$: 已知文本d，生成话题z的概率，是一个**多项分布**$P(w|z)$: 已知话题z，生成单词w的概率，也是一个**多项分布**而$P(d)$表示从所有文本集合中，随机选取一个文本的d的概率

现在我们希望知道，给定(单词，文本)对，它在这个空间中的生成的概率，即：
$$P(X) = \prod_{(w,d)\ \in \ W×D} P(w,d)^{cnt(w,d)}$$
，其中所有的$(w,d)$对应该有N×L（文本数×每个文本中要填空的单词数）个根据上面例子的思想，对于每个$(w,d)$对，其生成概率又可以用隐变量写为：
$$P(w,d) = P(d)P(w|d) = P(d) \sum_{z\ \in \ Z}P(w,z|d) = P(d) \sum_{z \ \in \ Z}P(z|d)P(w|z)$$
于是我们可以建立模型，对$P(z|d)$和$P(w|z)$分别进行估计，得到了这样的单层隐变量网络，即PLSA：![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698739378431-1588de4b-e0fa-4b92-9e93-41b00069028f.png#averageHue=%23ececec&clientId=ufc18b360-4daa-4&from=paste&height=170&id=u3e28fee4&originHeight=281&originWidth=320&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=51310&status=done&style=none&taskId=uc6222b97-991a-41af-878d-f7b93bcd9c6&title=&width=193.9393827299995)其中左侧的参数量是 NK，右侧为MK
> 现实中K远小于M，所以PLSA通过话题 对数据进行了更简洁地表示，减少了学习过程中过拟合的可能性  

继续使用极大似然估计的方法，试图求出让P(X)最大时的参数，取对数似然（下面的公式推导中，我们省略了长度是$O(NK+MK)$的参数向量θ，不然就太长了太难读了QAQ）：
$$
L_{W,Z,D} = log\prod_{(w,d)\ \in \ W×D} P(w,d)^{cnt(w,d)} \\ 
= log \prod_{i=1}^M \prod_{j=1}^NP(w_i,d_j)^{cnt(w_i,d_j)} \\
= \sum_{i=1}^M\sum_{j=1}^Ncnt(w_i,d_j)logP(w_i,d_j) \\
= \sum_{i=1}^M\sum_{j=1}^Ncnt(w_i,d_j)[logP(d_j) + log \sum_{k=1}^KP(w_i|z_k)P(z_k|d_j)]
$$
仍然是非常难计算，因此要使用上一节介绍的EM估计算法，为此我们计算Q函数。上面的似然函数$L_{W,Z,D}$已经是一个【完全数据】，隐藏变量Z被视为与W,D并列的随机变量。因此Q函数是每个$(w,d)$对为条件下，$L_{W,Z,D}$的期望，即：
$$Q = P(z_k|w_i,d_j)[\sum_{k=1}^K\sum_{i=1}^M\sum_{j=1}^Ncnt(w_i,d_j)logP(w_i,d_j,z_k)]$$
其中，$$P(w_i,d_j,z_k) = P(d_j)P(w_i|z_k)P(z_k|d_j)$$
就是按照“链式法则”生成一堆单词文本的联合概率，而$P(z_k|w_i,d_j)$则是**隐藏变量Z不被视为条件的**分布概率【不完全数据】，即：
$$P(z_k|w_i,d_j) = \frac{P(w_i|z_k)P(z_k|d_j)}{\sum_{k=1}^KP(w_i|z_k)P(z_k|d_j)}$$
这样，我们可以发现，要每一步更新的参数其实是$P(w_i|z_k)$和$P(z_k|d_j)$
因为我们已经有一个数据集，$P(d_j)$的值（即先验）可以直接由统计方法估计出来，cnt函数（即哪些对实际在数据集中同时出现了也是可以统计的已知量）。因此，EM近似的步骤为：

- 对于每个下标对(i,k)和(k,j)，分别求$P(w_i|z_k)$和$P(z_k|d_j)$的偏导数（第一步时为每个概率取初始值（如均匀分布），只要满足概率和为1即可）
- 然后将导数为0的值求出来，更新(i,k)和(k,j)对应参数向量的元素
- 取 k=k+1，求下一步的不完全数据，直到收敛

在用统计估计$P(d_j)$后，局部最优解的结果经过推导是：![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698741408380-c0e90139-8770-4324-bded-ae4e02fa9010.png#averageHue=%23f8f5f5&clientId=ufc18b360-4daa-4&from=paste&height=271&id=u7783056a&originHeight=407&originWidth=643&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=67826&status=done&style=none&taskId=u0b0867b8-0791-4362-b734-24576c13568&title=&width=428.6666666666667)PLSA的方法仍然胜在离散空间，计算复杂度低，可以快速迭代，但存在一个很重要的缺点：因为所学习的参数不仅依赖于单词库W，还依赖于文档数据集D，所有其可以生成其所在数据集的文档的模型，但**却不能生成新文档的模型**
### 潜在狄利克雷分布（LDA）
在对PLSA建模时，我们发现一个规律： **文本由话题的一个多项分布表示** ， **话题也由单词的一个多项分布表示**
> 以防你忘了我们在C1中介绍的多项分布，补充一下完整定义：
> ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698746598881-5de86cdb-810a-48e4-a197-24fa60ede82d.png#averageHue=%23f7f7f7&clientId=ufc18b360-4daa-4&from=paste&height=276&id=u8890defc&originHeight=414&originWidth=908&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=113477&status=done&style=none&taskId=u4cf54a77-7953-4d4c-9dd7-eddc3e08cf5&title=&width=605.3333333333334)
> ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698746618388-f604ee58-d992-4488-b24d-f639583c104d.png#averageHue=%23f6f6f6&clientId=ufc18b360-4daa-4&from=paste&height=247&id=u70d46e0a&originHeight=370&originWidth=920&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=99956&status=done&style=none&taskId=ud639c26e-3233-4915-bf15-78cb0c4194a&title=&width=613.3333333333334)

这不巧了嘛，我们在C2中提到，多项分布的贝叶斯生成模型，可以用 狄利克雷分布 作为先验。因此，可以假设 话题分布 和 单词分布 的先验都是 狄利克雷分布：

- 对于每个文本$d_m$，假设d中包含的单词组成了向量$d_m = \bold w_m \ (m=1,2,...,M)$,认为生成该文本话题的概率$P(z|\bold w_m) = \theta_m \sim Dir(\alpha)$
   - $\theta_m$参数向量有K个值，每个值表示文本$d_m$能生成对应话题$z_1,z_2,...,z_k$的概率
   - 因此，超参数$\alpha$也是一个K维向量
>    - 注意，在这里我们为了对每个单词写出一个话题分布，将文本的定义改成了单词组成的向量。因此代表文本集中的文本数量的常量不再是$N$，而是$M$。用$N_i(i=1,2,...,M)$来表示一个文本中的单词总数。而下面的部分中，我们假设整个单词库中的单词总数是$V$

- 对于每个话题$z_k\ (k=1,2,...,K)$，认为生成相应单词的概率$P(w|z_k) = \phi_k \sim Dir(\beta)$
- 而对于每个单词$w_m[n] \in \bold w_m \  (n=1,2,...,N_m)$，其对应的话题和单词都**链式地**服从多项分布：
   - $z_m[n] \sim Mult(\theta_m)$：先随机根据概率生成一个话题序列
   - $w_m[n]  \sim  Mult(\phi_{z_m[n]})$，再对每个话题，随机生成一个单词序列，共生成m个

在这个定义下，我们可以直接得到所有先验的表示，因此，可以使用最大后验概率估计MAP来求解参数，记住在这里w是观测量，z是隐变量，因此后验是已知观测量算隐变量的概率：$文本w的后验=P(\bold{\theta}, \bold z|\bold w, \alpha,\beta) = \frac{P(\bold w,\bold z,\theta,\phi|\alpha,\beta)}{P(\bold w|\alpha,\beta)}$然后，就像PLSA方法一样，根据向量的嵌套定义把概率计算拆成每个元素概率的乘积，式子很长，直接贴一下结果吧：![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698747831489-f09069cb-bba6-4eff-b002-ca123025dc5a.png#averageHue=%23f4f4f4&clientId=ufc18b360-4daa-4&from=paste&height=149&id=u62dfb3ab&originHeight=223&originWidth=864&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=57844&status=done&style=none&taskId=u4837be10-1c54-4122-ad02-90ac77cbb57&title=&width=576)自然，想对这种玩意求解析最大值，和自杀没什么区别。不过，既然这次我们估计的是后验概率分布，概率论中可以以 **变分推断** 的方法来处理
#### 变分推断
所谓变分推断，是取一个用隐变量z的分布$q(z)$来近似后验概率的条件分布$p(z|x)$的方法。为了比较二者的分布相似度，使用KL散度来计量：

- 如果能找到与$p(z|x)$在KL散度意义下最近的分布 $\hat q(z)$，则可以用这个分布近似后验概率分布

![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698749267860-10c1511c-1bcc-4bc9-bf1b-bf22d09ca40c.png#averageHue=%23fbfbfb&clientId=ufc18b360-4daa-4&from=paste&height=170&id=u4c14e519&originHeight=255&originWidth=551&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=30116&status=done&style=none&taskId=u39010661-b80a-4df6-b607-5a52d0fcf95&title=&width=367.3333333333333)
带入KL散度的公式：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698749295560-dcee8958-865e-4a91-988d-40aa2b7abe28.png#averageHue=%23f4f4f4&clientId=ufc18b360-4daa-4&from=paste&height=115&id=ubc9973b6&originHeight=172&originWidth=757&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=54966&status=done&style=none&taskId=u19149e84-eddd-4de6-a429-cf680f23e0a&title=&width=504.6666666666667)

则，如果想要KL散度更小，必有
$$
log\ p(x) \geq E_q[log\ p(x,z)] - E_q[log\ q(z)]
$$
因为先验x的分布$log\ p(x)$可以被视为常量，因此右侧关于q分布的期望式越大，KL散度就越小。数学上把右侧称为**证据下界**。因此，下面问题就变为求证据下界的最大化。还有一个假设，是对于隐变量z（向量），假设分布$q(z)$对 z 的所有分量都是独立的，即：
$$
q(z) = q(z_1)q(z_2),...,q(z_m)
$$
称其为平均场。

现在让我们回到LDA模型的最大后验概率估计，现在我们有了变分推断算法，可以定义“文本w的后验” 的证据下界：
$$
L(r,t,\alpha,\phi) = E_q[log\ p(\bold z, \bold w)] -E_q[log\ q(\bold z)]\\ = E_q[log\ p(\theta,\bold z, \bold w| \alpha,\phi)] -E_q[log\ q(\theta,\bold z|r,t)]
$$
其中，向量r和t是变分参数，r来估计隐变量z在单词向量中的分布参数$\theta$，t来估计话题向量中的分布参数$(z_1,z_2,...,z_n)$有了平均场假设，就可以对每个文本分来计算，得到所有文本的证据下界：
$$
L'(r,t,\alpha,\phi) =\sum_{m=1}^M{  E_q[log\ p(\theta_m,\bold z_m, \bold w_m| \alpha_m,\phi_m)] -E_q[log\ q(\theta_m,\bold z_m|r_m,t_m)]}
$$
此时我们发现它也是一个 **离散的期望最大化估计** 问题了，可以使用第二节提到的 EM算法来迭代更新参数，此时有四个参数向量，两个是为了变分推断引入的，另外两个则为模型建立的狄利克雷分布参数。
> 注意，实际上狄利克雷分布的参数是$\alpha 、\beta$，不过因为话题的分布参数$\phi$可以直接由参数$\beta$根据话题数 k 得到，因此这里简化一下模型，直接估计参数向量$\phi$

![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698750789928-c298eb01-d66f-471a-840f-2183e3d3a47f.png#averageHue=%23f4f4f4&clientId=ufc18b360-4daa-4&from=paste&height=213&id=u0516f1b1&originHeight=337&originWidth=991&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=129949&status=done&style=none&taskId=u841c8c1f-ec74-4b8d-b42d-df062035910&title=&width=625.6666870117188)这种方法被综合称为 **变分EM算法**
####  LDA和PLSA的比较  

- 相同点：都将 话题建模为单词的多项分布，文本建模为话题的多项分布  
- 不同点：
   - PLSA没有使用先验分布（ 或者说假设先验分布是均匀分布  ），使用MLE估计。
   - 而LDA假设了狄利克雷分布作为先验分布，且使用MAP估计。
      - 有先验分布的好处和C2中提到的一样，可以防止**过拟合**问题

![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698750921909-2787ed65-349d-49ec-8a6a-c9a1e094e64b.png#averageHue=%23dad0b6&clientId=ufc18b360-4daa-4&from=paste&height=268&id=ue371411a&originHeight=402&originWidth=1069&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=510704&status=done&style=none&taskId=ud6edc293-0798-4848-a12b-a3ae3f36ec1&title=&width=712.6666666666666)