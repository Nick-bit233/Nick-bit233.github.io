---
title: 生成模型基础与应用笔记-第4章
date: 2023-12-19 20:18:23
tags: [生成模型, 笔记]
katex: true
---
<meta name="referrer" content="no-referrer" />

C4 连续数据的生成模型
> 本文的部分公式格式未校对，正在施工中……

<!-- more -->

## C4 连续数据的生成模型
### 生成分类器
在C2中，我们提到，生成模型也是一个分类模型，通过对离散变量的贝叶斯建模，可以得到 朴素贝叶斯分类器（NBC），事实上，这一点也可以推广到连续数据。只需将概率建模改为“类条件密度函数”：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698751386020-ce1dc3d3-34b9-4c56-9454-6cf5699219b3.png#averageHue=%23f8f7f7&clientId=ufc18b360-4daa-4&from=paste&height=253&id=u9ba55a6f&originHeight=443&originWidth=809&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=56764&status=done&style=none&taskId=ua205703f-aaa4-4654-8066-24051d5620d&title=&width=462.3333740234375)
记好这个结论，在后面两节我们会使用其来训练一个** 多元高斯分布** 分类器
### 单一正态分布的高斯模型
这一节我们专注于高斯分布（正态分布），因为它是连续变量中的最常见的建模。
我们先一句带过 **一元正态分布**，对没错就是这个：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1699336972408-2a3567e8-a512-4698-b4c6-a1e4d5597fe7.png#averageHue=%23fafafa&clientId=u0e63a603-818a-4&from=paste&height=147&id=nJ8Hw&originHeight=328&originWidth=471&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=72377&status=done&style=none&taskId=u9109833c-810e-4bb7-8d83-995c7d9bd74&title=&width=211)
它的概率密度函数是$N(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\mu\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$，大家应该都知道吧。
实际上，多元连续变量也是具有正态分布的（概率论里其实也学过，不过大概大家都忘了）：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1698751514341-2973d5db-f85e-46e3-869b-03d6c48f8e8e.png#averageHue=%23fbf8f8&clientId=ufc18b360-4daa-4&from=paste&height=240&id=u6eebc3a8&originHeight=424&originWidth=1056&originalType=binary&ratio=1.6500000953674316&rotation=0&showTitle=false&size=122634&status=done&style=none&taskId=u793cd86b-188e-4893-ad23-bacb36a7944&title=&width=596.6666870117188)
其中，我们把 $(x-\mu)^T\bold \Sigma^{-1}(x-\mu)$ 称为** 马氏距离 **$D_M$**，这是欧式距离在多元空间中的推广：**
$D_M = (\bold x-\mu)^T\Sigma^{-1}(\bold x-\mu)$
**下面我们提这样一个问题：给定N个独立同分布的样本**$\bold x_i \sim N(\bold \mu, \bold \Sigma)$**，如何估计该多元高斯分布的参数呢？**

**同样，可以使用MLE和MAP两种估计方法。**

- 极大似然估计
   - 似然函数：

$L(\mu,\Sigma) = log P(\bold x|\mu,\Sigma) \\
= \sum_{i=1}^NlogP(\bold x_i|\mu,\Sigma) \\
= \sum_{i=1}^Nlog\frac{1}{(2\pi)^{D/2}6|\Sigma|^{1/2}}e^{-\frac{1}{2}\sum_{i=1}^N D_M(x_i)}$

   - 最大化时的参数结论：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1699339254751-0d40be6d-96d2-44d2-a5bf-c98ab063beaf.png#averageHue=%23fcfcfb&clientId=u0e63a603-818a-4&from=paste&height=153&id=u57523565&originHeight=229&originWidth=811&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=87365&status=done&style=none&taskId=u72efa041-2e9e-45c8-9c1d-4c937410ccd&title=&width=540.6666666666666)

- 最大后验概率估计
   - 似然函数

$L(\mu,\Sigma) = log P(\bold x|\mu,\Sigma) 
= (2\pi)^{-\frac{ND}{2}}|\Sigma|^{-\frac{N}{2}}e^{-\frac{1}{2}\sum_{i=1}^N D_M(x_i)}$
其中，对于$\sum_{i=1}^N D_M(x_i) = \sum_{i=1}^N (x_i-\mu)^T\Sigma^{-1}(x_i-\mu)$，可以用采样数据x的均值$\bar x$和散度$S_{\bar x}$表示为：
$\sum_{i=1}^N D_M(x_i) = tr(\Sigma^{-1}S_{\bar x}) + N(\bar x - \mu)^T \Sigma^{-1} (\bar x - \mu)$
,其中$S_{\bar x} =  \sum_{i=1}^N(x_i - \bar x)(x_i - \bar x)^T$

   - 最大化时的参数结论：
      - 因为是MAP，需要引入先验的参数：
         - $\bold m_0 = \bar x$，即先验均值
            - $k_0$：对$\bold m_0$的信任程度 ，为0或一个极小值
         - $\bold S_0 = \frac{diag(S_{\bar x})}{N}$，协方差的先验，并取平均
            - $v_0$：对$\bold S_0$的信任程度 ，$v_0 = D+2$，D是维度
         - 然后对先验均值和极大似然估计的结果做**  凸组合 ，定义：**

$\bold m_N = \frac{k_0}{k_0+N}\bold m_0 + \frac{N}{k_0+N}\bar x \\ \
\\
\bold S_N = \bold S_0 + S_{\bar x} + \frac{k_0 N}{k_0+N}(\bar x - \bold m_0)(\bar x - \bold m_0)^T \\ = \bold S_0 + S_{\bar x} + k_0\bold m_0\bold m_0^T-K_N \bold m_N\bold m_N^T \\ \ 
\\ 其中：
k_N = k_0 + N \\
v_N = v_0 + N$
则估计结果为：

      - ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1699340522524-968b4f6e-de54-4446-a436-bef4db536cf0.png#averageHue=%23fbf8f7&clientId=u0e63a603-818a-4&from=paste&height=246&id=u220176e4&originHeight=472&originWidth=919&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=165465&status=done&style=none&taskId=u9c62d9f6-5344-4492-a011-0e9d87c8828&title=&width=479.66668701171875)
### 高斯判别分析
上一节我们已经学习了如何以 **多元高斯分布** 为随机变量建模贝叶斯生成模型，结合第一节所说，下面我们尝试利用其生成一个分类器。
假设我们有多个训练样本数据$x_1,x_2,...$，它们对对应的类别分别是
$y=1,2,...C$
那么可以定义当条件为“随机变量被分为类别c”的条件密度函数：
$p(\bold x|y=c, \theta) = N(\bold x |\mu_c, \Sigma_c)$
其中$\mu_c, \Sigma_c$是 所有**被分为类别c的**训练样本数据构成的多元高斯分布 的 **均值** 和 **协方差矩阵**
按照生成分类器的 **后验估计最大** 为原则，和离散生成分类器的输出类似，可以得到分类的计算公式：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1699349725219-ec89c3b9-c84b-4c8b-97bf-cb15c8bfdd86.png#averageHue=%23e7e4de&clientId=u0e63a603-818a-4&from=paste&height=82&id=uec9e635d&originHeight=164&originWidth=821&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=128092&status=done&style=none&taskId=u9d0839da-f501-457c-b210-c9f24044e73&title=&width=411)
其中，$\pi$是分类先验的分布
 如果对于类别c，先验分布$\pi_c$是均匀的 ，则推导公式为：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1699349859415-c228bb8b-524f-46d3-a444-6c047e5fca37.png#averageHue=%23f6f5f5&clientId=u0e63a603-818a-4&from=paste&height=223&id=uc7f18086&originHeight=420&originWidth=1117&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=72362&status=done&style=none&taskId=u9e9d8e04-0848-40b1-8052-4d8f03d8d17&title=&width=593.6666870117188)

- 如果 x 是二元变量，那么y(x)的分类结果（决策边界）在平面图上会呈现一条二次曲线，因此，也将这种方法称为“二次判别分析”）下图是一个这样的例子：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702364438739-20599d54-6b2f-4108-9825-c37cb76f07ca.png#averageHue=%23f3f5f1&clientId=u4d400a54-f043-4&from=paste&height=192&id=u9cbbea9a&originHeight=769&originWidth=913&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=1340916&status=done&style=none&taskId=u567f29b0-145c-467b-900a-a12ca3bab76&title=&width=228)
> 在这个图示中，样本变量x是一个二元的变量，因此被绘制在平面上，每个点还对应一个y值，图中的圆圈表示的是y值的等高线。
> x相对y的值服从正态分布，直观地绘制出三维和二维的对比图，其实是这样的：
> ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702365947307-3d964d51-7b38-44ca-a9c4-9d31dde1b6cb.png#averageHue=%23fafaf9&clientId=u8a0783c9-8f57-4&from=paste&height=218&id=uf99c62ed&originHeight=327&originWidth=802&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=159339&status=done&style=none&taskId=ub8cb454e-d610-454d-9c93-2c07d88bc44&title=&width=534.6666666666666)


- 如果对于任意的类别c，有$\Sigma_c = \Sigma$，则决策边界恰好是一条直线，此时“二次判别分析”退化为“ 线性判别分析 ”,可以直接写出决策线的表达式：

$logP(y=c|\bold x,\theta) \propto \beta_c \bold x + \gamma_c$
其中参数矩阵$\beta_c = (\Sigma^{-1}\mu_c)^T$,$\gamma_c = -\frac{1}{2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c$，下图是一个这样的二元变量线性分类的例子：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702364578332-cec787c4-3c95-4c1c-96a5-add08e756fbc.png#averageHue=%23ecf2e8&clientId=u4d400a54-f043-4&from=paste&height=298&id=u25737511&originHeight=595&originWidth=736&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=785758&status=done&style=none&taskId=u608514d9-39fd-487d-99fb-a6ab1b1d1f3&title=&width=368)
我们可以看到，多元高斯分布分类器（判别器）的模型参数就是高斯分布的参数：
$\theta_c = (\mu_c, \Sigma_c)$，基于上一节的结论，直接使用极大似然估计来计算参数。
先计算对数似然函数，其即是 对每个分类的多元高斯分布 做 加权平均（没错这和离散情况的下的贝叶斯分类器其实是一样的）：
$log P(D|\theta) = \sum_{i=1}^N \sum_{c=1}^C (log \pi_c)^{if(Y_i =c)} + \sum_{c=1}^C[\sum_{i: y_i=c}log \ N(\bold x| \mu_c, \Sigma_c)]$
右侧的N即为类别c对应的多元高斯分布的概率密度函数。
而$\pi_c$表示第c个类的先验，其满足以下条件：
$0 \leq \pi_c \leq 1, \  \sum_{c=1}^C \pi_c = 1$
在实际使用时，往往以每个类c在数据集中的统计频率结果作为先验，带入极大似然估计可以得到，第c个类的多元高斯判别器参数为：
$\hat \mu_c = \frac{1}{N_c} \sum_{i: y_i=c} \bold x_i$
$\hat \Sigma_c = \frac{1}{N_c} \sum_{i: y_i=c} (\bold x_i - \hat \mu_c)(\bold x_i - \hat \mu_c)^T$
其中，$N_c$是类别c的样本在数据集中出现的次数。
### 多个正态分布的高斯混合模型
上面的单一正态分布模型，看起来很美好，但是存在一个问题：我们**假定了任何一个类别c的数据都只服从一个单一的正态分布**
事实上，我们无法假设任何要拟合的连续数据都真的只服从一个正态分布。
实际很多情况下， 真实的数据是 **多个不同正态分布的叠加**。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702365771017-75123d9b-7381-4f6e-99dc-527db0adabe8.png#averageHue=%23f5e9df&clientId=u4d400a54-f043-4&from=paste&height=219&id=u5c9143f1&originHeight=351&originWidth=459&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=153700&status=done&style=none&taskId=u27cbe03c-f7bc-4884-b5be-e6ab7e53128&title=&width=286)![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702365780459-b13478f6-330d-4068-ba1b-d9bc2f1c34db.png#averageHue=%23f3e0d5&clientId=u4d400a54-f043-4&from=paste&height=233&id=u5d6ed73a&originHeight=356&originWidth=451&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=134328&status=done&style=none&taskId=ue77ab893-0a6e-46bb-adab-c0eff078dad&title=&width=294.66668701171875)
因此，我哦们需要——**高斯混合模型**：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1699350642213-2eadb622-6a19-42d3-9569-89301c94f62c.png#averageHue=%23f9f5f4&clientId=u0e63a603-818a-4&from=paste&height=257&id=ua61e3e29&originHeight=526&originWidth=863&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=200798&status=done&style=none&taskId=u0bb3854c-b077-49c9-8642-80780033741&title=&width=421.3333740234375)
理论上来说，高斯混合模型可以模拟任意的分布函数。
为了能对数据进行 “混合高斯建模”，首先我们需要知道要建模的数据空间到底需要几个正态分布的类。
没错，所以建立高斯混合模型的第一步，是**聚类（clustering）**。
> 这就和上一节的生成分类器很不一样，前者我们已知（假设了）每个类别对应一个正态分布。而在这里是不同的，我们**不能相信标签里的类别信息**，只能将最终每个类别需要混合的正态分布的数量视为隐变量。

#### EM算法 估计高斯混合模型
我们依然可以写出MLE方法和MAP方法对混合高斯模型的对数似然函数：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702366551211-1bf69b57-7cb3-4c95-b22d-10e915a39111.png#averageHue=%23f7f3f1&clientId=u8a0783c9-8f57-4&from=paste&height=201&id=u1969a951&originHeight=301&originWidth=711&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=78386&status=done&style=none&taskId=u80fbc168-aace-40ce-ade8-d7797ccd22d&title=&width=474)
最大的问题在于：我们不知道隐变量z（即真实正态分布的分类数量）的维度，因此参数的解也是不唯一的。
如果我们假设有k个维度（即每个分类用k个正态分布混合表示），那么就会有k!种不同的解。
把k也加入模型的参数中，我们可以用C4中提到变分EM算法来进行最大似然估计。
公式的推导很长，如何以后有机会的话我把它整理一下放到另一个链接里……
这里我们简单回顾一下过程吧：

- 1、定义Q函数，步数t=0，定义一个初始先验。然后每一步迭代推导隐变量$z_i= k$的条件概率
- 2、E步：计算t-1步中，将Q函数表示为多元正态分布的参数
- 3、M步：最大化Q函数值，结果作为下一次迭代的参数。
- 4、重复2-3步

下图是一个实际迭代过程的可视化举例：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702366813995-a4fc6175-2119-4ea9-904f-5daf0a9bef1f.png#averageHue=%23f9f7f5&clientId=u8a0783c9-8f57-4&from=paste&height=312&id=ub6c85e43&originHeight=623&originWidth=809&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=253404&status=done&style=none&taskId=u52a83690-f8b8-4dc6-ae5a-3744b186645&title=&width=405)
#### K-means算法 估计高斯混合模型
你们可能在数据分析或者数据挖掘课上学到过K-means算法，你们有没有好奇过，这个算法的原理到底是什么？
> 天哪如果你只看上一节就敏锐地发现了其实K-means算法就是EM算法的近似变形的话那你可太NB（朴素贝叶斯）了，我上课的时候是一点没听懂，感觉这还是八竿子打不着的两个问题……

**K-means算法其实是上述 混合高斯模型估计的 一个近似特例**。
直观地说这么解释：它们在聚类的时候其实都是一开始未知类别的数量k，然后通过计算不断收敛的。
它做了两个假设：

- 每个要混合的正态分布的 **协方差矩阵 已知**  （假设它们都等于标准的方差）
- 每个要混合的类别的 **先验 已知**（假设它们都相等，即为1/k）

因此，在K-means算法中，我们只需要计算样本相对于每个先验点的均值，并以此来迭代估计出每个类别的分布：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702367291874-00d9ce28-67b7-47e8-aeca-a6be601d008d.png#averageHue=%23fbf7f7&clientId=u8a0783c9-8f57-4&from=paste&height=204&id=u8bcaaf0b&originHeight=407&originWidth=600&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=64288&status=done&style=none&taskId=u54c81312-f10b-4714-ab94-41d0ae80b34&title=&width=300)
好处显而易见：计算非常方便快捷，能在最短的时间得到收敛的聚类。
但是，因为做出了协方差和先验的假设，最后得到的高斯分布就再也不能用于进行概率密度估计了。因此K-means相对于直接建模混合高斯分布有以下缺点：

- 无法计算某一个样本点属于某个高斯分布类的概率值
- 因此，也就无法生成新的样本点

综上，K-means算法在数据分析中非常常用，但是它不能作为一个生成模型来使用。遗憾退场。
### 隐马尔可夫模型
隐马尔可夫模型是关于 **时序** 的概率模型 。
为此，首先你需要知道 马尔可夫过程 和 马尔可夫链 的基本定义。我假定你已经学过了《随机过程》……
> 什么，你没有学过？！这下可不好办了……咳咳……

问题不大，你只需要先记好这两点：

- 马尔可夫过程 是基于时序的，它假定随机变量的每个取值都随 **时间 **而变。
- **齐次**马尔可夫过程 假定 **每一个时刻 随机变量的概率取值  只与 上一个时刻的随机变量有关**。
> 你不足需要知道 齐次 是什么意思，总之在本篇的应用范围内，我们可以认为马尔可夫过程都是齐次的。

这样的一个过程中，状态组成的序列被称为马尔可夫链，因为每一个时刻都是可以和上一个时刻通过概率相连的。
而在隐马尔可夫模型中， **状态序列(state sequence)**是**隐藏的**，从外部数据无法观测。这也就是为什么被称为**隐**马尔可夫链。
那么外部观测的是什么呢？我们只知道**它和当前时刻的那个隐藏状态有关**。我们把每个时刻中观测到的数据也组成一个序列，称为**观测随机序列 (observation sequence )**
这样一定很不好理解，问题不大，我们来举一个例子：
> **骰子游戏**
> - 假设有三个不同的骰子：D6、D4、D8  （六面、四面和八面骰子）。
>    - （——为什么没有D100？！——兄弟你走错片场了，这里玩的是DND）
> - 每次投掷前，先等概率地随机从三个骰子中选取一个。
> - 然后投掷选到的骰子，骰子随机地产生一个数字。
> 
如果我们每次记录下每次得到的数字，那么它们就组成一个观测序列。
> 隐藏序列是什么？是我们每一次投的是哪一个骰子。
> 加入我只告诉你前面几次投掷的结果是：1 6 3 5 2 7 3 5 2 4 ，要估计下一次投掷的结果，那么隐藏状态必然是猜测下一次骰的是哪一个骰子。

 ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702376611865-7349741b-f17c-43b6-a2c1-780da88e9f91.png#averageHue=%23f7f6f2&clientId=u8a0783c9-8f57-4&from=paste&height=106&id=ue003380f&originHeight=159&originWidth=804&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=126366&status=done&style=none&taskId=udd18370e-9cb0-4435-81ea-57617afc0e3&title=&width=536)
接下来我们就以这个“生成骰子游戏点数”的模型为例子，来介绍隐马尔可夫模型。
隐马尔可夫模型 有三个要素：

- **初始概率分布** $\pi$：即第一次（0时刻）观测变量的概率分布
   - 如骰子游戏中，第一次试验可能得到1-8中的一个数字，可以通过组合方法计算出初始分布。
      - 比如，第一次试验产生数字1的概率$=\frac{1}{3}(\frac{1}{6}+\frac{1}{4}+\frac{1}{8})=\frac{13}{72}$
- **状态转移矩阵**$A$：每个隐藏状态在下一时刻可能转换为另一个隐藏状态的概率：$A(i,j) = p(z_t = j\ | \ z_{t-1}=i)$
   - 在骰子游戏中，三个骰子被视为三个隐藏状态。
   - 其中，每个状态在下一时刻都有等概率（1/3）转变为任意的三个状态（包括它自己）
- **观测概率矩阵** $B$：从每个已知隐藏状态到观测状态的概率

$B_t(j) = p(\bold x_t \ | \ z_t=j)$

   - 在骰子游戏中，_（如果t=0）_对于D4，每个观测状态（1-4）的概率是25%，其他三个骰子类似。

将马尔可夫过程的假设写成数学公式，就是：
1、齐次马尔可夫性假设  
隐马尔可夫链 $t$**时刻的状态 只和 **$t-1$**时刻的状态** 有关， 与其他时刻的状态及观测无关，**也与时刻**$t$**无关 ：**
$p(z_{1:T}) = p(z_1)p(z_2|z_1)p(z_3|z_1,z_2) .... p(z_T|z_1,z_2,...,z_{T-1}) \\
=p(z_1)p(z_2|z_1)p(z_3|z_2)....p(z_T|z_{T-1})$
2、 观测独立性假设 
 观测变量只和当前时刻的状态有关，与其他时刻的 观测和状态均无关  
$p(x_{1:T}|z_{1:T}) = p(x_1|z_1)p(x_2|z_2)p(x_3|z_3)....p(x_T|z_T)$
> 在骰子游戏中，观测变量是离散的，因此模型参数可以使用多项分布或贝塔分布，如果观测变量是连续的（如语音或文本），可以使用上面几节介绍的**多元高斯分布**来建模。
> 但是 **状态空间 **在隐马尔可夫模型中 **一定是离散的**

**根据任务的不同，隐马尔可夫模型要求解的问题可以被分为以下几类：**

- **学习：已知观测序列**$x_{1:T}$**，学习模型参数**$\theta$**,使得生成该观测序列的概率p最大**
- **解码（预测隐空间）：给定模型参数**$\theta$**和观测序列**$x_{1:T}$**，求最有可能的隐藏状态序列**$z_{1:T}$
- **计算（预测观测结果）**：**给定模型参数**$\theta$**和观测序列**$x_{1:T}$**，求该观测序列**$x_{1:T}$**生成的概率。按照是否已知整个序列的数据，还可以分为：**
   - 前向算法（在线学习）：已知中间观测序列1-t，求生成到该序列的概率：$p(z_{t}|x_{1:t})$
   - 后向算法（离线学习）：已知整个观测序列1-T，求$p(z_{t}|x_{1:T})$
#### 可观测隐变量学习
在骰子游戏这个例子中，虽然从给定的观测序列$x_{1:T}$中我们无法知道隐变量的分布**，但是我们知道隐变量定义为三种骰子的随机选取，它服从某种特定的规则。**
类似这种情况，我们仍为隐变量z**仍然是可观测**的，因此，可以直接**利用极大似然估计**法来估计隐马尔可夫模型参数 。
对于N个状态序列的离散变量，我们可以使用多项分布建模： 假设观测变量x服从多项分布，其中k表示隐藏状态，l表示观测到的序列：
$B(k,l) = p(x_t =l|z_t=k,\theta),\ K=1,2,..,k,\ l=1,2,...,L$
我们在C2章已经举过例子了，不如推导留给你们，这里是参数$\pi$和 A 的估计结果：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702379452508-26a8b0b7-8a84-4fa9-81b5-e0079e693329.png#averageHue=%23fbf9f9&clientId=u8a0783c9-8f57-4&from=paste&height=207&id=u97163441&originHeight=310&originWidth=684&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=56590&status=done&style=none&taskId=uc6fb1d30-1e7b-4083-b6c9-b5293f7ea8d&title=&width=456)
而参数矩阵B则使用统计方法计算：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702379756298-83edfcdb-3cc7-4754-a0de-5898ca9092f5.png#averageHue=%23f6f5f3&clientId=u8a0783c9-8f57-4&from=paste&height=149&id=u9dc34de5&originHeight=224&originWidth=803&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=82201&status=done&style=none&taskId=u243d5780-767b-49ac-8cc7-87f55650d8f&title=&width=535.3333333333334)
如果 **观测变量x服从正态分布**，那就如同前面几节的方法，如法炮制， 在计算观测概率B时，将分子分母替换为 均值 和 协方差 矩阵即可。
#### 不可观测隐变量学习
那么，更多的情况下，我们确实无法观测隐变量z，或者无法得知其的生成规律，此时我们需要一种 **非监督的学习方法**
Baum-Welch算法 ：一种改进的EM算法，用来解决隐马尔可夫模型的参数估计。
老三样：

- Q函数：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702380022496-38139e44-5063-4e6b-b06d-813d0423dbe3.png#averageHue=%23fbfaf9&clientId=u8a0783c9-8f57-4&from=paste&height=203&id=u06963748&originHeight=304&originWidth=678&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=109915&status=done&style=none&taskId=u824104f2-1f98-4a20-a0d1-0c4dddf58a8&title=&width=452)
> 这里因为符号t已经被用来标记马尔可夫过程的时刻，我们用old上表标记参数的上一次迭代。

- E步：在这里我们需要计算数据集中已知的序列在当前参数下的生成概率，具体方法参见后面的“计算问题”小节。![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702380066760-256018c9-9342-411d-8c0f-9ad327982898.png#averageHue=%23fcf9f8&clientId=u8a0783c9-8f57-4&from=paste&height=246&id=u407ad5ff&originHeight=491&originWidth=766&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=156709&status=done&style=none&taskId=u58c55dac-b067-4662-9cca-55124742b99&title=&width=383)
- M步：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702380144254-68e94a0c-0009-44dd-9e00-95c418b6e67c.png#averageHue=%23faf7f6&clientId=u8a0783c9-8f57-4&from=paste&height=208&id=u86f1b4f1&originHeight=416&originWidth=873&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=179636&status=done&style=none&taskId=ub4b374cc-0f6a-47e7-a0aa-e8953ff7704&title=&width=437)
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702380170552-17a3c2c6-d693-4e61-b9b8-34fccd3c388e.png#averageHue=%23fbf7f7&clientId=u8a0783c9-8f57-4&from=paste&height=206&id=uad7b35fd&originHeight=411&originWidth=718&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=154918&status=done&style=none&taskId=u256548f5-522d-429f-b64a-0c61cb75621&title=&width=359)
#### 计算问题

- 前向算法 ：从第0步开始，算出下一步的概率密度
   - ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702380602849-78c7cff1-55b1-44ab-b955-27afa256136f.png#averageHue=%23f6f6f6&clientId=u8a0783c9-8f57-4&from=paste&height=55&id=u3a03ef89&originHeight=82&originWidth=804&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=22210&status=done&style=none&taskId=ubcd8d239-d7ae-4811-853b-c1498c06a62&title=&width=536)
   - 我们可以发现，t时刻的隐藏状态依赖于t-1时刻的隐藏状态，利用马尔可夫链的特性，我们可以定义下面的两个转移函数，其中i，j都是某一个隐状态：
   - $\phi_t(j) = p(x_t|z_t=j,\theta)$
   - $\psi(i,j) = p(z_t=j|z_{t-1}=i,\theta)$
   - 定义t时刻的 信念状态 （belief state）：
   - ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702380970906-c536e8d3-01a4-445f-a2f0-ef3113b1718d.png#averageHue=%23f6f6f6&clientId=u8a0783c9-8f57-4&from=paste&height=127&id=u50f56917&originHeight=190&originWidth=822&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=53441&status=done&style=none&taskId=ufe7343e4-05d0-4312-a16e-f20bf5de082&title=&width=548)
   - 于是我们得到：$\alpha_t \propto \phi_t(j)(\Psi^T\alpha_{t-1})$其中$\Psi$是所有转移函数$\psi(i,j)$组成的矩阵，并由此可以从t=1开始计算信念状态，从而得到最中的概率密度链
- 后向算法： 与前向计算相反，使用 条件似然函数 从最终时间T（前提：我们知道整个序列）反向推导每个时间点的序列概率。
   - ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702381248351-afba479d-7b0c-4650-9307-17b75395bcd0.png#averageHue=%23faf7f7&clientId=u8a0783c9-8f57-4&from=paste&height=221&id=u3064e56a&originHeight=411&originWidth=635&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=80235&status=done&style=none&taskId=u652b3841-072b-4fed-b354-f93bbbfd7a4&title=&width=341.3333435058594)
   - ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702381265164-6e22ba45-bd64-4588-9754-bf3ad14a823c.png#averageHue=%23f8f7f6&clientId=u8a0783c9-8f57-4&from=paste&height=216&id=u1f2081c0&originHeight=429&originWidth=892&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=107090&status=done&style=none&taskId=u6a93663b-9cf8-40e7-a3a3-04b1017057b&title=&width=449.3333435058594)

#### 预测问题
假设我们已经知道了模型参数和观测得到的序列数据，现在，我们想要推断最有可能的隐状态序列：
$z* = argmax_{z_{1:T}}p(z_{1:T}|x_{1:T})$
我们可以从第一个时刻起，每个时刻取的隐状态z标记一个代价：即 前面所有步骤的计算概率是否最大：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702380467912-e955cf74-fec4-4789-b92a-5b4f8f3cc657.png#averageHue=%23fbfbfa&clientId=u8a0783c9-8f57-4&from=paste&height=67&id=u6da670ba&originHeight=101&originWidth=673&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=47468&status=done&style=none&taskId=u1284203b-96ba-43ca-a7bb-8059dabae03&title=&width=448.6666666666667)
这样，我们就将求隐状态序列转换为一个  **最大（优）路径问题。**
**然后，我们可以使用计算机算法来求解，例如贪心算法。**
下面介绍一个称为 **维特比算法**  的方法，它的思想是用 动态规划 解最大路径问题：

-  从终结点开始，由 后向前逐步求得结点 $z^*_{T}, z^*_{T-1},...,z^*_{1}$，得到最优路径  
- ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702380531583-8f9a2618-3a0a-4f70-872a-6ca351238c05.png#averageHue=%23faf5f3&clientId=u8a0783c9-8f57-4&from=paste&height=327&id=ud0684df5&originHeight=491&originWidth=839&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=189973&status=done&style=none&taskId=u701d72b5-2eb8-47d1-9c3b-7b235c75787&title=&width=559.3333333333334)
- ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1702380560293-57110d9d-d5b8-4cf3-abef-13be12b093e3.png#averageHue=%23faf6f5&clientId=u8a0783c9-8f57-4&from=paste&height=234&id=u42030b1c&originHeight=351&originWidth=457&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=74882&status=done&style=none&taskId=u611a2017-df9f-4dac-9969-487a0f6e77f&title=&width=304.6666666666667)