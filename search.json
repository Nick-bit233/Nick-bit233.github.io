[{"title":"Nickbit 2024影视游艺总结","path":"/2024/12/29/Nickbit 2024影视游艺总结/","content":"祝 新年快乐！(今年提前了两天发呢，简直是巨大进步！) 转眼间一年又过去了呀，又到了分享今年度精神生活的时间啦~ 今年一言蔽之来说，是属于是大作玩的少，小品级游戏和独立游戏却占据了大部分列表的一年。因此总的数量相比去年多出了一大截……说好了学业忙碌没时间玩游戏的呢？？不过确实因为忙碌了不少，很多游戏的体验时长基本上可以说是浅尝辄止了，在我这里用碎片时间打到通关就是胜利。这就导致一些游戏可能还有隐藏或者更新的内容没有玩到，私密马赛！总之，在今年的总结篇里我会尽量多聊体验，尤其是初见的体验，评分什么大伙看个乐呵就行了。 按照惯例我会对所有完整体验过的作品打分，该分数纯粹主观，但还是分为了客观和主观两个部分： 客观分数：将我的价值观对齐接触过的大多数社区和自媒体得到的评分，满分10分 主观分数：完全以我的喜好评定的分数，满分2分 游戏篇 （如果游戏名后面带有*的，说明这并非是今年发售的游戏） Alan Wake 2 * 发行时间：2023年10月 体验时间：1月 评分：10+（ 9.4 + 1.5 = 10.9 ） 作为一个没有玩过一代AW，控制也只玩了前两章没有坚持下去的，对恐怖和悬疑没什么爱好的玩家来说，很难说我是为什么会坚持打完AW2的。但是在第二章结束Wide Awake响起的时候，我的脑子里只有大呼过瘾的感觉，甚至认为去年年底没有玩这个游戏我已经亏了。AW2的成功很大程度归功于长板足够长：足够精美的画面，足够氛围感的场景，足够好的叙事和镜头（注意叙事是叙事，剧本是剧本），还有几段大家应该都熟知的超神演出，足够掩盖诸如战斗系统和跳脸这样的缺点。我觉得它值得这个10.9。 幻兽帕鲁 发行时间：1月 体验时间：1月-2月 评分：10（ 8.5 + 1.5 = 10 ） 今年的除夕夜推荐游戏！可能到年底了你们已经忘了小帕鲁在年初有多火了吧（doge），表面上是缝合怪，内心上是家里人。幻兽帕鲁其实是一个完成度很高的游戏，更不要说一直到年底都还在更新新内容了。我有空会想回到这个家园再看一看的。 Balatro（小丑牌） 发行时间：2月 体验时间：2月-3月 评分：8+（ 8.6 + 0.1 = 8.7 ） 同样是今年爆火，并且得奖无数的游戏，但是我要狠狠地泼一盆冷水。在我看来，小丑牌的优点只有精心设计的UI动效。它本质上脱离不开靠低运气获得精神刺激的博弈游戏（对说的就是你，德州扑克），靠和抽卡游戏一样的思路吸引到了大批轻量级玩家罢了。整个游戏没有体现出深度和肉鸽设计，诸如没有局外成长，策略迭代等等，加高难度也就只是降低胡的概率。最后，这个很有潜力的框架居然一直没有卡牌和玩法模式更新，好不容易联动了一次却只换来了扑克皮肤。看得出来作者一定是四处拉奖去了，没有好好考虑迭代玩法吧（doge），以至于后半年类小丑牌的玩法已经有遍地开花的势头了，希望在这些后继作品中能看到真正让我欣赏的好玩意。至于小丑牌，我只能说游戏还不错，年度？还是算了吧。 背包乱斗：福西法的宝藏 （EA版本） 发行时间：3月 体验时间：3月-4月 评分：9（ 8.5 + 1 = 9.5 ） 作为同样在社交媒体爆火的游戏，小丑牌你看看人家啊，勤勤恳恳地更新了一整年，平衡性也调整了，新物品一直出，对战大厅也有了。虽然说背包乱斗以体量和创新来说，也还没有把玩法打磨到大师级完美这样的程度，但是作为pvp博弈的竞技游戏，已经做的很好了。不带着太强的胜负欲去玩，合成大物品的一刻仍然是超有成就感的（参考上图的彩虹黏黏233），希望后续还能有更让人惊讶的更新。 神之天平外传：幻雾之洞窟 发行时间：3月 体验时间：3月 评分：10（ 9 + 1.5 = 10.5 ） 神作之DLC，没有了正传中强引导的剧情，外传客观上来说确实是单薄了不少，刷刷刷的玩法也很容易玩腻，可能都到达不了最终的结局。但是我的个人感受，这套玩法并非不可接受。外传相当于是完全强调了kezio的这套传承JRPG经典的成长玩法体系。不用像正传一样被剧情限制，可以充分地丰富的迭代数值、招式、道具。而且各个维度算得上环环相扣，是让玩家有全收集动力的设计，而不是收集环节看着像捡垃圾。因此，神之天平外传依然是非常不错的DLC佳作。 闪避刺客 SANABI * 发行时间：2023年11月 体验时间：3月 评分：9+（ 8.5 + 1.3 = 9.8 ） 非常标致的一款独游，漂亮的美术，爽快的玩法，诙谐又不失深度和感动的剧情，似乎很久都没有玩过在各方面都如此平衡的好作品了，你找不到什么开天辟地的创意，却也找不到可以评头论足的缺点（难度高这一点游戏都提供了无敌选项可以规避）。因此，这是一部推荐大家都来从头玩到尾的作品，而且我非常欣赏它的配乐！ 吸血鬼幸存者 * 发行时间：2022年10月 体验时间：3月- 4月 评分：10（ 9.2 + 1.2 = 10.4 ） 这是春促的时候购买的，有句话怎么说的来着？曾经沧海难为水。玩过开山鼻祖之后，再去看现在遍地开花的类吸血鬼幸存者游戏，会有一种去魅的感觉吧。 恶意不息 NO REST FOR THE WICKED（EA版本） 发行时间：4月 体验时间：4月 评分：8（ 6.5 + 1.5 = 8 ） 这位更是今年的头号惨字选手，开局优化暴雷，好不容易调整好了EA的内容，却发现半年以后毫无讨论度，年底总结的时候我不提估计大家都要忘了（ 但是，我是真的，真的，真的很喜欢Moon Studio的场景美术。以至于翻出这两张截图的时候，脑子里又梦回了那个月的晚上畅游奇幻世界的时候。虽然现在最多只能给个8，我依然秉持着恶意不息是个好的游戏底子，希望月亮工作室的完整版能憋个大的。 Patrick’s Parabox * 发行时间：2022年3月 体验时间：4月 / 9月 评分：10（ 9.6 + 1 = 10.6 ） 今年很多时间都用来补一些独立解谜的经典作品了，潘多拉盒子，不是，帕特里克的盒子就是其中之一，利用无穷迭代的空间悖论来解推箱子，难度下可消遣，上可登天。轻松愉快，锻炼思维能力，是和Baba is you一样的必玩作品。 Hades II （EA版本） 发行时间：5月 体验时间：5月 评分：10+（ 9.2 + 1.5 = 10.7） 我可以说哈迪斯2比1好玩不少吗？一代我都是咬着牙通关的，二代……虽然也是咬着牙通关的，但至少通关了地底还有待开发的地上世界等着大家。虽然一代的经典攻击和无限闪避招式没有了，但是新增的大量机制补充上了玩法的多样性，所以让我这种手残感到打不通结局也没关系，能体验新build就好啦~（可能有一些比较无脑的bulid连闪避都用不着按，才让我觉得好玩吧（doge Animal Well 发行时间：5月 体验时间：5月 - 6月 评分：12（ 10.0 + 2 = 12.0 ） 噔噔咚！让我们恭迎年度真神！ 我不管我不管，管你们发不发奖，正如我的视频标题所说，动物＃就是在5月份就斩获了今年最佳独立游戏， 何止今年最佳，我直接列为本人玩家史上最佳游戏殿堂。如果按照年度总结的标准，在我这里能拿12.0满分的作品之前只有三个（它们是《FEZ》、《Minecraft》、《极乐迪斯科》），现在我可以说迎来了第四位成员。 我想具体的可以不用多说，动物井已经成为了冒险解谜这一品类新的一座不可逾越的高山。要是再考虑到作者自制引擎这样的技术力，真会让人开始往思考宇宙存在是否都公平的那一块去了…… 不过不过，动物井并非完美，12.0的分数也并不是照着完美来定的。例如动物井在音乐方面就是一大短板，因此我一开始是想给11.9的，但是最后破格升级到了11.99999……，它值得在我这里成为必推游戏。 关于动物井的深入思考，总归涉及到一些游戏剧透，也许会在另一篇文章中详述（如果它能在年前写完的话（ 点老救世记 dotAGE * 发行时间：2023年10月 体验时间：6月 - 7月 评分：10（ 9 + 1.5 = 10.5 ） 除开动物井，今年最让我上头的游戏莫过点老救世记，虽然是去年推出的，不过今年上半年才迎来了官方中文（谜之声推荐必属精品）。 策略游戏总是给很多玩家太宏大太广阔的感觉，以至于上手门槛总是很高。点老救世记在保留了游戏深度和资源调度的基础上，简化了很多设定，并且有类似物品机制的解锁路线引导，不会让玩家上手就被过多的选项弄得晕头转向。游戏明亮的像素风村镇也十分讨喜，可以说是我第一个坚持下来的策略游戏，并且陷入了下一个回合就睡觉的痛苦循环之中。 怎么这段写的这么像IGN中国啊……我发誓这不是AI生成的！ Rabi-Ribi * ob一串字母女士：想啊，让我忘记一切吧（还在go） 发行时间：2016年1月 体验时间：7月 评分：9+（ 8.9 + 0.8 = 9.7 ） 较早的游戏了，是暑假打发时间开始玩的。一开始说是硬核萌豚我还是不信的，直到后面被打趴下了，本菜鸡默默地调到了最低难度（难度平衡这一块必须指出本作是有点问题的）。 买过来玩主要是想看看适不适合这个系列，以便购入新作tevi的，但结论也许是不适合（仅对于操作这一块）……但是，CreSpirit的地图设计还是非常有水平的，还请到了3R2等著名的作曲家。而且，谁能拒绝兔耳娘呢？ 粘粘世界2 发行时间：8月 体验时间：8月 评分：10（ 8 + 2 = 10.0 ） 没有技巧，全是感情的一集，2分的主观分可以说我给的全是情怀，毕竟是我的独游启蒙，第一款steam游戏啊。过了15年了迎来的官方续作，当年陪我在群里聊粘粘球的小伙伴现在都研究生毕业了…… 虽然在创新方面没有带来当年那么大的震撼（有几个关卡和章节还是可圈可点的），但是依然伴我度过了暑假难忘的三个夜晚。 黑神话：悟空 发行时间：8月 体验时间：8月 评分：10+（ 9.1 + 1.8 = 10.9 ） 篇幅有限，我只挑了我自认为拍的最好的三张截图。没错，今年的所有游戏里，只有黑神话是让我心甘情愿地在打boss的过程中还打开游戏摄影的~~（等一下，你玩的其他游戏也没几个有摄影模式的啊）~~，我觉得别的无需多言。虽然玩的过程中心态也经历过一些波折，但是咱们确实已经有了真正的电子游戏顶流作品。别看10.9好像还不够高，这可才是起点。 不过如此的去月球系列海滩特别篇 发行时间：9月 体验时间：9月 评分：9+（ 8 + 1.8 = 9.9 ） 去月球系列的大结局（之一）。瞰哥啊瞰哥，一开始我是不信你写这么轻松愉快的故事的，最后我又是不信你写这么快意断绝的故事的。难道伟大的艺术家的一辈子真的就是活在这么痛苦的矛盾中吗？不过作品一旦交到读者手中，作者就没有评判的价值了（doge）。因此瞰哥怎么写结局，也不会影响每个人心中的去月球三部曲。 从这个角度看，我们苦等了多年的结局篇，不如还是说粉丝回馈篇吧，毕竟塞了好多彩蛋和会心一笑的梗，还有那架能一直听的钢琴……希望高瞰老师不要燃尽在《最后一小时》里面，我们还等着看去月球电影呢（ OneShot * 发行时间：2016年 体验时间：10月 评分：10（ 8.8 + 1.7 = 10.5 ） 最早的一批重meta要素的独立作品，也算是慕名而来。虽然游戏流程很短，画面表现也比较难堪，但是包袱抖出来的时候仍然非常惊人。你要知道那是undertale、这里没有游戏和邪恶铭刻等作品都还没有面世的时代。游戏的剧情和音乐也非常契合，不像很多meta要素的游戏注重解构和戏谑，游戏结束以后还能收获一份感动，看在这个份（以及主角很可爱！）上给与破格1.7的主观得分。 Pentiment * 发行时间：2022年11月 体验时间：10月 评分：10+（ 9.2 + 1.5 = 10.7 ） 已经想玩很久的作品，刚发售时就已经关注，只是苦于这个专八水平的文本还没有翻译。去年底终于有了翻译，但因为各种原因一直耽搁，终于趁着国庆假期给一口气推完了。 一开始我还真以为是推理游戏了，苦恼哪个选项才是正确，后来才知道这是一个线性剧情的游戏，你的选择毫无意义（不过对于理解世界观并非毫无意义）。从第一章以后就慢慢进入心流状态，几乎就是在读一本非常扣人心弦的小说，再配上精致的手工绘本和文本框动画的那种书写感，是一种令人超放松的享受。谁说创造氛围感就一定要设计复杂的光线和镜头呢？ 感谢黑曜石，在现在这个时代的游戏里还能读到这样美的文字是一种至福。 Braid Anniversary Edition 发行时间：5月 体验时间：12月 评分：10（ 9.6 + 1 = 10.6 ） 算是补票。当年独立游戏大电影的三杰，只有Braid我没有打通就放弃了，也没去了解其中的隐藏秘密。趁着吹哥发售了重制版，我也重走了一遍时间之旅。 回过头来看，现在我可以客观的说，Braid确实就不如fez和吹哥的后来的见证者…… 哈哈哈，开个玩笑，早期作品是优缺点明显的。异常精巧的谜题设计和不友好的操作手感以及引导，Braid就是这样一个挺矛盾的作品，也许我会把它和一些比较疯魔的解谜作品放在一起（比如某个香肠卷）。不过我觉得吹哥应该也和以前的自己达成了和解，这部分去听一听开发者音频，我想你也会明白的。 Tunic * 行时间：2022年3月 体验时间：12月 评分：10+（ 9.3 + 1.5 = 10.8 ） 本来准备云的，但是还是在月底的最近几天迅速通关了。Tunic在两年前发售时其实有着非常广的宣发，成为了当年年度独立的有力竞争者（然后搞笑TGA就用三年彻底败坏了其在独立领域的话语权），但是当年我自以为Tunic的表皮之下是更加偏向类似巴别塔圣歌的语言解谜，从而没有尝试。 今年自从高强度以FEZ和见证者做对比分析动物井后，就有人一直提议我去玩Tunic。通关以后不得不说，小狐狸你才是FEZ和见证者的soulmate啊，真是soul-soul又mate-mate啊。有几个场景那都不是致敬了，是复刻（doge）。 Tunic基本符合这类冒险解谜游戏的精髓：表面上一个设计精巧的探索战斗流程，背地里一组隐藏的解谜收集内容，然后隐藏的隐藏中还有游戏世界观的彩蛋内容。 操作手册的设计是本作最突出也是最有原创价值的特点，文字的设计虽然复杂（谁会用这种比片假名还难读的东西啊doge），但却是非常有几何线条之美感。自创的语言文字仅在隐藏的隐藏谜题中扮演了一部分角色，不会也不影响99%的体验。而且Tunic相对FEZ和见证者友好很多，标准隐藏直接写进中后期的操作手册里，几乎等于明示隐藏解法，因此真结局对于所有玩家而言都不算硬门槛（到是战斗系统设计真的有些憋憋了，建议直接开无敌模式游玩，不要折磨自己）。 啊居然写了这么多，冲着小狐狸还蛮可爱的，给主观分加上0.5。 下面是我还没有通关或是中途弃坑的游戏： Raft * 发行时间：2022年6月 体验时间：1月 浏览上面的列表你也许会发现，今年不少玩的游戏都是想着补票一些前几年评价很好但我错过的游戏，其实这个列表里在第一个的，就是raft。 但是后来我退款了，我并不觉得这款游戏能和上面的老游戏坐一桌。至少单人模式是这样的。游戏的引导系统做的并不好，前期资源管理一有失误，就非常容易卡死自己。更痛苦的在于，游戏的基本的存档回档功能也不完善，最后只能进入死循环然后放弃存档。想打个mod提高一下游戏体验，发现很多mod从23年以后就再也没有更新过了，属实有点失望。 恶魔轮盘 Buckshot Roulette 发行时间：4月 体验时间：4月 感谢好朋友的友情赠送！风格化到了极致的小品作品，看来低色阶的画风真能吸引不少人。因为玩的比较少只打通了一遍开局，听说多周目以后有更多的，策略性更强的，头皮发麻小游戏~~，希望什么时候有朋友了好好体验一下~~，友尽对战应该也会很有意思的吧~ 动漫篇 唉今年看的番和影视作品太少了，很多时候都是刷刷新番总结玩玩梗，时间都拿去玩游戏了，对不起我的错，我的错…… （同样地，如果名称后面带有*的，说明这是补番） 迷宫饭 开播时间：2024.1 类型：半年番 评分：9（ 8.4 + 1.2 = 9.6 ） 半年番，但是只看了前半年的12集（后面的我会补的（立flag）），也是今年热度口碑双丰收的作品了。看起来非常轻松舒服，有一种玩全新trpg游戏，乐此不疲地翻看世界观说明书的感觉，人设和台词功底也非常深厚，值得推荐（就是为什么我对其中的吃的提不起兴趣捏，也许平时吃的还是太好了我还是不适合呆在地下城）。 BanG Dream! Episode of Roselia * 开播时间：2021.12 类型：剧场版 补番时间：1月 评分：8（ 7 + 1 = 8 ） 剧场版 BanG Dream! Poppin’Dream * 开播时间：2022.1 类型：剧场版 补番时间：8月 评分：8+（ 7.2 + 1.5 = 8.7 ） 两部放在一起说，都是邦邦的剧场版，一部冬天看的，一部夏天看的，这下算是补完了邦邦动画的全部内容了。 两步的剧情其实都没啥可说的，比较粉丝向，给前三季正片里剧情比较少的角色一个补完。不过我还是比较喜欢邦邦的情绪氛围渲染，R组炸团那一段也好，关岛篇PPP live的看星星段也好，都是比较循序渐进的，不会太强制把观众的情绪拉进来。作为观众看完后就会觉得美好的邦邦写了一个美好的Dream，终究不会是现实，但是又能给你烦躁的内心一点宽慰…… （不是，那我之前kirakira的邦邦怎么变成现在这个样子了？还有wsd我缺的MyGO!!!剧场版这一块谁给我补啊……） GRILS BAND CRY 开播时间：2024.4 类型：季番 评分：9（ 8.2 + 1 = 9.2 ） “强制把观众的情绪拉进来”，对没错你猜我在暗示什么。我们少女乐队企划现在是真的火了啊，居然连四大名著都有了。虽然我是邦邦粉丝，但是也不会因为这个立场就去黑GBC，相反我也要吹一吹这个3D，当时4月那是各大群聊社交媒体铺天盖地的GBC表情包啊，足以见得这个3D制作功底，从Mygo到GBC，似乎终于让3D再次伟大了，我泪目…… 至于剧情，就和刚才暗示的一样，GBC的剧本是希望强制把观众的情绪拉进来的，看进去了总是要失去一些理性思考的，然后会心服口服；但是看不进去的就旁观者清，能挖出一堆问题了。但是，这既不是缺点也不是优点，只是特色，而且是个人喜好罢了。不如来聊聊小孩姐唱的：姐我给你跪了，你是真会唱啊，我们家那个🐏，她比不了一点的（叠甲：并非烤羊，只是说🐏姐不适合这种风格） 夜晚的水母不会游泳 开播时间：2024.4 类型：季番 评分：8（ 7.2 + 1.3 = 8.5 ） 好的这是4月番的另一面，押宝失败的结果。我一开始也是非常看好水母的，到了最后只能说非常可惜，我甚至还想过这个结尾三集怎么改可以挽回剧本的口碑……不过现在看来也是无所谓了，到了评选年度的时候，这种级别的缺点，完全不能和放在TGA公式最后的仙人相比呢……总之，我还是很喜欢这部番的人设，以及40mp老师为其撰写的配乐。 魔法少女与邪恶曾经敌对。 开播时间：2024.7 类型：季番 评分：8（ 7.5 + 1 = 8.5 ） 看这个完全是被女主白夜的设计绑架了，不好意思这个真是太戳我了，根本没办法拒绝。剧情上是比较克制的工业糖精，没有什么硬伤，所以非常轻松顺畅地看完了。哦顺便一说，这番的op我愿意给年度op。 （唉今年都没有看过10分以上的番，写动漫部分的时候满脑子都是Ave Mujica，啊，我现在就要看阿贝母鸡卡……） 影视篇 周处除三害 开播时间：2023.10(中国台湾) / 2024.03(中国大陆) 观影时间：2024.10 评分：9.2（ 8.2 + 1 = 9.2 ） 挺高质量的影片，湾湾蛮会写的嘛，节奏没尿点表演也到位（居然引入大陆大部分都没被删减，也是说明这是真正的艺术了（doge）） 宇宙探索编辑部 开播时间：2023.04.01(中国大陆) 观影时间：2024.11 评分：9.7（ 8.7 + 1 = 9.7 ） 居然是在泛式直播间看完的（达成成就：直播间看电影），表面科幻喜剧片，实际致郁文艺片，把直播间的长颈鹿都给搞不会了。 侧面说明了这部片非常对电波，不能接受的人会非常、非常难受，看的下去的则会有特别的思考。有时换一换口味看看堂吉诃德式的作品，也是蛮不错的。 （哦我还在直播间看了749局，但是这个不算电影（聚众吃shi啊家人们），就不放进来了）"},{"title":"LangChain速成文档","path":"/2024/01/23/LangChain速成文档/","content":"注意，本篇基于langchain官方文档编写，众所周知LangChain的更新频率很高，因此本文不保证具有时效性 最后更新时间：2024-02-03， LangChain版本0.1.2 LangChain基础：使用链和LLM对话 安装langchain 使用pip： pip install langchainpip install langchain-core # 正常来说，上面一行命令也会同时安装langchain-corepip install langchain-community # 安装第三方贡献的langchain库pip install langchain-openai # 使用OPENAI的模型和api需要的集成包 如果你使用OPENAI 的 api，可能需要按照下面的方法配置api： 配置部分：采用环境变量，但所有变量在实例化类对象时均可以传入参数的形式覆盖# API_KEYapi_key = sk-xxx # 默认调用模型名称model_name = gpt-3.5-turbo# 默认调用模型的最大token数max_tokens = 4096# 默认调用模型的温度temperature = 0# 其他配置如果想把 OPENAI API 的请求根路由修改成自己或第三方的代理地址，可以通过设置环境变量 “OPENAI_API_BASE” 来进行修改。12/14备注：实测openai的python库无法正确使用http规则代理（我不知道为什么，也许要去看源码）， 如果使用官方接口，建议开启全局代理，然后不再设置代理环境变量# set openai api keyos.environ[OPENAI_API_KEY] = api_key# set openai proxy# os.environ[OPENAI_PROXY] = http://127.0.0.1:7890# set openai base urlos.environ[OPENAI_API_BASE] = https://aigptx.top/v1 基本使用 基础LLM查询 LangChain包括两种使用语言模型的方法： LangChain LLMs 类（接收字符串、输出字符串） LangChain 聊天模型（接收消息、输出消息，其中消息是一个langChain定义的类），类名通常以Chat开头 from langchain.llms import OpenAIfrom langchain.chat_models import ChatOpenAI# 导入消息类型from langchain.schema import ( AIMessage, HumanMessage, SystemMessage)###### 直接invoke llm 对象llm = OpenAI(model_name=model_name)result = llm.invoke(怎么评价人工智能)print(result)##########chat = ChatOpenAI(model_name=model_name) # 实例化一个chat对象# 消息类型：AIMessage, HumanMessage, SystemMessage（分别对应AI回复，用户输入，系统提示）messages = [ SystemMessage(content=You are a helpful assistant that translates English to French.), HumanMessage(content=Translate this sentence from English to French. I love programming.)]# invoke chat 对象，返回“AIMessage”类型的对象result = chat.invoke(messages)print(type(result)) # - AIMessageprint(result)##### 使用Prompt模板 帮助你通过简单输入构建提示模板： from langchain.prompts import PromptTemplatefrom langchain.prompts import ChatPromptTemplateif chat_prompt: chat_prompt = ChatPromptTemplate.from_template(What is a good name for a company that makes product?) print(chat_prompt.format(product=colorful socks)) return chat_prompt# 通过 PromptTemplate 类来创建提示模板prompt = PromptTemplate( input_variables=[product], # 这是一个可以修改的参数，会被填入下面的template字符串中 template=What is a good name for a company that makes product?,)# 类似string.format()的用法，传入参数填充模板中的变量print(prompt.format(product=colorful socks))return prompt 使用链式调用连接模板、模型和解析输出 from langchain.chat_models import ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserimport osdef basic_chain_usage_test(use_chain_pipe=True): # 基本结构：prompt + model + output parser prompt = ChatPromptTemplate.from_template( tell me a short joke about topic, use language ) model = ChatOpenAI(model_name=model_name, temperature=0.9) output_parser = StrOutputParser() if not use_chain_pipe: # langchain的各种类型都具有invoke函数 # prompt类型的invoke函数会将输入的参数填充到prompt模板中，返回一个带有message后的prompt对象 prompt_value = prompt.invoke(topic: ice cream, language: Chinese) print(fprompt: prompt_value) # model类型的invoke函数会将prompt value传递给已实例化的model对象，这里model是一个LLM聊天模型，返回一个message对象 message = model.invoke(prompt_value) print(type(message)) print(fmessage: message) # output parser类型的invoke函数会处理message对象中返回的信息。这里的StrOutputParser是一个基础输出器，将结果转换为字符串 result = output_parser.invoke(message.content) print(type(result)) print(foutput: result) return # piece together then different components into a single chain using LCEL # use | (unix pipe operator) as the operator to connect the components # 使用 | 作为连接符号，将不同的组件连接起来，以形成一个链式管道 chain = prompt | model | output_parser result = chain.invoke(topic: pigeons, language: Chinese) print(freply: result) LCEL和可运行管道 上述例子中，类似chain = prompt | model | output_parser的语法糖被称为LCEL（LangChain Expression Language）。 每个支持LCEL语法糖的对象，底层都被抽象为了一个“可运行”类：Runnable，多个可运行对象的输入输出相连接，组成“可运行管道”，即RunnableParallel。它允许接受来自invoke函数得到的输入（通常是一个字典类型），然后根据其上子类的实现不同，处理数据并得到输出。 记住下面的几点： 使用RunnableParallel()可以实例化一个可运行管道对象，该对象存储的数据结构被视为一个字典，可以通过参数赋值来设置字典的键值对，或是直接传入一个字典。 使用RunnablePassthrough()函数可以指代invoke函数的输入，返回类型是字典 使用RunnablePassthrough.assign()函数可以修改invoke函数的输入的字典，包括添加新的键值对，返回类型是字典 使用任何lambda表达式可以直接处理invoke函数的输入，返回的则是lambad表达式的结果 from langchain_core.runnables import RunnableParallel, RunnablePassthroughrunnable = RunnableParallel( passed=RunnablePassthrough(), extra=RunnablePassthrough.assign(mult=lambda x: x[num] * 3), modified=lambda x: x[num] + 1,)runnable.invoke(num: 1)# 输出结果： passed: num: 1, extra: num: 1, mult: 3, modified: 2 在LCEL的链式语法中，如果两个对象之间以|连接了，实例化RunnableParallel()的步骤可以直接简写为定义一个字典： retrieval_chain = ( # 下面的三种写法等价： # RunnableParallel(context: retriever, question: RunnablePassthrough()) # RunnableParallel(context=retriever, question=RunnablePassthrough()) context: retriever, question: RunnablePassthrough() | prompt | model | StrOutputParser()) 可以使用itemgetter函数快速检索一个特定的输入键值（需要线导入名称） from operator import itemgetterchain = ( context: itemgetter(question) | retriever, question: itemgetter(question), language: itemgetter(language), | prompt | model | StrOutputParser())chain.invoke(question: where did harrison work, language: italian) 可以使用RunnableLambda()函数，允许调用任何已定义的外部函数来处理输入数据，并返回你自定义的结果。但是，这些函数都必须有且只有一个类型为字典的参数。 def multiple_length_function(_dict): return len(_dict[text1]) * len(_dict[text2]) chain = ( x: text1: itemgetter(foo), text2: itemgetter(bar) | RunnableLambda(multiple_length_function), | prompt | model)chain.invoke(foo: abc, bar: gah)# x的内容是9 (3*3=9) 可运行管道组成的链是可以随意嵌套的，你可以将其视为一个带有特定功能的字典，允许多层套娃： prompt1 = ChatPromptTemplate.from_template( generate a attribute color. Return the name of the color and nothing else:)prompt2 = ChatPromptTemplate.from_template( what is a fruit of color: color. Return the name of the fruit and nothing else:)prompt3 = ChatPromptTemplate.from_template( what is a country with a flag that has the color: color. Return the name of the country and nothing else:)prompt4 = ChatPromptTemplate.from_template( What is the color of fruit and the flag of country?)color_generator = ( attribute: RunnablePassthrough() | prompt1 | color: model | StrOutputParser() )color_to_fruit = prompt2 | model | StrOutputParser()color_to_country = prompt3 | model | StrOutputParser()question_generator = ( color_generator | fruit: color_to_fruit, country: color_to_country | prompt4) 外部参数绑定 Runnable.bind()的函数允许你在外部添加可运行管道对象的参数。这里我们以为模型对象绑定参数为例。 最常用的功能是，为模型绑定stop参数，可以指定LangChain在匹配到到LLM的某些输出后终止LLM的后续输出，直接返回已输出的内容： prompt = ChatPromptTemplate.from_messages( [ ( system, Write out the following equation using algebraic symbols then solve it. Use the format EQUATION:... SOLUTION:... , ), (human, equation_statement), ])runnable = ( equation_statement: RunnablePassthrough() | prompt | model.bind(stop=SOLUTION) | StrOutputParser())print(runnable.invoke(x raised to the third plus seven equals 12))### 不加bind(stop=SOLUTION)的结果：EQUATION: x^3 + 7 = 12SOLUTION:Subtracting 7 from both sides of the equation, we get:x^3 = 12 - 7x^3 = 5### 加上bind(stop=SOLUTION)的结果：EQUATION: x^3 + 7 = 12 附加函数调用功能 如果你使用的是OPENAI等支持函数调用的模型，你还可以定义函数（这要求LLM以固定的格式输出结果），并使用bind函数将其绑定到LLM对象或聊天模型对象中。 from langchain.chat_models import ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParserfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParserfrom langchain_core.runnables import RunnableParallel, RunnablePassthroughdef prompt_chain_with_input_and_output_handle(): prompt = ChatPromptTemplate.from_template(tell me a joke about foo) model = ChatOpenAI(model_name=model_name, temperature=0.9) # 为model定义函数调用，这回要求model返回一个符合该function定义的，json格式的结果 functions = [ name: joke, description: A joke, parameters: type: object, properties: setup: type: string, description: The setup for the joke, punchline: type: string, description: The punchline for the joke, , , required: [setup, punchline], , ] # 这个管道的定义使得当用户输入ears时，map_对象中实际得到的数据是foo: ears map_ = RunnableParallel(foo=RunnablePassthrough()) # chain = ( # map_ # | prompt # | model.bind(function_call=name: joke, functions=functions) # | JsonKeyOutputFunctionsParser(key_name=setup) # 使用Parser从json中提取想要输出key（如：setup）字段 # ) chain = ( map_ | prompt | model.bind(function_call=name: joke, functions=functions) # 绑定函数和指定函数调用 | JsonOutputFunctionsParser() # 使用Parser提取整个json ) result = chain.invoke(ears) print(result) 模型运行时配置修改 如果你不想在某些模型运行时使用全局的语言模型配置参数（模型名称，温度，最大token限制等），可以使用以下方法： 首先，在实例化model时指定可以覆盖修改的参数，使用ConfigurableField，其中id字段要与你使用的模型后端匹配，而名称和描述可以自定义。 from langchain.prompts import PromptTemplatefrom langchain_core.runnables import ConfigurableFieldfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(temperature=0).configurable_fields( temperature=ConfigurableField( id=llm_temperature, name=LLM Temperature, description=The temperature of the LLM, )) 使用模型时（包括在LCEL中使用时），用函数即可覆盖配置： model.with_config( configurable= llm_temperature: 0.9 ).invoke(pick a random number) 还可以使用configurable_alternatives指定一套可替代的配置方案，详细请参考：Configure chain internals at runtime | 🦜️🔗 Langchain 加入对话记忆 如果需要进行多轮对话，则有必要将每次对话的内容记录下来，并在下一次提示时提供历史消息。 可以通过在定义prompt时添加关于历史的MessagesPlaceholder占位符来实现： def test_memory_chain(): from operator import itemgetter from langchain.chat_models import ChatOpenAI from langchain.memory import ConversationBufferMemory from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.runnables import RunnableLambda, RunnablePassthrough model = ChatOpenAI(model_name=model_name) # 使用聊天LLM模型和聊天提示模板 prompt = ChatPromptTemplate.from_messages( [ (system, You are a helpful chatbot), # 这是系统提示 MessagesPlaceholder(variable_name=history), # 这是一个占位符，用于将对话历史填充到模板中 (human, input), # 这是用户输入 ] ) memory = ConversationBufferMemory(return_messages=True) # 添加一个ConversationBufferMemory对象，用于存储对话历史 mem = memory.load_memory_variables() # 初始化memory，返回值为这样的字典：history: [] print(finit memory: mem) chain = ( RunnablePassthrough.assign( history=RunnableLambda(memory.load_memory_variables) | itemgetter(history) ) # 从memory中加载键值为history的变量，然后将其通过管道输入给下一步的prompt | prompt | model ) inputs = input: hi im bob response = chain.invoke(inputs) print(ffirst response: response) memory.save_context(inputs, output: response.content) # 一次chain invoke后，需要更新memory中的context mem = memory.load_memory_variables() # 重新加载memory，返回值中会有已经存储的对话历史 print(fmemory after first invoke: mem) # 再次进行对话，这次会将对话历史填充到prompt中 inputs = input: can you tell what is my name? response = chain.invoke(inputs) print(fsecond response: response) 记住，MessagesPlaceholder占位符不止用于记忆历史，可以记录任何外部内容，当你需要从外部添加信息时也可以使用。 LangChain实现检索增强（RAG） 检索增强的含义是，LLM可能不知道用户需要的特定数据，因此需要一个程序检索外部数据，然后在执行生成步骤时将其传递给LLM。 检索增强的步骤通常是将外部源数据（通常是各种文档）进行加载、拆分处理，然后转化为向量（被称为“嵌入”），这将保存数据中的语义信息，去除干扰，以方便语言模型理解，最后存储在一种介质中（如向量数据库），当模型需要时，通过一种方法从介质中“检索”数据。 加载文档 LangChain支持各种文档格式的读取，如果你的数据只是简单的文本格式（如markdown和txt），可以直接使用TextLoader获得一份存储在内存的文档对象。 from langchain_community.document_loaders import TextLoaderloader = TextLoader(./index.md)loader.load()# 文档对象的数据结构示例：[ Document( page_content=--- sidebar_position: 0 --- # Document loader ... , metadata=source: ../docs/docs/modules/data_connection/document_loaders/index.md )]# 每个文档包含两个对象，page_content是文本形式存储的内容，metadata包含文本的元数据，比如存储路径source 关于所有支持的文档格式和相关库，参考：https://python.langchain.com/docs/integrations/document_loaders/ 下面我们会介绍其他针对常用格式的Loader： CSV表格 from langchain_community.document_loaders.csv_loader import CSVLoader# 不同的CSV文档可能有不同的分隔符，读取时可以指定，并且可以指定表头的名称loader = CSVLoader(file_path=./example_data/mlb_teams_2012.csv, csv_args= delimiter: ,, quotechar: , fieldnames: [MLB Team, Payroll in millions, Wins])data = loader.load()# 读取CSV文件时，可以指定source_column参数将文本的元数据从默认的“文件路径”转化为每一列的表头名称# 在文本-问答任务中这种处理也许有用。loader = CSVLoader(file_path=./example_data/mlb_teams_2012.csv, source_column=Team) HTML网页 from langchain_community.document_loaders import UnstructuredHTMLLoader# 第一种方法是使用UnstructuredHTMLLoader，这将直接提取html中的纯文本内容到page_content中，而忽略其他的所有内容。loader = UnstructuredHTMLLoader(example_data/fake-content.html)data = loader.load()# 另一种方法是使用BeautifulSoup4from langchain_community.document_loaders import BSHTMLLoader# 这将使得html中的纯文本内容被提取到page_content中，而网页标题（如果有）被写入元数据中的title字段loader = BSHTMLLoader(example_data/fake-content.html)data = loader.load()data JSON JSONLoader类定义了加载JSON格式文件的方法。LangChain允许你从一个json文件中读取出多个不同的文档，并且可以指定读取哪些键对应的值的内容。这是通过一个名为jq的python库实现的。 在实例化JSONLoader的时候，可以传入jq_schema参数，其中指定了要从json文件的哪个键中提取对象。jq_schema的语法与lua字典类似，直接使用.可以访问子级键值对，如果对象是一个列表，使用[]可以提取整个列表中的每个对象，这会让JSONLoader返回多个Document对象组成的列表，其在列表中的索引序号将被写入元数据中。 loader = JSONLoader( file_path=./example_data/facebook_chat.json, jq_schema=.messages[].content, text_content=False)data = loader.load()# 示例：如果json长这这样： joinable_mode: link: , mode: 1, messages: [content: Bye!, sender_name: User 2, timestamp_ms: 1675597571851, ... content: Goodmorning! $50 is too low., sender_name: User 2, timestamp_ms: 1675577876645], title: User 1 and User 2 chat # data中的内容将会是： [ Document( page_content=Bye!, metadata= source: /Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json, seq_num: 1 ), ... Document( page_content=Goodmorning! $50 is too low., metadata= source: /Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json, seq_num: 10 ), ] 常见的json格式对应的jq_schema语法如下： JSON - [text: ..., text: ..., text: ...]jq_schema - .[].textJSON - key: [text: ..., text: ..., text: ...]jq_schema - .key[].textJSON - [..., ..., ...]jq_schema - .[] 如果你的json文档格式是一个后缀名为.jsonl的 JSON Lines文件，这种文件中的每一行是一个符合json格式的字符串，可以在一个文件中存储多个json对象。对于这种文件，在加载时json_lines=True参数即可。指定的jq_schema索引会对每一行的json对象起作用。 loader = JSONLoader( file_path=./example_data/facebook_chat_messages.jsonl, jq_schema=.content, text_content=False, json_lines=True)data = loader.load() 你还可以自定义要将哪些json文件的内容写入文档的metadata，即元数据字段中，这需要通过传入一个metadata_func来实现： # metadata_func有两个参数，第一个参数是源数据的json对象，第二个参数对应其在文档中的元数据对象。def metadata_func(record: dict, metadata: dict) - dict: # 将原json的sender_name字段的值写入文档元数据的sender_name字段 metadata[sender_name] = record.get(sender_name) # 你也可以修改已有元数据的内容，如这里修改了所有文档中元数据的路径， # 将它们路径中 /langchain 前的部分都删除了。 if source in metadata: source = metadata[source].split(/) source = source[source.index(langchain):] metadata[source] = /.join(source) return metadataloader = JSONLoader( file_path=./example_data/facebook_chat.json, jq_schema=.messages[], content_key=content, metadata_func=metadata_func)data = loader.load() Markdown from langchain_community.document_loaders import UnstructuredMarkdownLoadermarkdown_path = ../README.md# 如果加入参数mode=elements，将会保留所有markdown语法的符号（如```)，否则只会提取文本loader = UnstructuredMarkdownLoader(markdown_path, mode=elements)data = loader.load() PDF 有多种方法提取PDF文件的内容（基于底层的不同），详情参见：https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf 这里介绍使用PyPDFLoader读取文件，它返回一个Document的列表，每个对象都是pdf中一页的内容对应的文档，页码信息会被存储到元数据中。 # 确保先安装了pypdf# pip install pypdffrom langchain_community.document_loaders import PyPDFLoaderloader = PyPDFLoader(example_data/layout-parser-paper.pdf)pages = loader.load_and_split() 如果想从读取图像中的文字，可以使用 rapidocr-onnxruntime ,这是一个OCR包，可以识别图像中的文本并存储到文档中。安装完成后加入参数extract_images=True即可 # pip install rapidocr-onnxruntimeloader = PyPDFLoader(https://arxiv.org/pdf/2103.15348.pdf, extract_images=True)pages = loader.load() 如果对pdf文档的结构无所谓，使用非结构化数据可以只提取所有的纯编码数据到文档。 from langchain_community.document_loaders import UnstructuredPDFLoaderloader = UnstructuredPDFLoader(example_data/layout-parser-paper.pdf) # 同样，加入mode=elements可以保留元素data = loader.load() Python源码 可以使用PythonLoader类直接加载python代码为文档。 from langchain_community.document_loaders import PythonLoader 批量加载文档 从一个目录下加载目录中的所有文件。默认情况下，所有文件都以非结构化的方式（直接读取文本）加载到文档对象中，返回一个列表。 DirectoryLoader相关参数： glob : 用来控制加载符合哪些路径名和扩展名的文件 show_progress=True：显示进度条，需要先安装tqdm库 use_multithreading=True：使用多线程加快速度 loader_cls=TextLoader：指定所有文件使用某个特定的文档加载器类 silent_errors=True：如果加载过程中出现错误（如编码问题），忽略错误并继续加载（默认将会终止加载，并不会返回已加载的内容到内存） loader_kwargs= ：其他加载时的参数，以字典形式传入 要在加载过程中自动检测文件编码，传入'autodetect_encoding':True from langchain_community.document_loaders import DirectoryLoadertext_loader_kwargs=autodetect_encoding: Trueloader = DirectoryLoader(path, glob=**/*.txt, loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)docs = loader.load() 分割文档 处理长文本时，有必要将文本分割成块。尽管这听起来很简单，但这里有很多潜在的复杂性。理想情况下，您希望将语义相关的文本片段放在一起。“语义相关”的含义可能取决于文本的类型。 关于LangChain支持的所有分割器类型，参考：Text Splitters | 🦜️🔗 Langchain 这里只介绍两种：基于字符（单词）拆分和基于语义拆分。 按照字符拆分文档 最简单的方法，按照字符数切分文档，一般只适用于西文。 from langchain.text_splitter import CharacterTextSplittertext_splitter = CharacterTextSplitter( separator= , chunk_size=1000, chunk_overlap=200, length_function=len, is_separator_regex=False,)# 这里传入的是纯文本的列表对象，将会分别拆分每个对象。# metadata是可选的，会一一对应地写入被拆分的文档中的元数据。# 返回值是Document类型documents = text_splitter.create_documents( [doc1, doc2, ...], metadatas=[meta1, meta2, ...])print(documents[0]) 按照单词拆分 推荐普遍情况下有一堆长文本时使用，希望尽可能的将相近的段落和语句放在一个分块中，适用于各种语言的编码。 from langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show. chunk_size=100, chunk_overlap=20, length_function=len, is_separator_regex=False,)texts = text_splitter.create_documents([doc])print(texts[0])print(texts[1]) 根据语义拆分 这将根据文本中的语义相似性拆分文本，语义更相似的，更有可能放在一起。需要文本嵌入编码模型的介入。 这里使用OPENAI的文本嵌入api作为语义相似度检查的例子，使用前需要先配置好相关api环境变量: !pip install --quiet langchain_experimental langchain_openaifrom langchain_experimental.text_splitter import SemanticChunkerfrom langchain_openai.embeddings import OpenAIEmbeddingstext_splitter = SemanticChunker(OpenAIEmbeddings())docs = text_splitter.create_documents([doc])print(docs[0].page_content) 编码文档为向量嵌入 编码文档到向量嵌入非常简单，只需制定好模型，然后传入字符串格式的文本即可。前面小节中加载的文档对象，可以从page_content字段中取出字符串值。 可以使用的文本-向量编码模型及说明文档参考：Text embedding models | 🦜️🔗 Langchain 下面会使用在线调用OPENAI的文档编码模型为例子。 from langchain_openai import OpenAIEmbeddings# 如果你在全局设置了环境变量，则无需传入api key的参数embeddings_model = OpenAIEmbeddings(openai_api_key=...)# 这里直接使用字符串作为例子embeddings = embeddings_model.embed_documents( [ Hi there!, Oh, hello!, Whats your name?, My friends call me World, Hello World! ])len(embeddings), len(embeddings[0]) 可以使用文本-向量编码模型快速比较不同文本直接的相似度，如： embedded_query = embeddings_model.embed_query(What was the name mentioned in the conversation?)embedded_query[:5] # 输出embed_query传入的文本与embeddings_model中已编码的5个文本之间（各个）的相似度。# 输出举例：[0.0053587136790156364, -0.0004999046213924885, 0.038883671164512634, -0.003001077566295862, -0.00900818221271038] 创建向量存储器 下面的例子使用 FAISS 向量数据库，该数据库利用了Facebook AI相似性搜索（FAISS）库。 也可以选择其他不同的向量存储器库，参考：Vector stores | 🦜️🔗 Langchain 先安装相关依赖： pip install faiss-cpu 相关步骤如下： 加载一个示例文档 使用分割器将文档分割为固定长度的字符串 对于每个字符串切片，进行向量嵌入编码（使用OpenAIEmbeddings），然后使用FAISS.from_documents存储到数据库对象。 from langchain_community.document_loaders import TextLoaderfrom langchain_openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain_community.vectorstores import FAISS# 读取源文件raw_documents = TextLoader(../../../state_of_the_union.txt).load()# 分割文本为若干块，并转换为对应的文档对象text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)documents = text_splitter.split_documents(raw_documents)# 创建向量数据库，传入两个参数：文档对象列表 和 嵌入模型的实例化对象db = FAISS.from_documents(documents, OpenAIEmbeddings()) 相似性搜索 创建好向量数据库后，就可以根据相似性，搜索出需要的文本内容，然后解码为可读的文本。 similarity_search函数将会查询输入文本，找出最相关的多个文档，并返回其解码后的文档内容。 query = What did the president say about Ketanji Brown Jacksondocs = db.similarity_search(query)print(docs[0].page_content) 如果输入不是可读文本，而是一个已经编码好的向量，使用函数similarity_search_by_vector embedding_vector = OpenAIEmbeddings().embed_query(query)docs = db.similarity_search_by_vector(embedding_vector)print(docs[0].page_content) 嵌入缓存 使用带有缓存的文本嵌入类，可以使得从重复的文本创建向量数据库时，不再调用模型编码而直接查询缓存中的结果。这可能可以减少模型的调用量。 通常，嵌入缓存和向量数据库一起使用，当查询缓存命中时，LangChain直接从缓存中构建向量数据库，而不是从原始文本编码开始。下面是一个例子： 为了使用缓存，首先需要引入名称CacheBackedEmbeddings，并指定一个用于底层编码嵌入向量的模型，这里仍然使用OpenAIEmbeddings 创建一个缓存的存储位置，下面的例子中，缓存位于本地文件目录./cache/中。 你也可以将缓存设为存储在内存中（如果你的内存足够），使用如下的语句： from langchain.storage import InMemoryByteStore store = InMemoryByteStore() 指定一个命名空间参数namespace，命名空间比较重要，它保证了你使用 不同的嵌入模型 编码 同一文本 时不会触发缓存命中（否则这容易导致不同模型间的冲突）。一般来说，将其指定为嵌入模型的名称即可。 from langchain.embeddings import CacheBackedEmbeddingsfrom langchain.storage import LocalFileStorefrom langchain.text_splitter import CharacterTextSplitterfrom langchain_community.document_loaders import TextLoaderfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsunderlying_embeddings = OpenAIEmbeddings()store = LocalFileStore(./cache/)cached_embedder = CacheBackedEmbeddings.from_bytes_store( underlying_embeddings, store, namespace=underlying_embeddings.model) 完成后，你可以使用cached_embedder代替OpenAIEmbeddings()等底层嵌入模型创建向量数据库，它们会自动在需要时使用缓存。 使用list(store.yield_keys())可以查看缓存中的主键。 检索器 上面的内容介绍了简单的向量数据库语义搜索，它们直接返回相应的文本。但实际使用时，往往需要结构化的数据，因此需要包装检索器。 LangChain提供了许多内置的检索器对象，可以方便地完成多查询、排序、时间加权等功能。关于所有的检索器类，参考：Retrievers | 🦜️🔗 Langchain 基础检索器 - 基于向量数据库 可以直接从向量数据库对象构建检索器 # 省略了前置步骤……db = FAISS.from_documents(docs, embeddings)# 创建检索器retriever = db.as_retriever()# 查询相关的文档(默认以相似性搜索)docs = retriever.get_relevant_documents(what did he say about ketanji brown jackson) 在创建检索器时，可以指定其使用的检索方法， 如最大边缘相关性搜索（MMR）,前提是对应的向量数据库对象支持这种检索。 在search_kwargs参数中可以使用字典传入其他参数，如相似性分数阈值（只返回分数高于该阈值的文档），或在指定top k（只返回k个最相关的文档）等。 # 使用MMR检索retriever = db.as_retriever( search_type=mmr)# 使用带有阈值的相似性搜索retriever = db.as_retriever( search_type=similarity_score_threshold, search_kwargs=score_threshold: 0.5)# 指定Top kretriever = db.as_retriever( search_kwargs=k: 3) 多查询检索器 MultiQueryRetriever 这种查询器的目标时为了提高语义检索的鲁棒性。由于自然语言存在一定的冗余性和歧义，当查询措辞发生细微变化，或者嵌入不能很好地捕捉数据的语义时，检索可能会产生不同的结果。这通常会使得检索到的文档范围缩小（即同样的问题，只是改了一个措辞，就查不到相关文档了）。为了解决这样的问题，可以使用多查询：即每次查询时，使用多个语义相近的字符串代替单一字符串，进行多次查询，并统合最终的结果。 在LangChain中，自然地使用LLM来生成“多个语义相近的查询”，这样用户只需像往常一样输入一个查询语句，但可以达到多查询的效果。 from langchain.retrievers.multi_query import MultiQueryRetrieverfrom langchain_openai import ChatOpenAIfrom langchain_community.vectorstores import Chromafrom langchain_openai import OpenAIEmbedding# 构建一个向量数据库（这里使用的存储后端是Chroma）embeddings = OpenAIEmbeddings()vectordb = Chroma.from_documents(documents=docs, embedding=embeddings)llm = ChatOpenAI(temperature=0) # 实例化一个LLM对象，作为多查询生成器# 构建多查询检索器retriever_from_llm = MultiQueryRetriever.from_llm( retriever=vectordb.as_retriever(), llm=llm)import logging# 增加一个logging对象，以打印生成出的多查询内容logging.basicConfig()logging.getLogger(langchain.retrievers.multi_query).setLevel(logging.INFO)question = What are the approaches to Task Decomposition?unique_docs = retriever_from_llm.get_relevant_documents(query=question)# 输出示例：这里是log输出，即LLM为查询question生成的相近语义问题INFO:langchain.retrievers.multi_query:Generated queries: [ 1. How can Task Decomposition be approached?, 2. What are the different methods for Task Decomposition?, 3. What are the various approaches to decomposing tasks?] 作为生成多查询的基础，你可以修改底层LLM的配置。上述例子默认使用一个基本的LLM对话。你可以使用基本的LangChain链来将其自定义为带有特定prompt的LLM对话： 注意：为了使得检索器能正确找到LLM输出的多查询内容，在自定义LLM chain时需要将其输出格式化解析为键值对的形式，参考下面的例子。 # 定义一个LLM输出格式解析器，将输出安装换行符分割，存储到一个lines的字典里class LineList(BaseModel): # lines is the key (attribute name) of the parsed output lines: List[str] = Field(description=Lines of text)class LineListOutputParser(PydanticOutputParser): def __init__(self) - None: super().__init__(pydantic_object=LineList) def parse(self, text: str) - LineList: lines = text.strip().split( ) return LineList(lines=lines)output_parser = LineListOutputParser()# 自定义撰写提示，要求LLM输出5个多查询语句QUERY_PROMPT = PromptTemplate( input_variables=[question], template=You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: question,)llm = ChatOpenAI(temperature=0)# 构造LLM Chainllm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)# 使用自定义的LLM Chain 创建多查询检索器，其中，parser_key函数指定了# 检索器从输出数据的哪个key中获取最终的多查询语句列表retriever = MultiQueryRetriever( retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=lines) 上下文压缩检索器 有的时候，知识库中的每个文档都是长文本，而当检索时，用户只需要文档的概括内容，或是其中与查询最相关的内容，而非整个文档。通常这需要对查询结果进行后处理来实现，不过LangChain为此包装了压缩检索器，可以一并完成查询结果过滤的过程。 上下文压缩检索器基于基本检索器，在使用前，先从向量数据库构建一个基本检索器。 可以使用一个LLM实例来决定过滤掉结果文档中的哪些内容，这被称为LLMChainFilter： from langchain.retrievers.document_compressors import LLMChainFilter... # 省略知识库数据处理的过程# 创建基本检索器retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()# 需要一个LLM作为过滤器的后端llm = OpenAI(temperature=0)# 创建LLM过滤器_filter = LLMChainFilter.from_llm(llm)# 构建上下文压缩检索器compression_retriever = ContextualCompressionRetriever( base_compressor=_filter, base_retriever=retriever)# 进行查询，并返回压缩后的查询结果compressed_docs = compression_retriever.get_relevant_documents( What did the president say about Ketanji Jackson Brown) 但是，使用LLM作为压缩和过滤的后端，会让每次查询时都增加许多额外的LLM调用，如果想要节约成本，可以直接使用嵌入模型，查询结果文档内部语句的相似度，从而返回结果。这种情况下，使用EmbeddingsFilter from langchain.retrievers.document_compressors import EmbeddingsFilterfrom langchain_openai import OpenAIEmbeddings... # 省略知识库数据处理的过程# 创建基本检索器retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()embeddings = OpenAIEmbeddings()# 构建向量相似度过滤器embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)# 构建上下文压缩检索器compression_retriever = ContextualCompressionRetriever( base_compressor=embeddings_filter, base_retriever=retriever)# 进行查询，并返回压缩后的查询结果compressed_docs = compression_retriever.get_relevant_documents( What did the president say about Ketanji Jackson Brown) LangChain还提供多种过滤器，如去除冗余的向量嵌入等。并且，分割文本的工具类也可以视为过滤器。 当你需要对输出文本进行一些流水线处理时（如先分割，在去除冗余，最后提取高相似度文本），可以使用ocumentCompressorPipeline构建一个流水线压缩器，LangChain将会自动按步骤处理中间文档，只输出最终结果。 from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.text_splitter import CharacterTextSplitterfrom langchain_community.document_transformers import EmbeddingsRedundantFiltersplitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=. ) # 文本分割redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings) # 去除冗余relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76) # 相似度比较# 构建流水线压缩器pipeline_compressor = DocumentCompressorPipeline( transformers=[splitter, redundant_filter, relevant_filter]# 构建上下文压缩检索器)compression_retriever = ContextualCompressionRetriever( base_compressor=pipeline_compressor, base_retriever=retriever)compressed_docs = compression_retriever.get_relevant_documents( What did the president say about Ketanji Jackson Brown) 多向量检索器 MultiVector Retriever 注意：这与多查询不同，多向量的意思时，每个源文档在存入知识库（向量数据库）时，具有多个对应的向量嵌入，而它们的特性不同（如分别描述了摘要，重点问题。解决方案等文档内容的不同方面）。然后检索时，按照查询的语义偏好查找同一文档的不同向量嵌入。 TODO 带有时间加权的检索器 在检索语义时，也考虑同样的文档或知识内容 上一次被访问的时间。 语义相似度 + (1.0 - decay_rate) ^ 距离上次检索过去的时间（小时） decay_rate被设定为0-1之间的值，越接近0，意味着在检索时，新鲜时间对文档检索结果的影响越大，反之亦然。因此decay_rate为1时，时间加权相当于不存在；decay_rate为0时，则无论文档上一次访问的时间距离多久，文档都有相同的新鲜度。 from datetime import datetime, timedeltaimport faissfrom langchain.docstore import InMemoryDocstorefrom langchain.retrievers import TimeWeightedVectorStoreRetrieverfrom langchain.schema import Documentfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddings# 定义嵌入模型embeddings_model = OpenAIEmbeddings()# 初始化一个空的向量数据库embedding_size = 1536index = faiss.IndexFlatL2(embedding_size)vectorstore = FAISS(embeddings_model, index, InMemoryDocstore(), )# 修改decay_rate来测试时间加权的检索效果retriever = TimeWeightedVectorStoreRetriever( vectorstore=vectorstore, decay_rate=0.01, k=1)yesterday = datetime.now() - timedelta(days=1)# 添加一个字符串作为文档，设置上次检索时间为昨天retriever.add_documents( [Document(page_content=hello world, metadata=last_accessed_at: yesterday)])# 再添加一个字符串，不设定上次检索时间retriever.add_documents([Document(page_content=hello foo)])retriever.get_relevant_documents(hello world)# 在decay_rate接近0的时候会返回hello world，而接近1时会返回hello foo 将检索器包装为LLM Chain from langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain.chains import create_retrieval_chain... # 省略知识库数据处理的过程vector = FAISS.from_documents(documents, OpenAIEmbeddings())retriever = vector.as_retriever()... # 有两种方式构建带有检索器的链，一是在已有链的基础上使用create_retrieval相关函数chain = prompt | llm | output_parserretrieval_chain = create_retrieval_chain(retriever, chain)# 二是直接在链的RunnableParalle管道中传入retriever对象（也是Runnable的）retrieval_chain = ( context: retriever, question: RunnablePassthrough() | prompt | llm | output_parser) 将检索器包装为智能体工具 from langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain.tools.retriever import create_retriever_tool... # 省略知识库数据处理的过程vector = FAISS.from_documents(documents, OpenAIEmbeddings())retriever = vector.as_retriever()# 创建检索器工具retriever_tool = create_retriever_tool( retriever, langsmith_search, Search for information about LangSmith. For any questions about LangSmith, you must use this tool!,) LangChain Agent 核心概念 AgentAction：dataclass，表示Agent采取的一个动作，通常是使用工具 属性： tool 应该调用的工具的名称 tool_input 该工具的输入 实际使用时通常为带Log的AgentActionMessageLog AgentFinish：智能体准备返回给用户时的最终结果 属性： return_values 字典集，包含所有最终输出 通常有一个名为output的key，表示回复给用户的字符串 Intermediate Steps：记录所有智能体的历史动作，和与本次运行相关的输出 通常被保存为一个列表数据类型：List[Tuple(AgentAction, Observation)] 为了最大化框架的灵活性，Observation的类型可以是“任意”（Any），但通常是字符串 智能体链基类：Agent 这是一个在LangChain的链 的基础上构建的类，功能是让大模型决策下一步的行动。 再次提醒：记住 LangChain的链 的概念其实只是包装好一个提示大模型并解析输出的过程，并允许在这个过程中间插入额外的数据处理步骤。 既然Agent基于Chain类型构建，因此底层上可能使用两种LangChain提示策略包装，即 纯LLM 或 聊天模型（Chat Models），根据该类型的不同，Agent具有不同的 “预期类型” （Intended Model Type） 同时，LangChain为常用功能包装好了若干智能体类，它们支持的功能各不相同。 智能体执行器类：AgentExecutor 程序运行时，实际执行智能体功能的躯壳（运行时），进行实际调用智能体、执行它选择的动作、将动作输出传递回代理的操作，并重复这些操作。 AgentExecutor在运行时可以处理的问题： 如果Agent选择了不存在的工具 如果Agent的输出无法解析为合法的工具调用或最终输出 如果Agent选择的工具在运行时发生错误 记录Agent在所有级别上的运行日志（可以输出到控制台或使用LangSmith记录） 预置智能体的使用 预期类型是Chat Models的内置智能体 OpenAI Tools 使用OPENAI支持“工具调用”的模型，这些模型经过了微调，增强了检测何时调用函数，并输出 应该传递给该函数的参数 作为响应的能力。这使得LangChain可用利用这一特性调用多个函数，以此来选择AgentAction，进行动作规划 和下面的“函数调用”的区别是，“函数调用”只支持推理单个函数，而“工具调用”支持一个或多个函数。 只在OPENAI最新的模型（gpt-3.5-turbo-1106或gpt-4-1106）以后支持。 因为“工具调用”是“函数调用”的上位替代，因此OPENAI认为“函数调用”是废旧的功能，不建议再使用旧的OpenAI Functions，而是统一使用OpenAI Tools构建智能体。 需要导入的引用： # 支持OPENAI的agentfrom langchain.agents import AgentExecutor, create_openai_tools_agent# 底层的聊天模型from langchain_openai import ChatOpenAI# 其他适用于创建工具，网络通信等的包都省略了…… 创建智能体 # 定义可用的工具列表，这里是一个示例，导入Langchain内置的其中一个网络搜索工具from langchain_community.tools.tavily_search import TavilySearchResultstools = [TavilySearchResults(max_results=1)] # 通常工具定义为全局变量def new_openai_tools_agent(): # 定义一个提示词，可自定义，这里从一个langchain仓库中获取 from langchain import hub prompt = hub.pull(hwchase17/openai-tools-agent) # 实例化一个聊天模型对象 llm = ChatOpenAI(model=gpt-3.5-turbo-1106, temperature=0) # 实例化智能体对象 agent = create_openai_tools_agent(llm, tools, prompt)\treturn agent 实例化运行时并执行智能体 agent = new_openai_tools_agent()# 创建智能体执行器agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)# 输入内容input = input: what is LangChain?agent_executor.invoke(input) 默认Input只包含用户的一个字符串键值对，但鉴于OpenAI Tools智能体是基于聊天模型的，可再输入中添加任意的对话历史，这和聊天模型的链调用一样： from langchain_core.messages import AIMessage, HumanMessageagent_executor.invoke( input: whats my name? Dont use tools to look this up unless you NEED to, chat_history: [ HumanMessage(content=hi! my name is bob), AIMessage(content=Hello Bob! How can I assist you today?), ], ) OpenAI Functions 与OpenAI Tools一致，使用OPENAI模型内部的“函数调用”（Function calling）功能 使用时，只需修改上述导入的函数名create_openai_tools_agent为create_openai_functions_agent即可，其他配置完全相同。 也适用于其他开源模型提供的与OpenAI Functions兼容格式的API接口，因为这些模型通常没有随着OPENAI而更新api，所以使用时选择旧版的create_openai_tools_agent 需要导入的引用： # 支持OPENAI模型的agentfrom langchain.agents import AgentExecutor, create_openai_functions_agent# 底层的聊天模型from langchain_openai import ChatOpenAIprompt = hub.pull(hwchase17/openai-functions-agent)# 其他的部分省略，与OpenAI Tools 基本一致 Structured Chat Structured Chat（结构化对话）Agent与OpenAI Tools的原理基本相同，唯一的区别是，因为不依赖模型自身经过微调而输出函数调用的能力，结构化对话Agent通过**_ 引导大模型输出有效的 格式化（或称序列化）函数参数_** 来解析并调用工具，再将结果反馈给大模型，由此完成一步的AgentAction。 默认的序列化方法是JSON。 需要导入的引用： from langchain.agents import AgentExecutor, create_structured_chat_agent# 底层的聊天模型仍然可用使用OPENAI模型from langchain_openai import ChatOpenAI 实例化Agent # 定义可用的工具列表，这里是一个示例，导入Langchain内置的其中一个网络搜索工具from langchain_community.tools.tavily_search import TavilySearchResultstools = [TavilySearchResults(max_results=1)] # 通常工具定义为全局变量def new_structured_chat_agent(): # 定义一个提示词，可自定义，这里从一个langchain仓库中获取 from langchain import hub prompt = hub.pull(hwchase17/structured-chat-agent) # 实例化一个聊天模型对象 llm = ChatOpenAI(model=gpt-3.5-turbo, temperature=0) # 实例化智能体对象 agent = create_structured_chat_agent(llm, tools, prompt)\treturn agent 因为非“工具调用”的大模型可能不按照格式化要求输出，在实例化Agent时，可以通过参数控制是否自动处理可能的解析错误。 agent = new_structured_chat_agent()# 创建智能体执行器agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)# 输入内容input = input: what is LangChain?agent_executor.invoke(input) Structured Chat Agent内部将会以下面的方式输出函数调用的过程，但最后返回用户的结果output仍然是自然语言。 Entering new AgentExecutor chain...Action: { “action”: “tavily_search_results_json”, “action_input”: {“query”: “LangChain”} } # 这是工具调用返回的结果，这行注释是方便你理解内容的，并不在实际的输出中出现[url: https://www.ibm.com/topics/langchain, content: LangChain is essentially a library of abstractions for Python and Javascript...]Action: { “action”: “Final Answer”, “action_input”: “LangChain is an open source orchestration framework …” } Finished chain.input: what is LangChain?, output: LangChain is an open source orchestration framework .... JSON Chat 与Structured Chat Agent基本一致，唯一的区别是 JSON Chat Agent仅支持 一个输入参数 的工具调用情况，如果你的Agent需要调用含多个参数的工具，请使用Structured Chat Agent 强制模型使用JSON格式输出函数调用参数。适用于专门对JSON格式进行微调或引导的模型。 使用时，只需将上述步骤中导入的名称create_structured_chat_agent替换为create_json_chat_agent 如果观察中间输出，会发现JSON Chat与Structured Chat的区别是在输出序列化文本时不再加入markdown标记代码段的符号``` ```，整个对话的全局都视文本为JSON格式。 from langchain.agents import AgentExecutor, create_json_chat_agent# 底层的聊天模型仍然可用使用OPENAI模型from langchain_openai import ChatOpenAI 别忘了以上所有基于对话模型的Agent都可以在输入中简单地加入对话历史。 预期类型是纯LLM的内置智能体 XML 对XML特攻，要求模型以XML格式输出函数调用信息，适合于擅长XML处理的模型。与**JSON Chat Agent一样，仅支持单一参数的工具调用** 需要导入的引用： from langchain.agents import AgentExecutor, create_xml_agent# 使用 Anthropic’s Claude 模型from langchain_community.chat_models import ChatAnthropic 实例化并运行Agent # 定义可用的工具列表，这里是一个示例，导入Langchain内置的其中一个网络搜索工具from langchain_community.tools.tavily_search import TavilySearchResultstools = [TavilySearchResults(max_results=1)] # 通常工具定义为全局变量def new_xml_agent(): # 定义一个提示词，可自定义，这里从一个langchain仓库中获取 from langchain import hub prompt = hub.pull(hwchase17/xml-agent-convo) # 实例化一个claude-2模型查询对象\tllm = ChatAnthropic(model=claude-2) # 实例化智能体对象 aagent = create_xml_agent(llm, tools, prompt)\treturn agentagent = new_xml_agent()agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)agent_executor.invoke(input: what is LangChain?) 中间输出将会类似下面这样： Entering new AgentExecutor chain...tooltavily_search_results_json/tooltool_inputwhat is LangChain? !--这是工具调用返回的结果，这行注释是方便你理解内容的，并不在实际的输出中出现--[url: https://aws.amazon.com/what-is/langchain/, content: What Is LangChain? ......] final_answerLangChain is an open source framework ...../final_answer Finished chain. 底层不是对话模型的接口，因此模型不会认为Agent的一次调用是一场对话的一部分。但是，你仍然可以加入“历史消息”，只不过需要将历史的输入 从使用对话模型的Message类 改为 输入一个简单的字符串： agent_executor.invoke( input: whats my name? Only use a tool if needed, otherwise respond with Final Answer, # Notice that chat_history is a string, since this prompt is aimed at LLMs, not chat models chat_history: Human: Hi! My name is Bob AI: Hello Bob! Nice to meet you, ) ReAct 使用“ReAct策略，而不是某种格式化文本来引导Agent执行函数调用，具体参见：ReAct论文 使用方法和JSON Chat Agent以及 XML Agent一致，同样，让ReAct**调用的每个工具只能含有单一参数** 需要导入的引用 from langchain.agents import AgentExecutor, create_react_agent# 注意使用非聊天Chat的模型from langchain_openai import OpenAI 实例化并运行Agent # 定义可用的工具列表，这里是一个示例，导入Langchain内置的其中一个网络搜索工具from langchain_community.tools.tavily_search import TavilySearchResultstools = [TavilySearchResults(max_results=1)] # 通常工具定义为全局变量def new_react_agent(): # 定义一个提示词，可自定义，这里从一个langchain仓库中获取 from langchain import hub prompt = hub.pull(hwchase17/react) # 实例化一个模型对象\tllm = OpenAI() # 实例化智能体对象 agent = create_react_agent(llm, tools, prompt)\treturn agentagent = new_react_agent()agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)agent_executor.invoke(input: what is LangChain?) 中间输出将会类似下面这样： Entering new AgentExecutor chain...I should research LangChain to learn more about it.Action: tavily_search_results_jsonAction Input: LangChain# 这是工具调用返回的结果，这行注释是方便你理解内容的，并不在实际的输出中出现[url: https://www.ibm.com/topics/langchain, content: LangChain is essentially a library of .....] I should read the summary and look at the different features and integrations of LangChain.Action: tavily_search_results_jsonAction Input: LangChain features and integrations[url: https://www.ibm.com/topics/langchain, content: LangChain provides integrations for over 25 different embedding methods .....] I should take note of the launch date and popularity of LangChain.Action: tavily_search_results_jsonAction Input: LangChain launch date and popularity[url: https://www.ibm.com/topics/langchain, content: LangChain is an open source orchestration framework for .....] I now know the final answer.Final Answer: LangChain is an open source orchestration framework for building applications using large language models (LLMs) like chatbots and virtual agents ..... Finished chain. 以防你没有去看ReAct的论文，简单解释是，这里Agent的每个动作要求LLM输出三行文本： 第一行：解释你要采取的行动 第二行：选择一个要使用的工具函数的名称 第三行：给出使用这个工具的输入参数 Self Ask With Search 这是一个简单的包装，仅仅让模型执行一个简单的网络搜索步骤。 这个类用来帮助开发者熟悉如何自定义构建一个Agent，也适用于非常小的模型，执行简单和轻量化的搜索任务时使用。 使用此Agent时，只允许定义唯一的工具，该工具的名称（name字段）必须是Intermediate Answer，返回尽可能精简的搜索结果，Agent负责从中寻找和总结出一个最佳的答案。 from langchain.agents import AgentExecutor, create_self_ask_with_search_agentfrom langchain_community.llms import Fireworksfrom langchain_community.tools.tavily_search import TavilyAnswer# TavilyAnswer工具类只会返回精简的搜索结果，记得将其命名为Intermediate Answer# tools数组里只能有一个这样的工具tools = [TavilyAnswer(max_results=1, name=Intermediate Answer)]def new_self_ask_with_search_agent(): # 定义一个提示词，可自定义，这里从一个langchain仓库中获取 from langchain import hub prompt = hub.pull(hwchase17/self-ask-with-search) # 实例化一个模型对象\tllm = Fireworks() # 实例化智能体对象 agent = create_self_ask_with_search_agent(llm, tools, prompt)\treturn agentagent = new_self_ask_with_search_agent()agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)agent_executor.invoke(input: what is LangChain?) 工具类：Tool 工具是智能体可以调用的函数。 Tool 被抽象为两个组件： input schema：告诉LLM调用该工具需要哪些参数，需要为每个参数定义合理的命名及描述，它们将被LangChain嵌入进Agent的prompt中。 function：实际要运行的函数，通常类型即为一个python函数。 工具的使用和实现 Langchain内置工具 在这里查看目前的内置工具和它们的使用方法：https://python.langchain.com/docs/integrations/tools 步骤：【以使用Wikipedia查询工具为例】 首先导入Python名称，部分工具依赖于第三方python库或插件，需要先安装它们： %pip install --upgrade --quiet wikipedia 导入名称时，请参考各工具的文档，确保从正确的库路径导入。 部分工具的使用可能还需要导入为了调用该工具而编写的额外模块（如下面的例子所示） 更为繁琐的可能需要在本地计算机上配置服务，或前往第三方平台配置密钥等，请参考具体的内置工具使用文档 from langchain.tools import WikipediaQueryRun from langchain_community.utilities import WikipediaAPIWrapper 这里就需要一个包装维基百科API调用的模块 做好了这些以后，可以实例化生成工具对象 wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper()) 如果你不知道一个内置工具的具体定义，可以调试如下基本属性（下面以tool为例）： tool.name名称：字符串，定义工具的名称，可以在实例化时指定 tool.description描述：字符串，描述工具的功能，可以在实例化时指定 tool.args参数：默认使用JSON格式（即python字典对象），每一条参数的形式为参数名：'title': 参数标题，'description':参数描述, 'type': 参数类型，其中参数名是唯一的key，参数标题和描述是可选的，提高开发时的可读性，例如： 'query': 'title': 'Query', 'type': 'string' return_direct：布尔变量，定义函数的输出是否是直接返回给用户的内容 如果一个工具只有一个参数，可以直接使用tool.run(参数)调用工具，否则，调用工具时传入完整的参数字典，格式为参数名: 参数值，例如： tool.run(query:langchain) 自定义工具 创建自定义工具有多种方法，可能会需要导入如下的包： from langchain.pydantic_v1 import BaseModel, Fieldfrom langchain.tools import BaseTool, StructuredTool, tool 使用@tool 装饰器 最简单的方法，只需要在函数定义前加上@tool修饰符 会默认使用函数名称作为工具名称，但可以通过传递字符串作为第一个参数来覆盖此名称。 函数需要具有一个python风格的docstring，这将成为@tool修饰符生成的工具的描述。 例如： @tooldef multiply(a: int, b: int) - int: Multiply two numbers. # 这个就是docstring return a * bprint(multiply.name)print(multiply.description)print(multiply.args)## 输出如下：multiplymultiply(a: int, b: int) - int - Multiply two numbers.a: title: A, type: integer, b: title: B, type: integer 覆盖定义工具属性的例子： # 这个类定义了一个参数输入的基本模式，用来覆盖原函数定义的参数class SearchInput(BaseModel): query: str = Field(description=should be a search query)# 覆盖了工具的名称，参数（使用自定义的SearchInput）和return_direct属性@tool(search-tool, args_schema=SearchInput, return_direct=True)def search(query: str) - str: Look up things online. return LangChainprint(search.name)print(search.description)print(search.args)print(search.return_direct)## 输出如下：search-toolsearch-tool(query: str) - str - Look up things online.query: title: Query, description: should be a search query, type: stringTrue 继承BaseTool子类 子类化BaseTool类来显式定义自定义工具 自由度最高，但需要更多的代码量 需要先继承BaseModel定义工具需要传入的参数，然后继承BaseTool定义工具的其他属性 举个例子：还是乘法计算器 from typing import Optional, Typefrom langchain.callbacks.manager import ( AsyncCallbackManagerForToolRun, CallbackManagerForToolRun,)# 定义Input Model时，参数名称为变量名，参数类型显示指定，使用一个Field对象定义可选的属性（标题，描述等）class CalculatorInput(BaseModel): a: int = Field(description=first number) b: int = Field(description=second number)class CustomCalculatorTool(BaseTool): # 定义工具的基本属性 name = Calculator description = useful for when you need to answer questions about math args_schema: Type[BaseModel] = CalculatorInput # 设置参数时需要显示指定类型 return_direct: bool = True # 重写run函数，实现工具的执行 def _run( self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None ) - str: Use the tool. return a * b # 重写_arun函数，实现工具的异步执行 # 如果工具不需要异步执行，可以直接返回异常 async def _arun( self, a: int, b: int, run_manager: Optional[AsyncCallbackManagerForToolRun] = None, ) - str: Use the tool asynchronously. raise NotImplementedError(Calculator does not support async)# 实例化一个自定义工具search = CustomSearchTool()print(search.name)print(search.args)## 输出如下：Calculatora: title: A, description: first number, type: integer, b: title: B, description: second number, type: integerTrue 继承 StructuredTool 数据类 相当于混合了前两种方法。 有一定的自由度，同时写法简单。 先定义好一个函数，然后使用StructuredTool.from_function方法，传入工具的属性即可。 举例： # 工具要执行的函数def search_function(query: str): return LangChain# 工具参数会默认设为函数参数search = StructuredTool.from_function( func=search_function, name=Search, description=useful for when you need to answer questions about current events, # coroutine= ... - 同样可以指定异步执行的方法) 如果需要自定义参数覆盖函数的参数，可以指定args_schema: # 自定义参数类class CalculatorInput(BaseModel): a: int = Field(description=first number) b: int = Field(description=second number)# 工具要执行的函数def multiply(a: int, b: int) - int: Multiply two numbers. return a * bcalculator = StructuredTool.from_function( func=multiply, name=Calculator, description=multiply numbers, args_schema=CalculatorInput, return_direct=True, # coroutine= ... - 同样可以指定异步执行的方法) 错误处理 如果工具执行函数的过程种遇到异常并返回，正常情况下智能体会终止当前任务。 如果希望智能体在遇到某些影响不大的异常后也可以继续执行，需要在函数执行体内抛出ToolException 并相应地设置 handle_tool_error 。 当工具抛出 ToolException 时，代理不会停止工作，而是根据工具的异常处理函数handle_tool_error处理异常，处理结果将作为观察返回给代理，并以红色打印。 from langchain_core.tools import ToolExceptiondef search_tool1(s: str): raise ToolException(The search tool1 is not available.)search = StructuredTool.from_function( func=search_tool1, name=Search_tool1, description=A bad tool, handle_tool_error=True, # 添加异常处理属性标记)search.run(test) ### 输出：The search tool1 is not available. 如果只抛出了ToolException 异常而没有定义 handle_tool_error属性，则LangChain仍然不会让智能体处理异常。 当handle_tool_error属性仅定义为True而不是异常处理函数时，LangChain会将异常字符串作为处理结果返回给智能体。 handle_tool_error属性定义为一个函数时，其需要接受ToolException类型的异常对象，并返回字符串。 def _handle_error(error: ToolException) - str: return ( The following errors occurred during tool execution: + error.args[0] + Please try another tool. )search = StructuredTool.from_function( func=search_tool1, name=Search_tool1, description=A bad tool, handle_tool_error=_handle_error,)search.run(test)### 输出：The following errors occurred during tool execution:The search tool1 is not available. Please try another tool. 在OPENAI聊天模型中使用工具 如果你只是需要一个工具函数，或是觉得LangChain中的内置工具很好用，但不需要一个智能体来决定工具的调用，可以将工具调用显式地嵌入ChatOpenAI聊天模型的提示中： from langchain.schema import HumanMessagefrom langchain_openai import ChatOpenAIfrom langchain.tools import MoveFileTool, format_tool_to_openai_functionmodel = ChatOpenAI(model=gpt-3.5-turbo-0613)tools = [MoveFileTool()]functions = [format_tool_to_openai_function(t) for t in tools]message = model.predict_messages( [HumanMessage(content=move file foo to bar)], functions=functions) 自定义智能体 智能体其实只是一个带有 工具调用和记忆的 LLM查询或对话系统，在底层完全可以使用LangChain的ICLE语法（即“链”）来实现。在下面的例子中，使用ChatOpenAI作为基础对话模型来实现自定义智能体。 先定义好要使用的模型和工具 在对LLM提示的时候，记得加上MessagesPlaceholder,以实现和智能体的对话具有历史记忆能力。 from langchain_openai import ChatOpenAIfrom langchain.agents import toolfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.messages import AIMessage, HumanMessagefrom langchain_community.tools.convert_to_openai import format_tool_to_openai_function@tooldef get_word_length(word: str) - int: Returns the length of a word. return len(word)tools = [get_word_length] # 定义工具chat_history = [] # 记录历史llm = ChatOpenAI(model=gpt-3.5-turbo, temperature=0) # 定义llm# 将工具绑定到llmllm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])MEMORY_KEY = chat_historyprompt = ChatPromptTemplate.from_messages( [ ( system, You are very powerful assistant, but bad at calculating lengths of words., ), MessagesPlaceholder(variable_name=MEMORY_KEY), (user, input), MessagesPlaceholder(variable_name=agent_scratchpad), ]) 然后，从langchain.agents.format_scratchpad包中导入自定义创建智能体的基础类型。 这里，因为我们需要将函数调用转化为OPENAI api的形式，所以导入format_to_openai_function_messages函数，并同时导入OpenAIFunctionsAgentOutputParser来解析OPENAI 模型返回的输出。 使用链式方法创建智能体，通常一个链条的构成是： 输入：即核心概念中的AgentAction，将“用户输入”，“中间步骤”和“历史”三个部分按照字典键值对的格式配置。 必要的时候（如使用OPENAI api的工具调用），要将输入转化为模型接口支持的格式。 提示：按照已定义好的提示模板 已绑定好工具的LLM对象或聊天模型对象 解析输出的对象，根据你查询的模型决定。 from langchain.agents.format_scratchpad import format_to_openai_function_messagesfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParseragent = ( input: lambda x: x[input], agent_scratchpad: lambda x: format_to_openai_function_messages( x[intermediate_steps] ), chat_history: lambda x: x[chat_history], | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()) 最后，创建一个智能体执行器，配置好输入和历史即可执行： from langchain.agents import AgentExecutoragent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)input1 = how many letters in the word educa?result = agent_executor.invoke(input: input1, chat_history: chat_history)chat_history.extend( [ HumanMessage(content=input1), AIMessage(content=result[output]), ])input2 = is that a real word?agent_executor.invoke(input: input2, chat_history: chat_history)... AgentExecutor 的额外参数 verbose=True：是否输出详细执行信息 max_iterations=10：限制最大迭代次数 max_execution_time=10：限制最大执行时间（秒） handle_parsing_errors=True：是否自动处理解析错误（见下面的小节) handle_parsing_errors=...：处理解析错误，并使用自定义的字符串（见下面的小节) 智能体执行的监督和检查 按步骤执行 调用AgentExecutor类对象的iter函数，获得每一个中间步骤的迭代器。由此，可以自由控制执行哪些步骤，并随时可以插入其他功能的代码。 question = What is the product of the 998th, 999th and 1000th prime numbers?for step in agent_executor.iter(input: question): # 这种写法等价于: # output = step.get(intermediate_step) # if output: if output := step.get(intermediate_step): action, value = output[0] if action.tool == GetPrime: print(fUsed Tool GetPrime，get number： value ...) assert is_prime(int(value)) # 询问控制台用户是否继续Agent执行 _continue = input(Should the agent continue (Y/n)?: ) or Y if _continue.lower() != y: break 获得中间步骤的日志 智能体执行完毕后，agent_executor.invoke会返回一个对象，其中包含所有执行历史的情况，其结构如下： response = agent_executor.invoke(input: What is Leo DiCaprios middle name?)print(response.keys())# TODO 要获得每一个中间步骤的详细情况，获取response[intermediate_steps]即可，这是一个列表，其中每个元素是一个元组，包含两个对象，分别是： 一个AgentActionMessageLog对象，记录当前步骤调用工具的详细信息（包括工具名称，参数，日志字符串，实际嵌入LLM提示中的AIMessage对象等） 如果步骤是“输出结果返回给用户”，则这里会是一个AgentFinish对象。 一个字符串，为调用工具返回的“Observation” [( AgentActionMessageLog( tool=Wikipedia, tool_input=Leo DiCaprio, log= Invoking: `Wikipedia` with `Leo DiCaprio` , message_log=[ AIMessage( content=, additional_kwargs= function_call: name: Wikipedia, arguments: __arg1: Leo DiCaprio ) ] ), Page: Leonardo DiCaprio Summary: Leonardo Wilhelm DiCaprio (; Italian: [diˈkaːprjo]; born November 1)] 自我检查错误 注意：这和之前介绍的工具类的错误处理不同，前者只是处理调用工具时，工具可能发生的错误。而这里是从智能体的层面上，防止LLM在输出对任务的推理时出现幻觉，产生不符合规定格式的字符串（如json格式，ReAct范式等） 在实例化任意 AgentExecutor的时候加入参数handle_parsing_errors=True，可以使LangChain自动处理智能体的不当输出，这主要包括无效的格式或不完整的回应。 agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, handle_parsing_errors=True) 例如，如果Agent使用“ReAct策略，但LLM并未遵循提示输出“Action: ”字段时，LangChain将向LLM自动重发一条消息，告知错误格式的位置的应当的输出，以此尝试引导LLM纠正错误。 Entering new AgentExecutor chain...# LLM的返回，缺少 Action: 字段Thought: I should search for Leo DiCaprio on WikipediaAction Input: Leo DiCaprio# LangChain生成的错误反馈，将作为下一步的Input自动输入给LLMInvalid Format: Missing Action: after ...# LLM的下一步返回，这次的格式正确Thought:I should search for Leonardo DiCaprio on WikipediaAction: WikipediaAction Input: Leonardo DiCaprio# 正确执行工具调用后，返回给LLM工具调用的输出结果Page: Leonardo DiCaprio Summary: Leonardo Wilhelm DiCaprio (; Italian: [diˈkaːprjo]; born November 1 ... 如果要将自动生成的错误反馈修改为自定义的字符串，显示指定参数handle_parsing_errors时传入字符串而非布尔值即可。 agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, handle_parsing_errors=Check your output and make sure it conforms, use the Action/Action Input syntax,) 返回结构化输出 上面的所有情况，智能体都以字符串输出结果，如果想要智能体也输出结构化的数据，以便结合到更大范围的系统中。 我们可以将“返回给用户”这一步骤也视为一个工具调用，只不过调用的结果是输出结构化的文本。在LangChain种，可以定义Response类来实现这一功能。 【注意：以下示例以基于OPENAI聊天模型的智能体为例】 from typing import Listfrom pydantic import BaseModel, Fieldclass Response(BaseModel): Final response to the question being asked answer: str = Field(description=The final answer to respond to the user) sources: List[int] = Field( description=List of page chunks that contain answer to the question. Only include a page chunk if it contains relevant information ) 然后，在定义工具时，使用函数convert_pydantic_to_openai_function将Response类定义为工具函数: from langchain_openai import ChatOpenAIfrom langchain_community.tools.convert_to_openai import format_tool_to_openai_functionfrom langchain.utils.openai_functions import convert_pydantic_to_openai_functionllm = ChatOpenAI(temperature=0)llm_with_tools = llm.bind( functions=[ # 其他工具，例如一个 retriever tool format_tool_to_openai_function(retriever_tool), ... # Response 结构化返回工具 convert_pydantic_to_openai_function(Response), ]) 定义 解析Response工具输出的函数。这里我们没有使用预置智能体，因此引入名称AgentFinish和AgentActionMessageLog来包装LLM输出的中间步骤数据结构，以便得到可以被LangChain其他模块解析的对象。 import jsonfrom langchain_core.agents import AgentActionMessageLog, AgentFinishdef parse(output): # 如果LLM没有调用任何函数，将自定义回答结果直接包装为AgentFinish if function_call not in output.additional_kwargs: return AgentFinish(return_values=output: output.content, log=output.content) # 如果LLM要调用某个非Response类的函数，将其函数名和参数抽取出来 function_call = output.additional_kwargs[function_call] name = function_call[name] inputs = json.loads(function_call[arguments]) # 如果LLM调用了Response函数准备返回结构化数据，将返回包装为AgentFinish。 if name == Response: return AgentFinish(return_values=inputs, log=str(function_call)) # 否则，将返回包装为 agent action，这将使LangChain代替我们执行函数并继续下一步与智能体的交互。 else: return AgentActionMessageLog( tool=name, tool_input=inputs, log=, message_log=[output] ) 最后，使用parse作为链步骤中解析智能体输出的动作： from langchain.agents.format_scratchpad import format_to_openai_function_messagesprompt = ChatPromptTemplate.from_messages( [ (system, You are a helpful assistant), (user, input), MessagesPlaceholder(variable_name=agent_scratchpad), ])agent = ( input: lambda x: x[input], # Format agent scratchpad from intermediate steps agent_scratchpad: lambda x: format_to_openai_function_messages( x[intermediate_steps] ), | prompt | llm_with_tools | parse)agent_executor = AgentExecutor(tools=[retriever_tool], agent=agent, verbose=True)agent_executor.invoke( input: ..., return_only_outputs=True,) 工具包：Toolkits 对于许多常见任务，代理将需要一组相关的工具。为此，LangChain提供了工具包的概念——实现特定目标所需的大约3-5个工具的组合。 使用工具包非常方便，只需导入工具包的名称，然后使用get_tools()方法即可返回一个列表，包含该工具包中的所有工具，你可以直接将该列表作为参数传入实例化Agent的方法中，轻松愉快！ # Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()# Create agentagent = create_agent_method(llm, tools, prompt) Langchain内置工具包 所有内置工具包的使用文档：https://python.langchain.com/docs/integrations/toolkits COT TOT 和 思维图的实现"},{"title":"Nickbit 2023影视游艺总结","path":"/2023/12/31/Nickbit2023影视游艺总结/","content":"祝 新年快乐！ 今天是2023年的最后一天，如果你看过这个博客的一些历史帖子的话，你会发现有两个往年的影视游艺总结列表……没错它们是我当你想着“要好好记录一下每年都体验了什么有价值的作品”而创建的列表，然后事实就是，它们都没有在来年的1月1日之前被好好填写过。总之不是烂尾了就是什么时候突然想起来了马后炮式的补了 一下。以至于我现在回过头看，表里填的某些内容更是一点印象都找不回来了。 因此，今年决定痛定思痛，从列表的形式改为一篇完整的图文，并且一定要在2023.12.31晚上之前发布出来。当然体验过的内容不一定都是今年发布的，如果是老游戏/补番之类的会特别标注出来。此外，按照惯例我会对所有完整体验过的作品打分，该分数纯粹主观，但还是分为了客观和主观两个部分： 客观分数：将我的价值观对齐接触过的大多数社区和自媒体得到的评分，满分10分，最小分值0.1 主观分数：完全以我的喜好评定的分数，满分2分，最小分值0.5 好了，废话不多说，来看下2023年都看了玩了乐了写啥吧…… 游戏篇 神之天平 （ASTLIBRA Revision） 发行时间：2022.10 体验时间：1月-3月 评分：11（9.6 + 1.5 = 11.1） 虽然实际上这是22年的作品，但是不妨碍是整个寒假玩的最尽兴的游戏，荣获Nickbit 2023除夕夜推荐游戏，让我们引用一下当时的评语： 古老的横版机制，看起来胡乱的美术贴图，风格各异的配乐， 都没法掩盖一个内容充实，引入入胜，震撼人心的故事。 2022的最后给我这个独立游戏荒年一个最大的惊喜！ （这么一想确实，去年玩的游戏都没有什么让我特别惊喜的内容，而今年下面这些可都是重量级……好起来了啊！） 霍格沃茨之遗 发行时间：2023.2 体验时间：2月-4月 评分：9+（8.2 + 1.5 = 9.7） 我是隐藏哈迷，因此游戏出了很快我就开始玩了，虽然顶着当时已经用了4年的1650显卡笔记本，多数情况下30帧都跑不到，我还是顺利通关了主线内容。就游戏质量来说，《霍格沃茨之遗》是比较抽象的，尤其是当我换了高配电脑再来玩，发现它的优化确实很好，但是图像和光影质量还是不够magic，与哈利波特电影的质感相差甚远。至于开放世界的内容设计，简单概括为魔法《原神》（笑），但作为开发商的开发世界的处女作，我觉得无可厚非。有两个设计可以上大分：一是设计主角设计是扮演“我自己”，二是骑扫把的竞速关卡。 渔帆暗涌 发行时间：2023.3 体验时间：3月-4月 评分：9（8.1 + 1 = 9.1） 很有意思的小品游戏，整体设计非常工整，经营+探索+线性剧情链，克系恐怖要素自然吓不倒我这个经验丰富的COC调查员。不过自由度仍然不够高，如果，我是说如果，能和古神对战的话…… 塞尔达传说：王国之泪 发行时间：2023.5 体验时间：5月-6月 评分：11+（10 + 1.5 = 11.5） 我知道你们要说“唯一真神”了，不过可惜当时忙着毕设，基本上是匆匆过了主线，然后就没有打开过了。我其实没有玩过野炊，甚至云也没有云多少，但这不妨碍我对新世代的塞尔达传说系列有了一套自己的理解。任天堂的开放世界设计其实是一个“神经网络”，在这个无比空旷和丰富的世界里，你可以自由地做一切，但故事的结局总是一定的。你可以走无数的不同路径，但最终都会输出到一个节点。如果你不把自己扮演成林克，那么始终无法代入到这个梦幻的世界里，但如果把自己带入到林克，那孤独的感觉又会瞬间涌上心头……就从这个角度来说，王泪货真价实的艺术品 博德之门3 发行时间：2023.8 体验时间：8月-9月 评分：11+（9.8 + 2 = 11.8） 真正的“唯一真神”，GOTY！！！我也不知道这东西有什么魔力，让我的暑假平白无故地多了几个几乎通宵的晚上……只能说骰子虽然简单，但是它的魅力真的让人欲罢不能（对，就是说SL了两小时没投出一个重击这件事）。当然拉瑞安的网状叙事设计功不可没，博德之门3构建了一种与塞尔达完全不同的开发世界范式，剧情不再收敛到某个节点，而是你的自由选择，虽然现在还做不到像TRPG一样主持人的完全自由，但也许这才是未来…… ViewFinder 发行时间：2023.7 体验时间：8月-9月 评分：10（8 + 2 = 10） 非常惊艳的游戏，玩法创意无敌，属于是又刷新了我关于玩法设计的世界观。最后的限时挑战也很有紧张感，不过被不少人批评破坏了整体的游戏体验感hhhh。对于过度设计，批评的声音确实很到位，本作不少玩法设计都有虎头蛇尾的感觉，开始很惊艳，但是很快到下一个关卡就被抛弃转到另一个解谜思路去了，也许“短视”是这个实验性作品最大的遗憾。授予“年度最佳创意”。 PS：那只猫很可爱 星之海 Sea of Stars 发行时间：2023.8 体验时间：10月 评分：8+（8.2 + 0.5 = 8.7） 被普遍认为“名不副实”的年度最佳独立。在玩的时候我确实也是经历了一波“期待”——“失望”——“又期待”——“又失望”的过山车历程。在我玩到1/10的时候我以为我已经打完了1/3，当我玩到1/3的时候我以为我要打最终boss了，而当我穿越星之海发现还有一个新的大地图时，这个时候，相信我，已经不会再有新鲜感了。**内容表达的拖沓时这个游戏最大的问题。**尽管如此，我还是通关了。剧情虽然着实老套但好在表现力不错（虽然但是我也觉得叫“加尔传”更合适）。最实至名归的估计还只能是花费了大量心血的美术了，在此授予“年度最佳像素美术”。 巴别塔圣歌 发行时间：2023.9 体验时间：11月 评分：11（9.2 + 2 = 11.2） 我只玩了一个下午，然后在群里就是这么说的：“符号学爱好者直接高潮” 非常电波系的游戏，语言破译和符号解密的玩法自然只有爱好者才会被深深吸引，路人可能第一章就放弃了。但是游戏整体的美术风格和线条设计非常戳我，以至于在本文中获得了最多的图片展示位（通关了以后立刻就作为了我的桌面壁纸）。其实破译玩法的整体难度不大，这并非是一个硬核解密游戏，游戏内的文法也不是能拓展到真实的世界的，也许这反而是一个“不过度设计”的优秀例子。授予“年度最佳独立游戏”。 巴别号漫游指南 稀有的多线叙事的解谜游戏，逻辑闭环非常到位，解谜引导也十分贴心。在此佩服作者的想象力和逻辑力。故事咋一听不知所云，玩到一半就豁然开朗。看似是一个童话故事，但其实神似《去月球》、《灵魂摆渡者》，《巴别号漫游指南》是一个立意死亡教育的“罗曼·罗兰式”故事，都这个高度了，我是绝对不会批评的，授予“年度最佳独立剧情”，希望大家有生之年都能体验一番。 发行时间：2023.8 体验时间：11月-12月 评分：10（8.8 + 1.5 = 10.3） 节奏医生（第五章） 本来没有打算记录一些游戏的更新/长期运营内容的，但是《节奏医生》的新关卡我觉得有必要点一下，这次的设计直接从“类音游”拔高到了“节奏天国”的高度，无论是歌曲质量、演出和剧情立意都在保持短小精悍的同时有了极大的突破。打完的晚上心情久久不能平复……希望还能看到更好的！（如果能再优化优化难度曲线的话（菜鸡发言）） 更新时间：2023.11 体验时间：12月 评分：10+（9.1 + 1.5 = 10.6） 下面是一些我还没有通关或是中途弃坑的游戏： 潜水员戴夫 发行时间：2023.9 “最佳独立游戏”的有利竞争者，内容及其丰富是它的优势，但可惜不是我的菜。玩了几个小时有一种好像什么玩法都有，但是什么玩法都不够吸引我的感觉，加之剧情中卡了一次关，遂放弃，后面也没有继续玩过……玩到过程中常常有“这个任务也要做，这个任务也要做，要赚钱，还要想着晚上寿司店经营的部分……”有一种上班的痛苦…… 阿斯特赖亚：六面先知（Astrea: Six-Sided Oracles） 发行时间：2023.9 同样是被捧为“今年最佳肉鸽DBG”的游戏，但我只玩了新手的第一把就没有继续了……不是它不好玩，是我的问题：我的时间真的非常值钱……只玩了一把就能敏锐地发现，这个游戏玩起来真的很累：7滴血一条命的设计，如果不能保持高度的计算集中很容易翻车，然后等待你的就是无尽的SL。但是创意确实很好。 14种扫雷变体 发行时间：2022.11 室友生日友情赠送，顺便给我秀了一下1000小时的游戏时间。我：“就一个扫雷哪有这么耐玩……”打开过了几关：“对不起，对不起，这东西有毒，我觉得以我的意志力，这东西不能碰，不能碰……” 戴森球计划：黑雾崛起 发行时间：2023.12 “什么？你染上戴森球了？！！” 对不起，对不起，这东西更不能碰……但是圈养黑雾真的很有意思，duangduang材料就掉出来了，母星都不用出，戴森球就建好了……等等我箱子怎么又堵住了？ （此时博主的理智告诉它，完整内容还是等放寒假再接着玩吧，否则期末小命不保） 动漫篇 注意，对于影视作品和动漫的客观评分，会比游戏要苛刻，也许这是受到了bangumi的影响？（平衡党罪大恶极） 伍六七之暗影宿命 开播时间：2023.1 类型：追番 评分：8（7.2 + 1 = 8.2） 一贯的高水准制作国漫，伍六七的世界观和剧情我不过多评价，不过这一季的主题曲是真滴好听。 孤独摇滚 开播时间：2022.10 类型：补番 时间：6月-7月 评分：10+（8.8 + 2 = 10.8） 去年最大黑马，前室友大力推荐，我却因为对乐队番毫无兴趣直接pass的错付动漫。还好一毕业了，开始感到内心的空虚，于是拾起来看了看：啊，这就是青春啊。谢谢你波奇酱，救下了一个差点在暑假死去的心灵……还顺便改变了我对乐队题材的看法，然后有了…… BanG Dream! It’s MyGO!!! 开播时间：2023.6 类型：追番 评分：10+（8.9 + 2 = 10.9） “什么？你染上M……”不是，停停停，这个不是这样的。 我要在这里郑重声明：本人不是MyGO小鬼，我是邦邦婆罗门（划掉），今年3月入坑，那是动画还没官宣。音游人玩玩游戏听说邦邦歌曲多氪金少有阿b代理国服（虽然最后还是为了抽MyGO转战了日服）没毛病吧…… 所以你们快去看吧，我什么都会做的！有人没看过这个故事，我都会心痛的！ BanG Dream! （1-3季） 开播时间：2017.1 - 2020.4 类型：补番 时间：9月-11月 评分：8（7 + 1 = 8） 染上MyGO症状1：跑去补完前三季，客观点说，邦邦的前五年动画确实就是一个粉丝向作品，剧情非常套路，也吸引不来外人，入坑了以后当了解角色的补充材料看才是正解。 少女☆歌剧 Revue Starlight （TV+剧场版） 开播时间：2018.7（TV） 2021.6（剧场版） 类型：补番 时间：9月-10月 评分：TV： 8+（7.9 + 1） 剧场版： 10（8.5 + 1.5） 染上MyGO症状2：跑去看扭曲长颈鹿……相对于TV版，剧场版的后劲才是更大的，这是一种反正我从未见过的表达形式，并且极具冲击力。我认为少歌的立意是这些“扭曲怪”番里最高的，少女不是重点，成长也不是，甚至说台词的人都可以被解构掉，但是她还是仍然有一种奇妙的魅力，啊，哇卡利马斯.jpg（完全没搞懂） 奇巧计程车 ODD TAXI 开播时间：2021.4 类型：补番 时间：10月-11月 评分：10（8.5 + 1.5 = 10） 染上MyGO症状3：……不是，这个没关系吧。不过确实是追完了/补完了前面这些原创，突然觉得应该把近两年的好评原创都补一下，于是翻到的《奇巧计程车 》（当年和《奇蛋物语》同台竞技的时候我选择了看后者，唉残念……），事实证明也没有让我失望，漂亮的核心诡计和多线叙事，“动漫化日剧”名不虚传。 葬送的芙莉莲 开播时间：2023.10 类型：追番 评分：10（8.8 + 1.5 = 10.3） 2023公认年度最佳动漫，因此在今年的最后也开始跟上潮流追番，芙莉莲给我的最大观感是无虑。看这部动画没有任何负担，似乎每一个故事都是真的在哪个遥远的大陆发生，而和你又毫无关系。只需放轻松，享受这个虚构却又无比真实的故事……这是一种多么美好的感觉，也许就是神作为什么被称为神作吧…… 影视篇 流浪地球2 开播时间：2023.1 评分：10（8.6 + 1.5 = 10.1） 春节期间凌晨观影。帅是真滴帅，看出了编剧突破大刘宇宙的野心也是真的，至于一些缺点，瑕不掩瑜还是比较中肯的。 三体（电视剧） 开播时间：2023.1 评分：9（8.5 + 0.5 = 9） 三体电视剧，其实我没有看到大结局，但是我觉得有必要点一下，毕竟我几乎不看电视剧，这次是三体的IP终于有能成功影视化的转机了我才一看。当然，我的评价是非常不错的，虽然我脑子的里的三体不一定是这个味，但剧版三体的味至少还是香的。 八角笼中 上映时间：2023.7 评分：8（7.5 + 0.5 = 8） 看之前没抱希望，看到过程中有点震撼，看到结局有点可惜的作品，但是我的评价依然是“好”，毕竟声音还是越多越好。 奥本海默 上映时间：2023.7 评分：10+（9.2 + 1.5 = 10.7） 谨以此段怀念汤晓鸥老师，感谢他在生命的最后三个月时（也许他自己还不知道）还在讲座中力荐这部影片。“如今我已成为死神，世界的毁灭者。”，谁又知道人工智能会是什么样？ 关于电影本身，我的观感和看《模仿游戏》有一些共鸣，都是伟大人物的传记，都是复杂和严重的心理问题，都是社会、世界甚至宇宙的思考。"},{"title":"MuG Diffusion v1.0 源码解析","path":"/2023/12/27/MuG-Diffusion-v1-0-源码解析/","content":"警告，包含大量GPT4分析和代写内容，可能存在谬误 原仓库地址：https://github.com/Keytoyze/Mug-Diffusion （1）数据集准备部分 数据准备过程概况如下： 把下载好的数据集（osu谱面）放在data/文件夹下，可以准备一个子文件夹 数据集文件夹下新建一个beatmap.txt，记录谱面元数据，用于下一步 按照configs/mug/mania_beatmap_features.yaml中的配置，抽取谱面的特征 谱面特征需要转化为数据表，存放与beatmap.txt同一目录下的features.db数据库中 至少准备两个这样的数据集：一个训练集一个验证集 [mug/data] mug/data/convertor.py: 工具类，用来解析单个.osu谱面文件 mug/data/dataset.py: 构建用于pytorch训练的数据集格式，在此之前，请确保已经抽取了谱面特征，并存放在features.db中 mug/data/utils.py: （待分析）一些用于对齐的工具函数 准备好的数据集条目示例： meta: path: data/beatmap_4k/1469980 Silentroom vs. Frums - Aegleseeker/Silentroom vs. Frums - Aegleseeker ([Crz]Alleyne) [The Last Observation].osu, audio: data/beatmap_4k/1469980 Silentroom vs. Frums - Aegleseeker/audio.ogg, game_mode: 3, cs: 4.0, version: The Last Observation, set_id: 1469980 , convertor: frame_ms: 46.439909297052154, max_frame: 4096, mirror: False, random: False, mirror_at_interval_prob: 0, offset_ms: 0, rate: 1.0 , note: array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), valid_flag: array([1., 1., 1., ..., 0., 0., 0.]), audio: array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), feature: array([ 24, 39, 41, 53, 55, 58, 81, 110, 124, 142, 157, 179, 192, 212, 225, 237, 259, 275, 293, 314, 327]) note：谱面音符的数据（文本 - 向量） feature：谱面特征的数据，osu元数据和ett分数等 （文本 - 向量） audio：音频的张量数据 （有一个问题是，模型是怎么处理长度不同的音频的？） valid_flag：？一个标记向量，用于标记谱面中的音符是否有效，即是否在音频中有对应的音符？ # 数据集中各张量的shape[note].shape: (16, 4096)[audio].shape: (128, 32768)[feature].shape: (21,)[valid flag].shape: (4096,) 处理音频的缓存.npz文件放置于data/audio_cache文件夹中。 [scripts] 如果我的判断没错，准备数据这个阶段时完全手动的，看到这个文件夹下的脚本都没有被引用过…… scripts/prepare_beatmap.py: 用本地下载的osu谱面于制作beatmap.txt，给定输入输出文件夹 scripts/prepare_ranked_beatmap.py: 从osu网站上爬取谱面，制作beatmap.txt，需要用到kuit自己的网站服务 scripts/prepare_beatmap_from_ranking_mapper.py: 同上，不过似乎是从特定的谱师id中获取 scripts/prepare_beatmap_features.py: 从beatmap.txt中读取谱面元数据，抽取谱面特征，并提交到features.db中 抽取特征需要以下参数： beat_map_path = beatmap.txt 文件的路径，记得铺面文件的存放位置与该txt位于同一目录features_yaml = 配置feature文件的路径，如configs/mug/mania_beatmap_features.yamlosu_tools = 一个osu官方开源的计算osu谱面信息的工具，在这里下载：https://github.com/ppy/osu-tools/tree/master/PerformanceCalculator获得源码后，需要使用dotnet编译（参见仓库readme），编译完成后，在\\PerformanceCalculator\\bin\\Debug et6.0\\下可以找到PerformanceCalculator.dll文件，使用该文件的路径作为参数ranked_map_path = 存储每个谱面的rank（谱面审核）信息，该信息需要通过预处理获取，如果没有，可以指定为Nonedotnet_path = 为了运行PerformanceCalculator.dll，需要安装.NET6.0，安装并设置完成环境变量后，可直接使用填入dotnet scripts/filter_beatmap.py: （待分析）似乎是用于过滤谱面中的变速 …… 其他脚本似乎是用于处理Etterna谱面的，或者是一些用于算法测试的（破案了，是原来stable diffusion的一些测试代码），跳过这一部分 数据集配置 如果你要训练，必须准备基本的两个数据集文件夹： mug.data.dataset.OsuTrainDataset mug.data.dataset.OsuVaildDataset 将数据集路径指定在训练用的配置文件中，目前的路径是configs/mug/mug_diffusion.yaml， 同时最好有一个mug.data.dataset.OsuTestDataset数据集，用于测试. 训练数据集示例结构： TODO （2）VAE编解码部分 这部分的目的是为了得到两个模型： Encoder：将谱面信息编码为一个向量 Decoder：将一个向量解码为谱面信息 [mug/firststage] 这部分用于实现AutoEncoder，自动编码器是一种无监督学习的神经网络模型，主要用于学习输入数据的压缩表示 AutoEncoder的结构如下： 其中包含两个torch.nn.Module类的组件：编码器（Encoder）和解码器（Decoder）。编码器的作用是将输入数据压缩为一个低维度的表示，而解码器则将这个低维度的表示恢复为原始的高维度数据 注意：mug/firststage/autoencoder.py中的部分模型使用的网络模块定义在mug/model/modules.py中 Encoder模型结构： 输入通道数C=16 经过一个3*3的卷积层，输出中间通道数64 经过一系列下采样操作：在每次下采样之前，先经过一个残差块，并让每两次下采样后，网络的通道数翻倍 即: 64 * [1,1,2,2,4,4] 下采样网络：可选 卷积 或 平均池化 卷积：3*3卷积层，stride=2 平均池化：2*2平均池化层，stride=2 中间层：使用2个残差块，每个残差块包含两个3*3卷积层，通道数不变 输出层：首先通过一个归一化层对数据进行归一化处理，然后通过一个33卷积层将数据的通道数转换为编码器的输出通道数，输出 目标输出通道数2 的向量表示 如：目标输出通道数为32，则输出64维的向量表示 Decoder模型结构： 与Encoder模型结构正好相反，先将输入向量卷积到中间层，然后再经过一系列上采样操作，最后经过归一化和一个3*3卷积层，输出目标输出通道数的向量表示 损失函数：AutoEncoderKL类使用了两个损失函数，分别是重构损失和KL散度损失 重构损失：在mug/firststage/losses.py中定义，计算重构的谱面数据与原始谱面数据的误差，包括多种误差计算方式 KL损失：计算编码器输出的均值和方差与标准正态分布的KL散度 KL损失在加入总损失时，需要乘以一个权重系数self.kl_weight，默认为0（不考虑KL损失） AutoEncoder模型的使用： 参见下面diffusion的部分 （3）Diffusion训练部分 条件嵌入模型 [mug/cond] mug/cond这个文件夹里实现的都是条件编码模型，即将谱面的特征提示信息和音频信息一起编码，然后输入Diffusion模型的去噪过程，用来控制输出结果的条件向量。 特征输入：特征定义在符合yaml格式的文本上，里面的信息包括如 难度sr，long note比例，键形特征等，训练时特征存储在数据集的database里，推理是可以手动输入，或是从一张想要参考的谱面中提取出来 音频输入：通过频谱变换离散化后的音频数据，有多种处理方式 noise level：[不知道这个是在哪里被嵌入的……] feature_cond.py - BeatmapFeatureEmbedder 谱面特征（提示）嵌入 主要模型为BeatmapFeatureEmbedder的类，该类继承自torch.nn.Module，用于实现特征编码和嵌入。 这里嵌入的特征是谱面元数据，即configs/mug/mania_beatmap_features.yaml中定义的特征。该文件路径需要作为参数传入的构造函数中。 读入mania_beatmap_features.yaml文件的信息后，会使用mug/util.py中的count_beatmap_features函数，计算每个特征需要在嵌入向量中用几个值来表示，例如，一个bool类型的特征需要3个值（表示True/False或者没有这个特征）。 使用 torch.nn.Embedding创建一个嵌入层self.embedding，其维度为传入的参数的embed_dim。 def __init__(self, path_to_yaml, embed_dim): super().__init__() with open(path_to_yaml) as f: self.feature_dicts = yaml.safe_load(f) self.embedding = torch.nn.Embedding(count_beatmap_features(self.feature_dicts), embed_dim) forward方法是模型的前向传播过程。它接收一个输入x，将其转换为长整型，然后通过嵌入层进行转换。最后，使用rearrange函数调整张量的维度顺序。 pattern解释： 输入时二维向量，其形状为[B, F]，其中B代表批次大小，F代表特征数量。 经过嵌入层后，输出的形状为[B, F, H]，其中H代表嵌入向量的维度，即参数embed_dim。 def forward(self, x): x = rearrange(self.embedding(x.long()), b f h - b h f) # [B, H, F] return x summary方法使用torchsummary库的summary函数，打印出模型的概要信息，包括输出大小、参数数量、内核大小等。 def summary(self): import torchsummary torchsummary.summary(self, input_data=[5, ], dtypes=[torch.long], col_names=(output_size, num_params, kernel_size), depth=10, device=torch.device(cpu)) 测试：输入一个特征配置文件，打印出模型的概要信息。 if __name__ == __main__: BeatmapFeatureEmbedder(path_to_yaml=configs/mug/mania_beatmap_features.yaml, embed_dim=128).summary() wave.py - 音频数据嵌入 这里定义了STFTEncoder，MelspectrogramEncoder等模型，主要实现将音频信息编码为嵌入向量 STFTEncoder类是一个音频编码器，使用了短时傅里叶变换（STFT）进行音频编码。 MelspectrogramEncoder和MelspectrogramEncoder1D类是另外两种音频编码器，使用了梅尔频谱图（Melspectrogram）进行音频编码。 S4BidirectionalLayer类是一个双向S4模型层，它包含了一个归一化层和一个双向S4模型。 TimingDecoder类是一个解码器，它用于将编码后的音频数据解码回原始数据 MelspectrogramScaleEncoder1D类是一个一维的梅尔频谱图编码器，它使用了注意力机制进行音频编码。 实际模型训练时，通过指定yaml配置文件中的wave_stage_config字段选择音频编码模型。 在已知的两种的配置中，编码音频分别使用了STFTEncoder和MelspectrogramScaleEncoder1D模型，分别对应配置文件mug_diffusion_stft.yaml 和mug_diffusion.yaml LDM/DDPM模型 - [mug/diffusion] 模型训练使用的是Pytorch-Lightning框架，这是一个pytorch深度神经网络api的抽象和包装。它的好处是可复用性强，易维护，逻辑清晰等。 主要的代码结构(本项目主要在mug目录下)： -mug/ |-data/ |-... 数据集的中间接口 |-model/ |-... 模型的中间接口 -lr_scheduler.py -util.py-main.py 定义命令行参数，实例化数据集、模型、Trainer，开始训练 diffusion.py - DDPM模型 mug/diffusion/diffusion.py是扩散模型的核心文件，根据DDPM论文的方法实现了整个框架模型，因此类名也叫DDPM： DDPM继承了pl.LightningModule类。一个继承了pl.LightningModule的类被称为Lightning Module，需要实现以下三个核心的组件： 1 模型：self.model，训练用的模型网络，可嵌套torch.nn.Module类型 2 优化器：实现configure_optimizers()函数，其返回值可以是一个优化器或数个优化器，又或是两个List（优化器，Scheduler） 3 训练步骤：实现以下函数： forward()函数，定义前向转播过程，返回如何计算损失，这是用来做梯度下降的 training_step()函数，这个函数其实是包装了一个pytorch的训练过程（就是一个for循环，如果你学过基础的pytorch神经网络实现的话） 其接收一个batch的来自训练数据集的数据，然后，通常这个函数会调用类自身（self(batch)），来通过已定义的模型对整个批量进行一步推理， 然后根据forward()中的定义计算loss损失。 需要返回一个字典，包含loss和log两个字段。loss返回损失，log则是可选的，记录一些需要记录的信息，loss字段一定要存在， 否则pytorch lightning会认为此batch是无效的，从而被跳过训练部分。 validation_step()函数：[可选]。功能和training_step()函数一样，不过是在验证过程中调用的，不会去更新模型的权重，只输出log信息， 主要是给你一个工具，用来自定义输出验证过程中要观察的变量的。 training_epoch_end()函数：[可选]。这个函数是在每个epoch结束时调用的，用于记录一些信息，比如训练集的平均损失等。 回到DDPM类本身，在定义模型时，使用的语句是： # mug/diffusion/diffusion.pyself.model = MugDiffusionWrapper(unet_config, first_stage_config, wave_stage_config, cond_stage_config) 这里，额外定义了一个MugDiffusionWrapper类，用来包装MuG Diffusion模型中包含的四个子模型，分别是： cond_stage_model：前文所述的BeatmapFeatureEmbedder模型，用来得到【谱面提示】的压缩表示（由可读数据压缩到向量） 谱面数据包含 osu的谱面元数据（难度等）和 通过minacalc计算得到的谱面提示（各个键形的分布、键形难度等），不包含note数据 wave_model：前文所述的STFTEncoder或MelspectrogramScaleEncoder1D模型，用来编码和解码【音频波形】（由音频数据到条件向量？） first_stage_model：前文所述的AutoEncoderKL模型，用来编码和解码 【谱面note数据】（从 向量 编码到 潜空间向量 和 从 潜空间向量 解码） unet_model：与Stable Diffusion类似的U-Net结构，用来学习反向扩散去噪（Denosing）的步骤 即，unet_model接受潜空间的两种输入：1、【潜空间的谱面向量数据】2、【条件嵌入向量，包括音频和谱面特征（即“提示 prompt”）】， 通过多步扩散迭代，学习正向扩散参数，或是反向扩散输新的谱面向量表示。 在训练时，unet_model的输入是一个【训练数据集的谱面潜空间向量】和【该谱面对应的音频向量、提示向量】，输出是一个【潜空间向量】， 每一次迭代，都会使得【潜空间向量】接近于从高斯分布采样的【噪声数据】，并同时更新网络权重 在推理生成新谱面时，unet_model的输入是一个【从高斯分布种采样的噪声谱面向量】和【用户想要的谱面音频和特征的输入向量】，通过多次反向扩散， 得到生成的【谱面潜空间向量】，通过first_stage_model解码得到可读的【谱面note数据】 每个模型的详细结构在下一个小节描述 对比：Stable Diffusion（v1）的模型模块和 MuG Diffusion的模型模块 输入编码器（条件嵌入） 潜空间扩散网络 输出解码器 Stable Diffusion 基于 CLIP 模型的文本编码器 U-Net AutoEncoder（图像） MuG Diffusion 基于STFT/Mel的音频编码器 + 谱面提示嵌入 U-Net（魔改） AutoEncoder（谱面Note） DDPM使用的优化器是AdamW，如果yaml配置文件中定义了学习率调度器，还会使用Pytorch的LambdaLR创建一个学习了调度器。 # mug/diffusion/diffusion.pyopt = torch.optim.AdamW(params, lr=lr)...if self.use_scheduler: assert target in self.scheduler_config scheduler = instantiate_from_config(self.scheduler_config)... 在MuG Diffusion的两种配置文件里，mug_diffusion.yaml中定义了学习率调度器为Stable Diffusion使用的 ldm.lr_scheduler.LambdaLinearScheduler，而mug_diffusion_stft.yaml中没有定义学习率调度器 unet.py - U-Net模型 mug/diffusion/unet.py中定义了U-Net模型，这个模型是DDPM的核心，用于进行潜空间的扩散和反向扩散。 wave_model、cond_stage_model和first_stage_model（即VAE）的输出都将作为U-Net的输入，下面从模型输入和输出的shape简要描述一下模型。 # mug/diffusion/diffusion.py ... def forward(self, x, t, c, w): x 对应 UNetModel 的 x 参数，表示输入的特征张量。 t 对应 UNetModel 的 timesteps 参数，表示时间步长。 c 对应 UNetModel 的 context 参数，表示上下文信息[谱面提示]，用于条件生成 *w 对应 UNetModel 的 *audios 参数，表示音频数据，用于音频处理 return self.unet_model(x, t, c, *w) ... 整个网络的结构遵循UNet的典型编码器-解码器结构，通过逐渐增加和减少特征图的尺寸来学习丰富的特征表示，最终输出与输入相同尺寸的张量。 U-Net模仿了Stable Diffusion中的U-Net结构，包括注意力层等，但是做了一些改动，主要是为了适应【音频数据具有天然的时序性】。 Stable Diffusion U-Net的结构参见（图源水印）： 因此，U-Net模型中可以选择加入下面的部分来增强模型处理序列数据的能力。 LSTM层，使用LSTM模型处理时序信息，这也是参考已有生成谱面的工作，大多采用了LSTM块来设计网络。 S4层：使用Structured State Space（S4）模型，是一种基于状态空间模型（SSM）的新型序列建模方法。它本质上是连续时间的，并且在建模数据中的长期依赖性方面表现出色。 模型结构流程描述： [ i ] 首先进行一个卷积，将输入张量数据（谱面note数据）的通道数转换为model_channels （model_channels是配置文件中定义的参数(128)，代表模型的通道数）。 [ ii ] Downstamps（input_blocks） 下采样:U“形状的左半边，每次通过两个网络模块： 1、AudioConcatBlock，将当前分辨率等级的音频通道数直接加到到模型的通道数中，并连接音频数据和输入数据。 音频通道数对于不同的采样分辨率分别是[256,512,512,512] 2、一个ResBlock（残差块），其中包含三层： 第一 TimestepResBlock，和Stable Diffusion的方法一样，将时间步数据嵌入到残差块中（没细看，不知道这里有没有魔改…… 第二 ContextualTransformer（自定义注意力层）， 第三：按照配置中的指定，在注意力层中后面额外添加一个LSTM层或S4层。 每个TimestepResBlock都会从外部连接一个timestep_emb，作为时间信息的编码。这里采用了和Stable Diffusion一样的策略， timestep_emb的长度为model_channels的4倍。 每个ContextualTransformer层都会从外部连接一个相同的context张量，这个就是之前传入的c，从BeatmapFeatureEmbedder获得的谱面特征提示数据 在原版Stable Diffusion里，注意力块名称是SpatialTransformer，因为生成图像的文本提示contex是不定长的，而这里生成谱面的提示是定长的， 且不需要进行2D卷积。因此这里重新写了一个ContextualTransformer，二者都是从BasicTransformerBlock继承而来。 每次通过完整的input_block以后，进行一次下采样，倍增模型的通道数（按照配置，倍率为[ 1,2,4,4 ]），压缩数据的维度和注意力层的分辨率 [ iii ] middle_block: U形状的底部，也是模型的中间层，与Stable Diffusion的结构仍然相同，通过了一个三明治形状的TimestepResBlock-ContextualTransformer-TimestepResBlock网络块。 [ iv ] Upstamps（output_blocks） 上采样：U形状的右半边，老样子，与下采样相反即可，每次通过相同的TimestepResBlock-ContextualTransformer-LSTM/S4层后进行一次上采样。 [ v ] 输出层：上采样结果先通过一个归一化层和一个SiLU激活函数，然后最后通过一个卷积层将特征映射到目标输出通道数，over。 好奇：这里和原版SD一样，用了个zero_module来初始化卷积层，不知是什么优化技巧…… 各个模型的input size和Output size： 下表默认省略了batch size，即应该添加在各个input size前面的批量维度B 模型 Input size Output size 备注 (VAE)Encoder (16, 4096) (64, 128) 16是谱面note数据的输入通道数，它是由“4K”按键的四个位置，每个位置分配4个通道构成的。经过多次下采样后得到形状256*128，然后通过卷积压缩到潜空间的形状为64*128 (VAE)Decoder (64, 128) (16, 4096) 与上面相反，多次上采样后得到形状64*4096, 然后通过卷积恢复到谱面形状16*4096 BeatmapFeatureEmbedder (f) (f, 128) f是feature的数量？（数据集中一个谱面的feature长度为21） MelspectrogramScaleEncoder1D (128, 32768) (512, 64) 32768代表最大的序列长度（梅尔频谱图的长度），和下面的傅里叶变换相比，是将两个音频通道混合到一起，数量*2 STFTEncoder (2, 1025, 16384) (32, 256) 2=输入通数道（对应复数的实部和虚部），1025=频率分辨率，16384=最大的序列长度，输出形状中的32是输出通道数 U-Net(使用STFT) (544, 4096) (32, 4096) 在这个方法中，音频通道和输入通道（32）一开始就通过直接连接合并到一起，544 = 512+32 (我也不知道为什么使用STFT的通道数多一倍，对不起我没学过信号处理QAQ) U-Net(使用MelScaleEncoder1D) (16, 4096) (16, 4096) x是从噪声分布中采样的，与谱面note数据形状相同的随机输入。 而提示数据、时间步数据和音频数据都是从外部嵌入的。 ddim.py - DDIM模型 DDIM是DDPM的一个改进模型，用于推理时快速采样x，貌似训练时没有用到。 训练入口[main.py] 加载模型：model = instantiate_from_config(config.model),config里配置的模型即为DDPM模型 加载数据： data = instantiate_from_config(config.data)data.prepare_data()data.setup()print(#### Data #####)for k in data.datasets: print(fk, data.datasets[k].__class__.__name__, len(data.datasets[k])) 开始训练，使用trainer.fit()函数，传入模型和数据集 if opt.train: try: trainer.fit(model, data) except Exception: melk() raiseif not opt.no_test and not trainer.interrupted: trainer.test(model, data) （4）WebUI和推理部分 交互式推理入口[webui.py] TODO"},{"title":"生成模型基础与应用笔记-第4章","path":"/2023/12/19/生成模型基础与应用笔记-第4章/","content":"C4 连续数据的生成模型 本文的部分公式格式未校对，正在施工中…… C4 连续数据的生成模型 生成分类器 在C2中，我们提到，生成模型也是一个分类模型，通过对离散变量的贝叶斯建模，可以得到 朴素贝叶斯分类器（NBC），事实上，这一点也可以推广到连续数据。只需将概率建模改为“类条件密度函数”： 记好这个结论，在后面两节我们会使用其来训练一个** 多元高斯分布** 分类器 单一正态分布的高斯模型 这一节我们专注于高斯分布（正态分布），因为它是连续变量中的最常见的建模。 我们先一句带过 一元正态分布，对没错就是这个： 它的概率密度函数是N(x∣μ,σ2)=12μσ2e−(x−μ)22σ2N(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\mu\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}N(x∣μ,σ2)=2μσ2​1​e−2σ2(x−μ)2​，大家应该都知道吧。 实际上，多元连续变量也是具有正态分布的（概率论里其实也学过，不过大概大家都忘了）： 其中，我们把 (x−μ)TΣ−1(x−μ)(x-\\mu)^T\\bold \\Sigma^{-1}(x-\\mu)(x−μ)TΣ−1(x−μ) 称为** 马氏距离 DMD_MDM​，这是欧式距离在多元空间中的推广：** DM=(x−μ)TΣ−1(x−μ)D_M = (\\bold x-\\mu)^T\\Sigma^{-1}(\\bold x-\\mu)DM​=(x−μ)TΣ−1(x−μ) 下面我们提这样一个问题：给定N个独立同分布的样本xi∼N(μ,Σ)\\bold x_i \\sim N(\\bold \\mu, \\bold \\Sigma)xi​∼N(μ,Σ)，如何估计该多元高斯分布的参数呢？ 同样，可以使用MLE和MAP两种估计方法。 极大似然估计 似然函数： L(μ,Σ)=logP(x∣μ,Σ)=∑i=1NlogP(xi∣μ,Σ)=∑i=1Nlog1(2π)D/26∣Σ∣1/2e−12∑i=1NDM(xi)L(\\mu,\\Sigma) = log P(\\bold x|\\mu,\\Sigma) \\\\ = \\sum_{i=1}^NlogP(\\bold x_i|\\mu,\\Sigma) \\\\ = \\sum_{i=1}^Nlog\\frac{1}{(2\\pi)^{D/2}6|\\Sigma|^{1/2}}e^{-\\frac{1}{2}\\sum_{i=1}^N D_M(x_i)}L(μ,Σ)=logP(x∣μ,Σ)=∑i=1N​logP(xi​∣μ,Σ)=∑i=1N​log(2π)D/26∣Σ∣1/21​e−21​∑i=1N​DM​(xi​) 最大化时的参数结论： 最大后验概率估计 似然函数 L(μ,Σ)=logP(x∣μ,Σ)=(2π)−ND2∣Σ∣−N2e−12∑i=1NDM(xi)L(\\mu,\\Sigma) = log P(\\bold x|\\mu,\\Sigma) = (2\\pi)^{-\\frac{ND}{2}}|\\Sigma|^{-\\frac{N}{2}}e^{-\\frac{1}{2}\\sum_{i=1}^N D_M(x_i)}L(μ,Σ)=logP(x∣μ,Σ)=(2π)−2ND​∣Σ∣−2N​e−21​∑i=1N​DM​(xi​) 其中，对于∑i=1NDM(xi)=∑i=1N(xi−μ)TΣ−1(xi−μ)\\sum_{i=1}^N D_M(x_i) = \\sum_{i=1}^N (x_i-\\mu)^T\\Sigma^{-1}(x_i-\\mu)∑i=1N​DM​(xi​)=∑i=1N​(xi​−μ)TΣ−1(xi​−μ)，可以用采样数据x的均值xˉ\\bar xxˉ和散度SxˉS_{\\bar x}Sxˉ​表示为： ∑i=1NDM(xi)=tr(Σ−1Sxˉ)+N(xˉ−μ)TΣ−1(xˉ−μ)\\sum_{i=1}^N D_M(x_i) = tr(\\Sigma^{-1}S_{\\bar x}) + N(\\bar x - \\mu)^T \\Sigma^{-1} (\\bar x - \\mu)∑i=1N​DM​(xi​)=tr(Σ−1Sxˉ​)+N(xˉ−μ)TΣ−1(xˉ−μ) ,其中Sxˉ=∑i=1N(xi−xˉ)(xi−xˉ)TS_{\\bar x} = \\sum_{i=1}^N(x_i - \\bar x)(x_i - \\bar x)^TSxˉ​=∑i=1N​(xi​−xˉ)(xi​−xˉ)T 最大化时的参数结论： 因为是MAP，需要引入先验的参数： m0=xˉ\\bold m_0 = \\bar xm0​=xˉ，即先验均值 k0k_0k0​：对m0\\bold m_0m0​的信任程度 ，为0或一个极小值 S0=diag(Sxˉ)N\\bold S_0 = \\frac{diag(S_{\\bar x})}{N}S0​=Ndiag(Sxˉ​)​，协方差的先验，并取平均 v0v_0v0​：对S0\\bold S_0S0​的信任程度 ，v0=D+2v_0 = D+2v0​=D+2，D是维度 然后对先验均值和极大似然估计的结果做** 凸组合 ，定义：** \\bold m_N = \\frac{k_0}{k_0+N}\\bold m_0 + \\frac{N}{k_0+N}\\bar x \\\\ \\ \\\\ \\bold S_N = \\bold S_0 + S_{\\bar x} + \\frac{k_0 N}{k_0+N}(\\bar x - \\bold m_0)(\\bar x - \\bold m_0)^T \\\\ = \\bold S_0 + S_{\\bar x} + k_0\\bold m_0\\bold m_0^T-K_N \\bold m_N\\bold m_N^T \\\\ \\ \\\\ 其中： k_N = k_0 + N \\\\ v_N = v_0 + N 则估计结果为： - ![image.png](https://cdn.nlark.com/yuque/0/2023/png/23169257/1699340522524-968b4f6e-de54-4446-a436-bef4db536cf0.png#averageHue=%23fbf8f7clientId=u0e63a603-818a-4from=pasteheight=246id=u220176e4originHeight=472originWidth=919originalType=binaryratio=1.5rotation=0showTitle=falsesize=165465status=donestyle=nonetaskId=u9c62d9f6-5344-4492-a011-0e9d87c8828title=width=479.66668701171875) 高斯判别分析 上一节我们已经学习了如何以 多元高斯分布 为随机变量建模贝叶斯生成模型，结合第一节所说，下面我们尝试利用其生成一个分类器。 假设我们有多个训练样本数据x1,x2,...x_1,x_2,...x1​,x2​,...，它们对对应的类别分别是 y=1,2,...Cy=1,2,...Cy=1,2,...C 那么可以定义当条件为“随机变量被分为类别c”的条件密度函数： p(x∣y=c,θ)=N(x∣μc,Σc)p(\\bold x|y=c, \\theta) = N(\\bold x |\\mu_c, \\Sigma_c)p(x∣y=c,θ)=N(x∣μc​,Σc​) 其中μc,Σc\\mu_c, \\Sigma_cμc​,Σc​是 所有被分为类别c的训练样本数据构成的多元高斯分布 的 均值 和 协方差矩阵 按照生成分类器的 后验估计最大 为原则，和离散生成分类器的输出类似，可以得到分类的计算公式： 其中，π\\piπ是分类先验的分布 如果对于类别c，先验分布πc\\pi_cπc​是均匀的 ，则推导公式为： 如果 x 是二元变量，那么y(x)的分类结果（决策边界）在平面图上会呈现一条二次曲线，因此，也将这种方法称为“二次判别分析”）下图是一个这样的例子： 在这个图示中，样本变量x是一个二元的变量，因此被绘制在平面上，每个点还对应一个y值，图中的圆圈表示的是y值的等高线。 x相对y的值服从正态分布，直观地绘制出三维和二维的对比图，其实是这样的： 如果对于任意的类别c，有Σc=Σ\\Sigma_c = \\SigmaΣc​=Σ，则决策边界恰好是一条直线，此时“二次判别分析”退化为“ 线性判别分析 ”,可以直接写出决策线的表达式： logP(y=c∣x,θ)∝βcx+γclogP(y=c|\\bold x,\\theta) \\propto \\beta_c \\bold x + \\gamma_clogP(y=c∣x,θ)∝βc​x+γc​ 其中参数矩阵βc=(Σ−1μc)T\\beta_c = (\\Sigma^{-1}\\mu_c)^Tβc​=(Σ−1μc​)T,γc=−12μcTΣ−1μc+logπc\\gamma_c = -\\frac{1}{2}\\mu_c^T\\Sigma^{-1}\\mu_c+log\\pi_cγc​=−21​μcT​Σ−1μc​+logπc​，下图是一个这样的二元变量线性分类的例子： 我们可以看到，多元高斯分布分类器（判别器）的模型参数就是高斯分布的参数： θc=(μc,Σc)\\theta_c = (\\mu_c, \\Sigma_c)θc​=(μc​,Σc​)，基于上一节的结论，直接使用极大似然估计来计算参数。 先计算对数似然函数，其即是 对每个分类的多元高斯分布 做 加权平均（没错这和离散情况的下的贝叶斯分类器其实是一样的）： logP(D∣θ)=∑i=1N∑c=1C(logπc)if(Yi=c)+∑c=1C[∑i:yi=clog N(x∣μc,Σc)]log P(D|\\theta) = \\sum_{i=1}^N \\sum_{c=1}^C (log \\pi_c)^{if(Y_i =c)} + \\sum_{c=1}^C[\\sum_{i: y_i=c}log \\ N(\\bold x| \\mu_c, \\Sigma_c)]logP(D∣θ)=∑i=1N​∑c=1C​(logπc​)if(Yi​=c)+∑c=1C​[∑i:yi​=c​log N(x∣μc​,Σc​)] 右侧的N即为类别c对应的多元高斯分布的概率密度函数。 而πc\\pi_cπc​表示第c个类的先验，其满足以下条件： 0≤πc≤1, ∑c=1Cπc=10 \\leq \\pi_c \\leq 1, \\ \\sum_{c=1}^C \\pi_c = 10≤πc​≤1, ∑c=1C​πc​=1 在实际使用时，往往以每个类c在数据集中的统计频率结果作为先验，带入极大似然估计可以得到，第c个类的多元高斯判别器参数为： μ^c=1Nc∑i:yi=cxi\\hat \\mu_c = \\frac{1}{N_c} \\sum_{i: y_i=c} \\bold x_iμ^​c​=Nc​1​∑i:yi​=c​xi​ Σ^c=1Nc∑i:yi=c(xi−μ^c)(xi−μ^c)T\\hat \\Sigma_c = \\frac{1}{N_c} \\sum_{i: y_i=c} (\\bold x_i - \\hat \\mu_c)(\\bold x_i - \\hat \\mu_c)^TΣ^c​=Nc​1​∑i:yi​=c​(xi​−μ^​c​)(xi​−μ^​c​)T 其中，NcN_cNc​是类别c的样本在数据集中出现的次数。 多个正态分布的高斯混合模型 上面的单一正态分布模型，看起来很美好，但是存在一个问题：我们假定了任何一个类别c的数据都只服从一个单一的正态分布 事实上，我们无法假设任何要拟合的连续数据都真的只服从一个正态分布。 实际很多情况下， 真实的数据是 多个不同正态分布的叠加。 因此，我哦们需要——高斯混合模型： 理论上来说，高斯混合模型可以模拟任意的分布函数。 为了能对数据进行 “混合高斯建模”，首先我们需要知道要建模的数据空间到底需要几个正态分布的类。 没错，所以建立高斯混合模型的第一步，是聚类（clustering）。 这就和上一节的生成分类器很不一样，前者我们已知（假设了）每个类别对应一个正态分布。而在这里是不同的，我们不能相信标签里的类别信息，只能将最终每个类别需要混合的正态分布的数量视为隐变量。 EM算法 估计高斯混合模型 我们依然可以写出MLE方法和MAP方法对混合高斯模型的对数似然函数： 最大的问题在于：我们不知道隐变量z（即真实正态分布的分类数量）的维度，因此参数的解也是不唯一的。 如果我们假设有k个维度（即每个分类用k个正态分布混合表示），那么就会有k!种不同的解。 把k也加入模型的参数中，我们可以用C4中提到变分EM算法来进行最大似然估计。 公式的推导很长，如何以后有机会的话我把它整理一下放到另一个链接里…… 这里我们简单回顾一下过程吧： 1、定义Q函数，步数t=0，定义一个初始先验。然后每一步迭代推导隐变量zi=kz_i= kzi​=k的条件概率 2、E步：计算t-1步中，将Q函数表示为多元正态分布的参数 3、M步：最大化Q函数值，结果作为下一次迭代的参数。 4、重复2-3步 下图是一个实际迭代过程的可视化举例： K-means算法 估计高斯混合模型 你们可能在数据分析或者数据挖掘课上学到过K-means算法，你们有没有好奇过，这个算法的原理到底是什么？ 天哪如果你只看上一节就敏锐地发现了其实K-means算法就是EM算法的近似变形的话那你可太NB（朴素贝叶斯）了，我上课的时候是一点没听懂，感觉这还是八竿子打不着的两个问题…… K-means算法其实是上述 混合高斯模型估计的 一个近似特例。 直观地说这么解释：它们在聚类的时候其实都是一开始未知类别的数量k，然后通过计算不断收敛的。 它做了两个假设： 每个要混合的正态分布的 协方差矩阵 已知 （假设它们都等于标准的方差） 每个要混合的类别的 先验 已知（假设它们都相等，即为1/k） 因此，在K-means算法中，我们只需要计算样本相对于每个先验点的均值，并以此来迭代估计出每个类别的分布： 好处显而易见：计算非常方便快捷，能在最短的时间得到收敛的聚类。 但是，因为做出了协方差和先验的假设，最后得到的高斯分布就再也不能用于进行概率密度估计了。因此K-means相对于直接建模混合高斯分布有以下缺点： 无法计算某一个样本点属于某个高斯分布类的概率值 因此，也就无法生成新的样本点 综上，K-means算法在数据分析中非常常用，但是它不能作为一个生成模型来使用。遗憾退场。 隐马尔可夫模型 隐马尔可夫模型是关于 时序 的概率模型 。 为此，首先你需要知道 马尔可夫过程 和 马尔可夫链 的基本定义。我假定你已经学过了《随机过程》…… 什么，你没有学过？！这下可不好办了……咳咳…… 问题不大，你只需要先记好这两点： 马尔可夫过程 是基于时序的，它假定随机变量的每个取值都随 **时间 **而变。 齐次马尔可夫过程 假定 每一个时刻 随机变量的概率取值 只与 上一个时刻的随机变量有关。 你不足需要知道 齐次 是什么意思，总之在本篇的应用范围内，我们可以认为马尔可夫过程都是齐次的。 这样的一个过程中，状态组成的序列被称为马尔可夫链，因为每一个时刻都是可以和上一个时刻通过概率相连的。 而在隐马尔可夫模型中， 状态序列(state sequence)是隐藏的，从外部数据无法观测。这也就是为什么被称为隐马尔可夫链。 那么外部观测的是什么呢？我们只知道它和当前时刻的那个隐藏状态有关。我们把每个时刻中观测到的数据也组成一个序列，称为观测随机序列 (observation sequence ) 这样一定很不好理解，问题不大，我们来举一个例子： 骰子游戏 假设有三个不同的骰子：D6、D4、D8 （六面、四面和八面骰子）。 （——为什么没有D100？！——兄弟你走错片场了，这里玩的是DND） 每次投掷前，先等概率地随机从三个骰子中选取一个。 然后投掷选到的骰子，骰子随机地产生一个数字。 如果我们每次记录下每次得到的数字，那么它们就组成一个观测序列。 隐藏序列是什么？是我们每一次投的是哪一个骰子。 加入我只告诉你前面几次投掷的结果是：1 6 3 5 2 7 3 5 2 4 ，要估计下一次投掷的结果，那么隐藏状态必然是猜测下一次骰的是哪一个骰子。 接下来我们就以这个“生成骰子游戏点数”的模型为例子，来介绍隐马尔可夫模型。 隐马尔可夫模型 有三个要素： 初始概率分布 π\\piπ：即第一次（0时刻）观测变量的概率分布 如骰子游戏中，第一次试验可能得到1-8中的一个数字，可以通过组合方法计算出初始分布。 比如，第一次试验产生数字1的概率=13(16+14+18)=1372=\\frac{1}{3}(\\frac{1}{6}+\\frac{1}{4}+\\frac{1}{8})=\\frac{13}{72}=31​(61​+41​+81​)=7213​ 状态转移矩阵AAA：每个隐藏状态在下一时刻可能转换为另一个隐藏状态的概率：A(i,j)=p(zt=j ∣ zt−1=i)A(i,j) = p(z_t = j\\ | \\ z_{t-1}=i)A(i,j)=p(zt​=j ∣ zt−1​=i) 在骰子游戏中，三个骰子被视为三个隐藏状态。 其中，每个状态在下一时刻都有等概率（1/3）转变为任意的三个状态（包括它自己） 观测概率矩阵 BBB：从每个已知隐藏状态到观测状态的概率 Bt(j)=p(xt ∣ zt=j)B_t(j) = p(\\bold x_t \\ | \\ z_t=j)Bt​(j)=p(xt​ ∣ zt​=j) 在骰子游戏中，_（如果t=0）_对于D4，每个观测状态（1-4）的概率是25%，其他三个骰子类似。 将马尔可夫过程的假设写成数学公式，就是： 1、齐次马尔可夫性假设 隐马尔可夫链 ttt**时刻的状态 只和 **t−1t-1t−1时刻的状态 有关， 与其他时刻的状态及观测无关，也与时刻ttt无关 ： p(z1:T)=p(z1)p(z2∣z1)p(z3∣z1,z2)....p(zT∣z1,z2,...,zT−1)=p(z1)p(z2∣z1)p(z3∣z2)....p(zT∣zT−1)p(z_{1:T}) = p(z_1)p(z_2|z_1)p(z_3|z_1,z_2) .... p(z_T|z_1,z_2,...,z_{T-1}) \\\\ =p(z_1)p(z_2|z_1)p(z_3|z_2)....p(z_T|z_{T-1})p(z1:T​)=p(z1​)p(z2​∣z1​)p(z3​∣z1​,z2​)....p(zT​∣z1​,z2​,...,zT−1​)=p(z1​)p(z2​∣z1​)p(z3​∣z2​)....p(zT​∣zT−1​) 2、 观测独立性假设 观测变量只和当前时刻的状态有关，与其他时刻的 观测和状态均无关 p(x1:T∣z1:T)=p(x1∣z1)p(x2∣z2)p(x3∣z3)....p(xT∣zT)p(x_{1:T}|z_{1:T}) = p(x_1|z_1)p(x_2|z_2)p(x_3|z_3)....p(x_T|z_T)p(x1:T​∣z1:T​)=p(x1​∣z1​)p(x2​∣z2​)p(x3​∣z3​)....p(xT​∣zT​) 在骰子游戏中，观测变量是离散的，因此模型参数可以使用多项分布或贝塔分布，如果观测变量是连续的（如语音或文本），可以使用上面几节介绍的多元高斯分布来建模。 但是 **状态空间 **在隐马尔可夫模型中 一定是离散的 根据任务的不同，隐马尔可夫模型要求解的问题可以被分为以下几类： 学习：已知观测序列x1:Tx_{1:T}x1:T​，学习模型参数θ\\thetaθ,使得生成该观测序列的概率p最大 解码（预测隐空间）：给定模型参数θ\\thetaθ和观测序列x1:Tx_{1:T}x1:T​，求最有可能的隐藏状态序列z1:Tz_{1:T}z1:T​ 计算（预测观测结果）：给定模型参数θ\\thetaθ和观测序列x1:Tx_{1:T}x1:T​，求该观测序列x1:Tx_{1:T}x1:T​生成的概率。按照是否已知整个序列的数据，还可以分为： 前向算法（在线学习）：已知中间观测序列1-t，求生成到该序列的概率：p(zt∣x1:t)p(z_{t}|x_{1:t})p(zt​∣x1:t​) 后向算法（离线学习）：已知整个观测序列1-T，求p(zt∣x1:T)p(z_{t}|x_{1:T})p(zt​∣x1:T​) 可观测隐变量学习 在骰子游戏这个例子中，虽然从给定的观测序列x1:Tx_{1:T}x1:T​中我们无法知道隐变量的分布**，但是我们知道隐变量定义为三种骰子的随机选取，它服从某种特定的规则。** 类似这种情况，我们仍为隐变量z仍然是可观测的，因此，可以直接利用极大似然估计法来估计隐马尔可夫模型参数 。 对于N个状态序列的离散变量，我们可以使用多项分布建模： 假设观测变量x服从多项分布，其中k表示隐藏状态，l表示观测到的序列： B(k,l)=p(xt=l∣zt=k,θ), K=1,2,..,k, l=1,2,...,LB(k,l) = p(x_t =l|z_t=k,\\theta),\\ K=1,2,..,k,\\ l=1,2,...,LB(k,l)=p(xt​=l∣zt​=k,θ), K=1,2,..,k, l=1,2,...,L 我们在C2章已经举过例子了，不如推导留给你们，这里是参数π\\piπ和 A 的估计结果： 而参数矩阵B则使用统计方法计算： 如果 观测变量x服从正态分布，那就如同前面几节的方法，如法炮制， 在计算观测概率B时，将分子分母替换为 均值 和 协方差 矩阵即可。 不可观测隐变量学习 那么，更多的情况下，我们确实无法观测隐变量z，或者无法得知其的生成规律，此时我们需要一种 非监督的学习方法 Baum-Welch算法 ：一种改进的EM算法，用来解决隐马尔可夫模型的参数估计。 老三样： Q函数： 这里因为符号t已经被用来标记马尔可夫过程的时刻，我们用old上表标记参数的上一次迭代。 E步：在这里我们需要计算数据集中已知的序列在当前参数下的生成概率，具体方法参见后面的“计算问题”小节。 M步： 计算问题 前向算法 ：从第0步开始，算出下一步的概率密度 我们可以发现，t时刻的隐藏状态依赖于t-1时刻的隐藏状态，利用马尔可夫链的特性，我们可以定义下面的两个转移函数，其中i，j都是某一个隐状态： ϕt(j)=p(xt∣zt=j,θ)\\phi_t(j) = p(x_t|z_t=j,\\theta)ϕt​(j)=p(xt​∣zt​=j,θ) ψ(i,j)=p(zt=j∣zt−1=i,θ)\\psi(i,j) = p(z_t=j|z_{t-1}=i,\\theta)ψ(i,j)=p(zt​=j∣zt−1​=i,θ) 定义t时刻的 信念状态 （belief state）： 于是我们得到：αt∝ϕt(j)(ΨTαt−1)\\alpha_t \\propto \\phi_t(j)(\\Psi^T\\alpha_{t-1})αt​∝ϕt​(j)(ΨTαt−1​)其中Ψ\\PsiΨ是所有转移函数ψ(i,j)\\psi(i,j)ψ(i,j)组成的矩阵，并由此可以从t=1开始计算信念状态，从而得到最中的概率密度链 后向算法： 与前向计算相反，使用 条件似然函数 从最终时间T（前提：我们知道整个序列）反向推导每个时间点的序列概率。 预测问题 假设我们已经知道了模型参数和观测得到的序列数据，现在，我们想要推断最有可能的隐状态序列： z∗=argmaxz1:Tp(z1:T∣x1:T)z* = argmax_{z_{1:T}}p(z_{1:T}|x_{1:T})z∗=argmaxz1:T​​p(z1:T​∣x1:T​) 我们可以从第一个时刻起，每个时刻取的隐状态z标记一个代价：即 前面所有步骤的计算概率是否最大： 这样，我们就将求隐状态序列转换为一个 最大（优）路径问题。 然后，我们可以使用计算机算法来求解，例如贪心算法。 下面介绍一个称为 维特比算法 的方法，它的思想是用 动态规划 解最大路径问题： 从终结点开始，由 后向前逐步求得结点 zT∗,zT−1∗,...,z1∗z^*_{T}, z^*_{T-1},...,z^*_{1}zT∗​,zT−1∗​,...,z1∗​，得到最优路径","tags":["生成模型","笔记"]},{"title":"【笔记】生成模型基础与应用 - 第3章","path":"/2023/11/27/生成模型基础与应用笔记-第3章/","content":"C3 主题模型：主题模型是一类用于文本分析的非监督学习方法，旨在从文本数据中发现隐藏的主题结构… C3 主题模型 Topic models：以非监督学习的方式对文本的隐含结构进行发现或生成的模型 主题模型是一类用于文本分析的非监督学习方法，旨在从文本数据中发现隐藏的主题结构。这些主题模型的目标是识别文本集合中的主题，而无需事先标记的主题标签或监督信息。主题模型最常见的应用之一是用于文本数据的主题建模，其中文档被看作是多个主题的混合，而每个主题又由一组词汇表示。 主题模型的发展： LSA PLSA LDA 2003 HDP 2005 单词向量空间 将文档中每个单词的出现的频数（或加权的频数）表示为向量 可以事先定义 有效单词的 语料库，只提取文档中有效单词的出现频率生成向量（也可以视为不在语料库中的单词其权值为0），所有的语料库中的有效单词用集合W表示 一个文档就表示成：d=(fw1,fw2,....,fwN)d=(f_{w_1},f_{w_2},....,f_{w_N})d=(fw1​​,fw2​​,....,fwN​​)，每个fif_ifi​都是有效单词iii出现的次数 一个文档集中的所有文档（设一共有N个文档组成了集合D）组成了一个向量集合，即 单词向量空间 对于两个不同的文档di, djd_i,\\ d_jdi​, dj​，可以使用它们之间的数学度量，来表示文本之间的语义相似度： 计算方法可以为 文档向量的 内积 或 标准化内积 单词向量空间的表示方法： 1、单词-文本矩阵：将单词在每个文档的出现频率向量作为列向量，组成的矩阵。通常为稀疏矩阵 单词-文本矩阵的例子： - 计算机处理稀疏矩阵，非常浪费算力，因此，需要一个等价的数据结构来表示相同给信息，于是提出了下面这种表示方法。 2、单词频率-逆文本频率（TF-IDF）： 统计单词wiw_iwi​在文本dkd_kdk​中出现的权值 ： TF−IDF(wi,dk)=f(wi,dk)len(dk)logNdf(wi)TF-IDF(w_i,d_k) = \\frac{f(w_i,d_k)}{len(d_k)}log\\frac{N}{df(w_i)}TF−IDF(wi​,dk​)=len(dk​)f(wi​,dk​)​logdf(wi​)N​ - tf(w,b)：单词w在文本b中的出现频数 - len(b)：文本b中的单词总数 - df(w)：整个文本集合中，含有单词w的文本数 - N：整个文本集合的大小（含有的文本数量） 单词向量空间的优缺点优点：模型简单、计算效率高缺点：内积计算的相似度不一定能准确表达两个文档间的相似度 话题向量空间 定义话题：假设所有的文本中一共含有K个话题 假设每个话题都是一个M维度的向量ttt，ttt由语料库（集合W）中的M个单词组成。 t=(w1,w2,...,wM)Tt = (w_1,w_2,...,w_M)^T t=(w1​,w2​,...,wM​)T 这样，一共K个话题就组成了一个话题向量空间，记为T=[t1,t2,...,tk]=[w11 w12... wMK]T = [t_1,t_2,...,t_k]=[w_{11}\\ w_{12} ... \\ w_{MK}]T=[t1​,t2​,...,tk​]=[w11​ w12​... wMK​]【单词-话题矩阵】 根据单词向量空间的定义，可以推导出： 假设一篇文本xxx在单词向量空间中被表示为：x=(fw1,fw2,....,fwN)x = (f_{w_1},f_{w_2},....,f_{w_N})x=(fw1​​,fw2​​,....,fwN​​)，而在话题空间中被表示为：$$y = g(x) = (f_{t_1},f_{t_2},…,f_{t_k})$$ 为了方便，我们将话题的出现频率 记为ft1=y1,ft2=y2...f_{t_1} = y_1, f_{t_2} = y_2...ft1​​=y1​,ft2​​=y2​...这样，一个文档x，它可以表示为所有话题和单词的线性组合： x=t1y1+t2y2+...tkykx = t_1y_1 + t_2y_2 + ... t_ky_k x=t1​y1​+t2​y2​+...tk​yk​ 如果我们定义话题-文本矩阵Y，即将每个话题表示的文本yyy组合起来，则由上述的线性组合关系，对整个文档集合D，即可由话题向量空间的 话题矩阵T 来表示。 因此，单词-文本矩阵、单词-话题矩阵和话题-文本矩阵具有如下的关系： XMN=TMKYKNX_{MN} = T_{MK}Y_{KN} XMN​=TMK​YKN​ 潜在语义分析即是 将文本在 单词向量空间的表示 通过 线性变换 转换为** 在话题向量空间中的表示**的方法 期望最大化算法 在讲潜在语义分析之前，必须要了解一下什么是贝叶斯概率模型的潜变量，以及对潜变量模型的估计算法。期望最大化算法（Exception Maximization Algorithm，简称EM算法）是一种启发式的迭代算法，用于对含有隐变量的概率模型 的参数做 极大似然估计 假设我们有这样一个概率模型： 有三枚硬币A,B,CA,B,CA,B,C，抛掷它们正面朝上的概率不同，记为r,p,qr,p,qr,p,q 重复执行以下试验（Ex(X)表示对事件X做一次实验，1表示正面向上，0表示反面向上）： Ex(Y)=Ex(B) if Ex(A)==1 else Ex(C)Ex(Y)= Ex(B) \\ if \\ Ex(A)==1 \\ else \\ Ex(C) Ex(Y)=Ex(B) if Ex(A)==1 else Ex(C) 即抛硬币A，如果是正面抛硬币B，否则抛硬币C，你只能观测到最后一次抛掷硬币的结果。 那么如何估计三个参数r,p,qr,p,qr,p,q呢？ 因为这个随机事件包含两个概率模型的 复合。因此我们无法直接对结果建模，数学家们由此提出，能否加入“隐变量”ZZZ： 用它来表示试验中抛掷A硬币的中间结果（虽然我们并不能直接观测到），假设共抛了n次，第i次观测结果的值记作ziz_izi​ -假设我们只看最终结果Y，将参数表示为向量θ=(r,p,q)\\theta = (r,p,q)θ=(r,p,q)，则实际要观测的事件Y的似然可以表示为： P(Y∣θ)=P(Y,Z∣θ)=∑i=1nP(yi,zi∣θ)=∑i=1nP(zi∣θ)P(yi∣zi,θ)=rp∑(yi=1)(1−p)∑(yi=0)+(1−r)q∑(yi=1)(1−q)∑(yi=0)P(Y|\\theta) = P(Y,Z|\\theta)= \\sum_{i=1}^{n}P(y_i,z_i|\\theta) = \\sum_{i=1}^{n}P(z_i|\\theta)P(y_i|z_i,\\theta) \\\\= rp^{\\sum(y_i=1)}(1-p)^{\\sum(y_i=0)}+(1-r)q^{\\sum(y_i=1)}(1-q)^{\\sum(y_i=0)} P(Y∣θ)=P(Y,Z∣θ)=i=1∑n​P(yi​,zi​∣θ)=i=1∑n​P(zi​∣θ)P(yi​∣zi​,θ)=rp∑(yi​=1)(1−p)∑(yi​=0)+(1−r)q∑(yi​=1)(1−q)∑(yi​=0) 直接使用MLE估计这个似然函数？不行，包含三个未知变量，且需要求log和的极大值，算不出解析解。 既然没有准确的解析解，数学家于是提出了用迭代法逼近最大值的求解办法，这就是EM算法。我们先定义好两种情况的似然函数，既然要求极大似然，我们对似然函数直接取log，定义对数似然函数： 不将隐变量Z视为随机变量的情况【称为：不完全数据】 LY(θ)=log[∏i=1nP(yi∣θ)]=∑i=1nlog[∑zi∈ Ex(Z)P(yi,zi∣θ)]L_Y(\\theta) = log[\\prod_{i=1}^nP(y_i|\\theta)] = \\sum_{i=1}^nlog[\\sum_{z_i \\in \\ Ex(Z)}P(y_i,z_i|\\theta)] LY​(θ)=log[i=1∏n​P(yi​∣θ)]=i=1∑n​log[zi​∈ Ex(Z)∑​P(yi​,zi​∣θ)] 将隐变量Z视为随机变量的情况【称为：完全数据】 LY,Z(θ)=log[∏i=1nP(yi,zi∣θ)]=∑i=1nlog[P(yi,zi∣θ)]L_{Y,Z}(\\theta) = log[\\prod_{i=1}^nP(y_i,z_i|\\theta)] = \\sum_{i=1}^nlog[P(y_i,z_i|\\theta)] LY,Z​(θ)=log[i=1∏n​P(yi​,zi​∣θ)]=i=1∑n​log[P(yi​,zi​∣θ)] 我们假设θ\\thetaθ从一个初始值θ0\\theta_0θ0​开始，每次迭代的时候更新（下标+1），那么我们希望 在已知观测结果Y的情况下，让完全数据的对数似然函数LY,Z(θ)L_{Y,Z}(\\theta)LY,Z​(θ)尽可能取得最大的期望，由此定义Q函数： Q(θ,θt)=E[LY,Z(θ)∣Y,θt]=∑zi∈ Ex(Z)P(Z∣Y,θt)log[P(Y,Z∣θ)]Q(\\theta,\\theta_t) = E[L_{Y,Z}(\\theta)|Y,\\theta_t] = \\sum_{z_i \\in \\ Ex(Z)}{P(Z|Y,\\theta_t)log[P(Y,Z|\\theta)]} Q(θ,θt​)=E[LY,Z​(θ)∣Y,θt​]=zi​∈ Ex(Z)∑​P(Z∣Y,θt​)log[P(Y,Z∣θ)] 其中，θt\\theta_tθt​是第t步更新的参数，Q是关于θ\\thetaθ的函数，让它最大，则需要对θ\\thetaθ求偏导，并取导数为0。此时θt\\theta_tθt​取一个新值，即： \\theta_{t+1} = argmax_\\theta{Q(\\theta,\\theta_t)} $$【MLE】 这是采用极大似然估计的方法，如果你说要考虑先验概率（即MAP方法），那么每次更新是需要加上先验函数的对数值： \\theta_{t+1} = argmax_\\theta{Q(\\theta,\\theta_t)} + logP(\\theta) 【MAP】 现在我们试着对刚才的问题应用EM算法： - 先假设$\\theta^{(0)} = (0.5,0.5,0.5)$（为了下标不打架，我们用上括号标记步数t） - 则$P(Z|Y,\\theta_t)$表示： - 假设第$i$次的最终**观测结果是**$y_i(=1 \\ or\\ 0)$**，这个结果是由 抛硬币B**$(z_i=1)$** 还是 抛硬币C**$(z_i=0)$** 得到的概率，将其记为**$p_i$**，**我们可用直接以伯努利实验的结果计算： $p_i = \\frac{P(y_i,z_i=1)}{P(y_i,z_i=0)+P(y_i,z_i=1)}=\\frac{rp}{rp+(1-r)q}(y_i=1) \\ or \\ \\frac{r(1-p)}{r(1-p)+(1-r)(1-q)}(y_i=0)$**【对于每一步i】** - **然后求其关于Z的期望，即：** $$Q(\\theta) = \\sum_{i=1}^{n}p_i\\log[P(y_i=1,Z|\\theta)] + (1-p_i)\\log[P(y_i=0,Z|\\theta)] 注意：pip_ipi​与每一步要更新的θt\\theta_tθt​有关，因此其表示为的r,p,qr,p,qr,p,q**也需要每一步更新。**而log[P(Y,Z∣θ)]log[P(Y,Z|\\theta)]log[P(Y,Z∣θ)]则直接表示为上述的似然函数，注意它与要逼近的θt\\theta_tθt​无关，可以直接表示为r,p,qr,p,qr,p,q的式子，故： Q(θ,θt)=∑i=1npi(t−1)log⁡[rpyi(1−p)1−yi]+(1−pi(t−1))log⁡[(1−r)qyi(1−q)1−yi]Q(\\theta,\\theta_t)=\\sum_{i=1}^{n}p_i^{(t-1)}\\log[rp^{y_i}(1-p)^{1-y_i}] + (1-p_i^{(t-1)})\\log[(1-r)q^{y_i}(1-q)^{1-y_i}] Q(θ,θt​)=i=1∑n​pi(t−1)​log[rpyi​(1−p)1−yi​]+(1−pi(t−1)​)log[(1−r)qyi​(1−q)1−yi​] 对该函数的r,p,qr,p,qr,p,q分别求偏导，即可得到： r(t+1)=∑i=1npi(t)nr^{(t+1)} = \\frac{\\sum_{i=1}^{n}p_i^{(t)}}{n} r(t+1)=n∑i=1n​pi(t)​​ p(t+1)=∑i=1npi(t)yi∑i=1npi(t)p^{(t+1)} = \\frac{\\sum_{i=1}^{n}p_i^{(t)}y_i}{\\sum_{i=1}^{n}p_i^{(t)}} p(t+1)=∑i=1n​pi(t)​∑i=1n​pi(t)​yi​​ q(t+1)=∑i=1n(1−pi(t))yi∑i=1n(1−pi(t))q^{(t+1)} = \\frac{\\sum_{i=1}^{n}(1-p_i^{(t)})y_i}{\\sum_{i=1}^{n}(1-p_i^{(t)})} q(t+1)=∑i=1n​(1−pi(t)​)∑i=1n​(1−pi(t)​)yi​​ 问题： 为什么EM算法能近似实现对观测数据 的极大似然估计？ 答：可以通过数学证明：可以通过L(θ)L(\\theta)L(θ)函数构造Q函数，当作为中间函数时，可以证明在迭代过程中极大对数似然函数L(θ)L(\\theta)L(θ)的下界单调增加，即：L(θt+1)≥L(θt)L(\\theta_{t+1}) \\geq L(\\theta_t)L(θt+1​)≥L(θt​) 证明就省略啦~ 概率潜在语义分析（PLSA） 是一种利用概率生成模型 对 文本集合进行话题分析的 无监督学习方法 假设： 每个文本由一个话题的分布决定 每个话题由一个单词的分布决定 因此，从给定文本的表示到生成单词，就是一个概率模型，其中话题是隐变量 举个例子：假设一个离散空间中，【话题】和其对应的单词的概率分布如下： 【教育】 = {大学：0.5，老师：0.3，课程：0.2 } 【经济】 = {市场：0.4，企业：0.2，金融：0.4} 【交通】 = {高铁：0.5，汽车：0.2，飞机：0.3} 而对于一个挖了空位的文本，其每个位置对应话题的概率分布如下： D(xx场景下如何看待xx和xx的关系) = {教育：0.5，经济：0.3，交通：0.2} 那么，生成 “大学场景下如何看待大学与企业的关系”的概率 是： [P(t=教育)P(w=大学∣t=教育)]2×P(t=经济)P(w=企业∣t=经济)=0.00375[P( t = 教育 )P(w = 大学|t = 教育) ]^2× \\\\P(t = 经济) P(w = 企业 |t = 经济 ) =0.00375[P(t=教育)P(w=大学∣t=教育)]2×P(t=经济)P(w=企业∣t=经济)=0.00375 假设文本的集合是D={d1,d2,...,dN}D=\\{d_1,d_2,...,d_N\\}D={d1​,d2​,...,dN​}单词的集合是：W={w1,w2,...,wM}W=\\{w_1,w_2,...,w_M\\}W={w1​,w2​,...,wM​}话题的集合是：Z={z1,z2,...,zK}Z=\\{z_1,z_2,...,z_K\\}Z={z1​,z2​,...,zK​}我们可以看到，在求文本的生成分布时，涉及到以下的条件概率：P(z∣d)P(z|d)P(z∣d): 已知文本d，生成话题z的概率，是一个多项分布P(w∣z)P(w|z)P(w∣z): 已知话题z，生成单词w的概率，也是一个多项分布而P(d)P(d)P(d)表示从所有文本集合中，随机选取一个文本的d的概率 现在我们希望知道，给定(单词，文本)对，它在这个空间中的生成的概率，即： P(X)=∏(w,d) ∈ W×DP(w,d)cnt(w,d)P(X) = \\prod_{(w,d)\\ \\in \\ W×D} P(w,d)^{cnt(w,d)} P(X)=(w,d) ∈ W×D∏​P(w,d)cnt(w,d) ，其中所有的(w,d)(w,d)(w,d)对应该有N×L（文本数×每个文本中要填空的单词数）个根据上面例子的思想，对于每个(w,d)(w,d)(w,d)对，其生成概率又可以用隐变量写为： P(w,d)=P(d)P(w∣d)=P(d)∑z ∈ ZP(w,z∣d)=P(d)∑z ∈ ZP(z∣d)P(w∣z)P(w,d) = P(d)P(w|d) = P(d) \\sum_{z\\ \\in \\ Z}P(w,z|d) = P(d) \\sum_{z \\ \\in \\ Z}P(z|d)P(w|z) P(w,d)=P(d)P(w∣d)=P(d)z ∈ Z∑​P(w,z∣d)=P(d)z ∈ Z∑​P(z∣d)P(w∣z) 于是我们可以建立模型，对P(z∣d)P(z|d)P(z∣d)和P(w∣z)P(w|z)P(w∣z)分别进行估计，得到了这样的单层隐变量网络，即PLSA：其中左侧的参数量是 NK，右侧为MK 现实中K远小于M，所以PLSA通过话题 对数据进行了更简洁地表示，减少了学习过程中过拟合的可能性 继续使用极大似然估计的方法，试图求出让P(X)最大时的参数，取对数似然（下面的公式推导中，我们省略了长度是O(NK+MK)O(NK+MK)O(NK+MK)的参数向量θ，不然就太长了太难读了QAQ）： LW,Z,D=log∏(w,d) ∈ W×DP(w,d)cnt(w,d)=log∏i=1M∏j=1NP(wi,dj)cnt(wi,dj)=∑i=1M∑j=1Ncnt(wi,dj)logP(wi,dj)=∑i=1M∑j=1Ncnt(wi,dj)[logP(dj)+log∑k=1KP(wi∣zk)P(zk∣dj)]L_{W,Z,D} = log\\prod_{(w,d)\\ \\in \\ W×D} P(w,d)^{cnt(w,d)} \\\\ = log \\prod_{i=1}^M \\prod_{j=1}^NP(w_i,d_j)^{cnt(w_i,d_j)} \\\\ = \\sum_{i=1}^M\\sum_{j=1}^Ncnt(w_i,d_j)logP(w_i,d_j) \\\\ = \\sum_{i=1}^M\\sum_{j=1}^Ncnt(w_i,d_j)[logP(d_j) + log \\sum_{k=1}^KP(w_i|z_k)P(z_k|d_j)] LW,Z,D​=log(w,d) ∈ W×D∏​P(w,d)cnt(w,d)=logi=1∏M​j=1∏N​P(wi​,dj​)cnt(wi​,dj​)=i=1∑M​j=1∑N​cnt(wi​,dj​)logP(wi​,dj​)=i=1∑M​j=1∑N​cnt(wi​,dj​)[logP(dj​)+logk=1∑K​P(wi​∣zk​)P(zk​∣dj​)] 仍然是非常难计算，因此要使用上一节介绍的EM估计算法，为此我们计算Q函数。上面的似然函数LW,Z,DL_{W,Z,D}LW,Z,D​已经是一个【完全数据】，隐藏变量Z被视为与W,D并列的随机变量。因此Q函数是每个(w,d)(w,d)(w,d)对为条件下，LW,Z,DL_{W,Z,D}LW,Z,D​的期望，即： Q=P(zk∣wi,dj)[∑k=1K∑i=1M∑j=1Ncnt(wi,dj)logP(wi,dj,zk)]Q = P(z_k|w_i,d_j)[\\sum_{k=1}^K\\sum_{i=1}^M\\sum_{j=1}^Ncnt(w_i,d_j)logP(w_i,d_j,z_k)] Q=P(zk​∣wi​,dj​)[k=1∑K​i=1∑M​j=1∑N​cnt(wi​,dj​)logP(wi​,dj​,zk​)] 其中，$$P(w_i,d_j,z_k) = P(d_j)P(w_i|z_k)P(z_k|d_j)$$ 就是按照“链式法则”生成一堆单词文本的联合概率，而P(zk∣wi,dj)P(z_k|w_i,d_j)P(zk​∣wi​,dj​)则是隐藏变量Z不被视为条件的分布概率【不完全数据】，即： P(zk∣wi,dj)=P(wi∣zk)P(zk∣dj)∑k=1KP(wi∣zk)P(zk∣dj)P(z_k|w_i,d_j) = \\frac{P(w_i|z_k)P(z_k|d_j)}{\\sum_{k=1}^KP(w_i|z_k)P(z_k|d_j)} P(zk​∣wi​,dj​)=∑k=1K​P(wi​∣zk​)P(zk​∣dj​)P(wi​∣zk​)P(zk​∣dj​)​ 这样，我们可以发现，要每一步更新的参数其实是P(wi∣zk)P(w_i|z_k)P(wi​∣zk​)和P(zk∣dj)P(z_k|d_j)P(zk​∣dj​) 因为我们已经有一个数据集，P(dj)P(d_j)P(dj​)的值（即先验）可以直接由统计方法估计出来，cnt函数（即哪些对实际在数据集中同时出现了也是可以统计的已知量）。因此，EM近似的步骤为： 对于每个下标对(i,k)和(k,j)，分别求P(wi∣zk)P(w_i|z_k)P(wi​∣zk​)和P(zk∣dj)P(z_k|d_j)P(zk​∣dj​)的偏导数（第一步时为每个概率取初始值（如均匀分布），只要满足概率和为1即可） 然后将导数为0的值求出来，更新(i,k)和(k,j)对应参数向量的元素 取 k=k+1，求下一步的不完全数据，直到收敛 在用统计估计P(dj)P(d_j)P(dj​)后，局部最优解的结果经过推导是：PLSA的方法仍然胜在离散空间，计算复杂度低，可以快速迭代，但存在一个很重要的缺点：因为所学习的参数不仅依赖于单词库W，还依赖于文档数据集D，所有其可以生成其所在数据集的文档的模型，但却不能生成新文档的模型 潜在狄利克雷分布（LDA） 在对PLSA建模时，我们发现一个规律： 文本由话题的一个多项分布表示 ， 话题也由单词的一个多项分布表示 以防你忘了我们在C1中介绍的多项分布，补充一下完整定义： 这不巧了嘛，我们在C2中提到，多项分布的贝叶斯生成模型，可以用 狄利克雷分布 作为先验。因此，可以假设 话题分布 和 单词分布 的先验都是 狄利克雷分布： 对于每个文本dmd_mdm​，假设d中包含的单词组成了向量dm=wm (m=1,2,...,M)d_m = \\bold w_m \\ (m=1,2,...,M)dm​=wm​ (m=1,2,...,M),认为生成该文本话题的概率P(z∣wm)=θm∼Dir(α)P(z|\\bold w_m) = \\theta_m \\sim Dir(\\alpha)P(z∣wm​)=θm​∼Dir(α) θm\\theta_mθm​参数向量有K个值，每个值表示文本dmd_mdm​能生成对应话题z1,z2,...,zkz_1,z_2,...,z_kz1​,z2​,...,zk​的概率 因此，超参数α\\alphaα也是一个K维向量 注意，在这里我们为了对每个单词写出一个话题分布，将文本的定义改成了单词组成的向量。因此代表文本集中的文本数量的常量不再是NNN，而是MMM。用Ni(i=1,2,...,M)N_i(i=1,2,...,M)Ni​(i=1,2,...,M)来表示一个文本中的单词总数。而下面的部分中，我们假设整个单词库中的单词总数是VVV 对于每个话题zk (k=1,2,...,K)z_k\\ (k=1,2,...,K)zk​ (k=1,2,...,K)，认为生成相应单词的概率P(w∣zk)=ϕk∼Dir(β)P(w|z_k) = \\phi_k \\sim Dir(\\beta)P(w∣zk​)=ϕk​∼Dir(β) 而对于每个单词wm[n]∈wm (n=1,2,...,Nm)w_m[n] \\in \\bold w_m \\ (n=1,2,...,N_m)wm​[n]∈wm​ (n=1,2,...,Nm​)，其对应的话题和单词都链式地服从多项分布： zm[n]∼Mult(θm)z_m[n] \\sim Mult(\\theta_m)zm​[n]∼Mult(θm​)：先随机根据概率生成一个话题序列 wm[n]∼Mult(ϕzm[n])w_m[n] \\sim Mult(\\phi_{z_m[n]})wm​[n]∼Mult(ϕzm​[n]​)，再对每个话题，随机生成一个单词序列，共生成m个 在这个定义下，我们可以直接得到所有先验的表示，因此，可以使用最大后验概率估计MAP来求解参数，记住在这里w是观测量，z是隐变量，因此后验是已知观测量算隐变量的概率：文本w的后验=P(θ,z∣w,α,β)=P(w,z,θ,ϕ∣α,β)P(w∣α,β)文本w的后验=P(\\bold{\\theta}, \\bold z|\\bold w, \\alpha,\\beta) = \\frac{P(\\bold w,\\bold z,\\theta,\\phi|\\alpha,\\beta)}{P(\\bold w|\\alpha,\\beta)}文本w的后验=P(θ,z∣w,α,β)=P(w∣α,β)P(w,z,θ,ϕ∣α,β)​然后，就像PLSA方法一样，根据向量的嵌套定义把概率计算拆成每个元素概率的乘积，式子很长，直接贴一下结果吧：自然，想对这种玩意求解析最大值，和自杀没什么区别。不过，既然这次我们估计的是后验概率分布，概率论中可以以 变分推断 的方法来处理 变分推断 所谓变分推断，是取一个用隐变量z的分布q(z)q(z)q(z)来近似后验概率的条件分布p(z∣x)p(z|x)p(z∣x)的方法。为了比较二者的分布相似度，使用KL散度来计量： 如果能找到与p(z∣x)p(z|x)p(z∣x)在KL散度意义下最近的分布 q^(z)\\hat q(z)q^​(z)，则可以用这个分布近似后验概率分布 带入KL散度的公式： 则，如果想要KL散度更小，必有 log p(x)≥Eq[log p(x,z)]−Eq[log q(z)]log\\ p(x) \\geq E_q[log\\ p(x,z)] - E_q[log\\ q(z)] log p(x)≥Eq​[log p(x,z)]−Eq​[log q(z)] 因为先验x的分布log p(x)log\\ p(x)log p(x)可以被视为常量，因此右侧关于q分布的期望式越大，KL散度就越小。数学上把右侧称为证据下界。因此，下面问题就变为求证据下界的最大化。还有一个假设，是对于隐变量z（向量），假设分布q(z)q(z)q(z)对 z 的所有分量都是独立的，即： q(z)=q(z1)q(z2),...,q(zm)q(z) = q(z_1)q(z_2),...,q(z_m) q(z)=q(z1​)q(z2​),...,q(zm​) 称其为平均场。 现在让我们回到LDA模型的最大后验概率估计，现在我们有了变分推断算法，可以定义“文本w的后验” 的证据下界： L(r,t,α,ϕ)=Eq[log p(z,w)]−Eq[log q(z)]=Eq[log p(θ,z,w∣α,ϕ)]−Eq[log q(θ,z∣r,t)]L(r,t,\\alpha,\\phi) = E_q[log\\ p(\\bold z, \\bold w)] -E_q[log\\ q(\\bold z)]\\\\ = E_q[log\\ p(\\theta,\\bold z, \\bold w| \\alpha,\\phi)] -E_q[log\\ q(\\theta,\\bold z|r,t)] L(r,t,α,ϕ)=Eq​[log p(z,w)]−Eq​[log q(z)]=Eq​[log p(θ,z,w∣α,ϕ)]−Eq​[log q(θ,z∣r,t)] 其中，向量r和t是变分参数，r来估计隐变量z在单词向量中的分布参数θ\\thetaθ，t来估计话题向量中的分布参数(z1,z2,...,zn)(z_1,z_2,...,z_n)(z1​,z2​,...,zn​)有了平均场假设，就可以对每个文本分来计算，得到所有文本的证据下界： L′(r,t,α,ϕ)=∑m=1MEq[log p(θm,zm,wm∣αm,ϕm)]−Eq[log q(θm,zm∣rm,tm)]L(r,t,\\alpha,\\phi) =\\sum_{m=1}^M{ E_q[log\\ p(\\theta_m,\\bold z_m, \\bold w_m| \\alpha_m,\\phi_m)] -E_q[log\\ q(\\theta_m,\\bold z_m|r_m,t_m)]} L′(r,t,α,ϕ)=m=1∑M​Eq​[log p(θm​,zm​,wm​∣αm​,ϕm​)]−Eq​[log q(θm​,zm​∣rm​,tm​)] 此时我们发现它也是一个 离散的期望最大化估计 问题了，可以使用第二节提到的 EM算法来迭代更新参数，此时有四个参数向量，两个是为了变分推断引入的，另外两个则为模型建立的狄利克雷分布参数。 注意，实际上狄利克雷分布的参数是α、β\\alpha 、\\betaα、β，不过因为话题的分布参数ϕ\\phiϕ可以直接由参数β\\betaβ根据话题数 k 得到，因此这里简化一下模型，直接估计参数向量ϕ\\phiϕ 这种方法被综合称为 变分EM算法 LDA和PLSA的比较 相同点：都将 话题建模为单词的多项分布，文本建模为话题的多项分布 不同点： PLSA没有使用先验分布（ 或者说假设先验分布是均匀分布 ），使用MLE估计。 而LDA假设了狄利克雷分布作为先验分布，且使用MAP估计。 有先验分布的好处和C2中提到的一样，可以防止过拟合问题","tags":["生成模型","笔记"]},{"title":"【笔记】生成模型基础与应用 - 第2章","path":"/2023/11/27/生成模型基础与应用笔记-第2章/","content":"C2 离散数据的生成模型 C2 离散数据的生成模型 生成模型的目的：学习联合概率分布 P(X,Y)结合第一节课的知识，接下来我们会介绍一些比较传统的生成模型，它们不会涉及DL和神经网络，但是是后续生成模型的数学基础。 贝叶斯概念学习（以离散模型举例） 只提供正向的样本，让模型学习正向的特征，然后判断输入是否属于要训练的类别（概念） 与二分类任务的原理相同，但是训练二分类模型会提供 both 正向样本和负向样本 **以学习一个猜数字的模型为例：**问题：有一个由数字组成的数据集D\\mathcal {D}D，问如何学习一个概念C（C是未知的，描述数据集D\\mathcal {D}D中数字应该服从的分布），这里的数字都在0-100之间。 假设空间：人为地提出“假设”，用来猜测概念C。根据某一个概念假设（如“偶数”），所有可能生成的元素组成h的集合。比如“偶数”，则对应的假设数据集h={2,4,6,...,100}h=\\{2,4,6,...,100\\}h={2,4,6,...,100}。所有可以提出假设组成假设空间 版本空间：即所以“有效的”假设组成的空间。即满足：数据集D中的所有元素在假设的集合h中，这些假设组成的空间。 注意：一般来说，数据集D中包含的元素数量越多，版本空间就会越小，因为会有很多假设连数据集中的采样数据都过不了关，就被筛除了。 似然（likehood）：从h中独立采样N次，从假设能生成 数据集D中数据的概率 P(D∣h)P(\\mathcal {D}|h)P(D∣h) P(D∣h)=[1∣h∣]NP(\\mathcal {D}|h) = [\\frac{1}{|h|}]^NP(D∣h)=[∣h∣1​]N, 这里|h|表示h的集合大小 考虑假设1：h=“偶数”，假设2：h=“2的幂” 若 D = {16}，则假设1的概率P(D|h) = 1/6，则假设2的概率P(D|h) = 1/50 若 D = {2,8,16,64} 此时，假设2 的 似然概率是 假设1 的4812.5倍 奥卡姆剃刀原则：选择似然相近的假设时，假设h中包含的数据数量越少越好 先验概率：当提出假设时，为其中的数据赋予先验概率P(h)P(h)P(h) 。当提出的假设在数据集中对应的实际意义不太自然时，为其赋予低的先验概率 比如，D = {50,60,70,90}，给与两个假设，分别包含数据 67 和 210 如果数据集描述的是自然数，那么包含210的假设拥有更高的P(h) ，因为210和D中数据都是10的倍数 如果数据集描述的人的体重数据(kg)，那么包含67的假设拥有更高的P(h)，因为210在不太可能是符合概念的数据 后验：判断版本空间中的概念谁更接近要学习的概念。根据贝叶斯公式，后验概率与 先验概率和似然的乘积成正比：$$P(h|\\mathcal {D}) \\propto P(\\mathcal {D}|h) P(h)$$，后验概率最大的假设可以被认为是最优假设。 如果用H标记假设空间中所有假设i组成的集合，那么将数据集D与每种假设的联合概率求和，就是后验概率的基数了，即： P(h∣D)=P(D∣h)P(h)∑i∈HP(D,i)P(h|\\mathcal {D}) = \\frac{P(\\mathcal {D}|h)P(h)}{\\sum_{i \\in H}P(\\mathcal {D},i)} P(h∣D)=∑i∈H​P(D,i)P(D∣h)P(h)​ 注意：预先定义的假设空间不一定是完备的，真·概念C不一定在你的空间中，如果这样（现实中大多数情况也是如此），找到的最优假设只能说是最接近C的。 接下来我们尝试直接用数学推导出求最大后验概率的公式： 最大后验概率估计 MAP 我们要求一个h，其会使得后验概率最大，数学中我们用argmax函数来描述使得一个函数取最大值时，自变量的值，因此： h^map=argmaxh(P(h∣D))=argmaxh(P(D∣h)P(h)∑i∈HP(D,i))\\hat h^{map} = argmax_h(P(h|D)) = argmax_h(\\frac{P(\\mathcal {D}|h)P(h)}{\\sum_{i \\in H}P(\\mathcal {D},i)}) h^map=argmaxh​(P(h∣D))=argmaxh​(∑i∈H​P(D,i)P(D∣h)P(h)​) 分母对所有假设的联合概率求和，这与h无关，因此我们可以认为：h^map=argmaxhP(D∣h)P(h)=argmaxh[log([1∣h∣]N)+log(P(h))]\\hat{h}^{map} = argmax_h P(\\mathcal {D}|h)P(h) = argmax_h [log([\\frac{1}{|h|}]^N) + log(P(h))]h^map=argmaxh​P(D∣h)P(h)=argmaxh​[log([∣h∣1​]N)+log(P(h))] 右边我们取对数，在不破坏函数的单调性的同时将乘积的概率变为加法，然后我们会发现： 常量N会影响最大函数的取值 当N越大，即数据足够多时，P(D∣h)P(\\mathcal {D}|h)P(D∣h)即log([1∣h∣]N)log([\\frac{1}{|h|}]^N)log([∣h∣1​]N)的对数增长速率远大于P(h)P(h)P(h) 因此在计算出最大概率的h时，后者影响越来越小，直到可以忽略不计，因此，在实际建模中，我们常常只估计似然P(D∣h)P(\\mathcal {D}|h)P(D∣h)，我们将这种近似的方法称为：极大似然估计 MLE : \\hat{h}^{mle}\\ \\~= \\ argmax_h \\ P(\\mathcal {D}|h) = argmax_h \\ log([\\frac{1}{|h|}]^N) 于是，当N足够大时，将每个H空间中的假设h代入上式，使得函数取值最大的h可以被视为最优假设。 生成新数据：已知最优假设h后，便可以直接以该估计作为条件，求出生成各种元素（x）的概率： p(xˉ∣D)=∑h∈HP(h∣D)P(xˉ∣h)≈P(xˉ∣h^map)p(\\bar x| \\mathcal {D} ) = \\sum_{h\\in H}{P(h|\\mathcal {D})P(\\bar x|h)} \\approx P(\\bar x|\\hat{h}^{map}) p(xˉ∣D)=h∈H∑​P(h∣D)P(xˉ∣h)≈P(xˉ∣h^map) 以上是一个离散分布的贝叶斯概念模型学习的例子，根据概率论，这个方法也可以推广到的连续随机变量的情况，比如服从二项分布的概率模型。相应地，我们需要将计算似然和概率估计的概率P(X)改为概率密度p(x)，而将求和计算转为积分。下面会介绍两个连续分布的贝叶斯概念模型 Beta二项分布生成模型 现在来看连续随机变量的情况，以抛硬币的二项分布为例： 抛一个硬币N次，记录有N0N_0N0​次正面朝上，N1N_1N1​次反面朝上 假设每次抛硬币的结果为随机变量x，x服从二项分布，正面朝上记为1，反面朝上记为0 那么，二项分布的参数θ\\thetaθ（正面朝上的概率）在[0,1]之间，是一个连续变量，也是我们要估计的 注意：我们的目标是估计θ\\thetaθ的分布，而不是直接求出一个结果（不然直接频率作为概率不就完了，不行，你这不贝叶斯啊）。 因为有很多可能的θ\\thetaθ，都可以使得我们的抛硬币实验得到上述的结果。 我们现在已知N次独立实验的结果，N0N_0N0​次正面朝上，N1N_1N1​次反面朝上就是“数据集”的数据，因此可以写出已知采样与我们实际要求的“抛硬币哪一面向上”分布的似然： likehood：$$P(\\mathcal {D} | \\theta) = \\theta{N_1}(1-\\theta){N_0}$$ 记住我们要求后验概率，它等价于先验概率和似然的乘积。现在的问题在于：我们怎么知道先验概率p(θ)p(\\theta)p(θ)？我们可以用贝塔分布来作为我们的先验概率模型，之所以是它的理由如下： 贝塔分布常用于估计[0,1]分布上，缺少足够先验样本的分布 贝塔分布有一个重要的特性：共轭先验，即将其带入贝叶斯模型，得到的后验概率的函数形式与先验相同。而如果我们将独立实验的结果N0N_0N0​或N1N_1N1​次作为随机变量计算先验，其服从伯努利分布，似然函数的形式也与贝塔分布的先验相同。 我们假设先验服从贝塔分布：p(θ)∼Beta(θ∣a,b)p(\\theta) \\sim Beta(\\theta|a,b)p(θ)∼Beta(θ∣a,b)于是我们现在可以计算后验： P(θ∣D)∝[θN1(1−θ)N0][θa−1(1−θ)b−1]=[θN1+a−1(1−θ)N0+b−1]=Beta(θ∣N1+a,N0+b)P(\\theta|\\mathcal {D}) \\propto [\\theta^{N_1}(1-\\theta)^{N_0}][ \\theta^{a-1}(1-\\theta)^{b-1}] = [\\theta^{N_1+a-1}(1-\\theta)^{N_0+b-1}]\\\\ = Beta(\\theta|N_1+a,N_0+b) P(θ∣D)∝[θN1​(1−θ)N0​][θa−1(1−θ)b−1]=[θN1​+a−1(1−θ)N0​+b−1]=Beta(θ∣N1​+a,N0​+b) 现在，我们只需估计后验函数取尽可能大值时，参数θ的分布，根据贝塔分布的特性， Beta(a,b) 分布的随机变量取最大值时，参数为a−1a+b−2\\frac{a-1}{a+b-2}a+b−2a−1​ 得到最大后验概率估计：θ^map=a−1+N1a+b−2+N\\hat \\theta^{map} = \\frac{a-1+N_1}{a+b-2+N}θ^map=a+b−2+Na−1+N1​​，根据共轭先验的性质，我们要估计的θ\\thetaθ分布也是一个贝塔分布，其具体形状与参数a，b有关。 如果我们使用极大似然估计，即不考虑先验，则结果退化为：θ^mle=N1N\\hat\\theta^{mle} = \\frac{N_1}{N}θ^mle=NN1​​，即直接用频率估计概率的结果，如果我们取上述贝塔分布的a=b=1（均匀分布，认为先验的结果硬币正面朝上的概率就是五五开的），那么也会得到这个结果。 使用MAP的结果，我们可以生成下一次抛硬币正面朝上的结果估计： P(xˉ=1∣D)=∫01P(xˉ=1∣θ)P(θ∣D)dθ=∫01θBeta(θ∣a+N1,b+N0)dθ=E[θ∣D]=a+N1a+b+NP(\\bar x=1|\\mathcal D) = \\int_0^1P(\\bar x =1 |\\theta)P(\\theta|\\mathcal D)d\\theta = \\int_0^1\\theta Beta(\\theta|a+N_1,b+N_0) d\\theta \\\\= E[\\theta|D] = \\frac{a+N_1}{a+b+N} P(xˉ=1∣D)=∫01​P(xˉ=1∣θ)P(θ∣D)dθ=∫01​θBeta(θ∣a+N1​,b+N0​)dθ=E[θ∣D]=a+b+Na+N1​​ 可以发现，结果正好是我们的贝塔分布模型的均值。有趣的是，只要我们使用MAP的生成结果，即使取a=b=1的均匀分布，也会使得估计正面朝上的概率中，分子至少为1，不会出现 _因为实验数据集中没有正面朝上，就将其判定为不可能事件 _的情况。因此，这种方法也被称为**”（拉普拉斯）+1平滑“** 狄利克雷多项分布生成模型 再来看一个复杂一点的情况：随机变量有K个可能的取值，比如说，估计一个有k面的骰子，每个面朝上的概率。我们假设做了N次实验，每个面朝上的次数分别为：N1,N2,...NkN_1,N_2,...N_kN1​,N2​,...Nk​和伯努利实验类似，可以这样定义： 似然函数： p(D∣θ)=θ1N1θ2N2...θkNkp(\\mathcal D|\\theta) = \\theta_1^{N_1}\\theta_2^{N_2}...\\theta_k^{N_k} p(D∣θ)=θ1N1​​θ2N2​​...θkNk​​ 先验函数，将之前贝塔分布的共轭先验推广到多个变量，这就是狄利克雷多项分布： p(θ)=Dir(θ∣α)p(\\theta) = Dir(\\theta|\\alpha) p(θ)=Dir(θ∣α) 后验计算： 最大后验概率估计： θ^k=Nk+αk−1N+∑k=1Kαk−K\\hat \\theta_k = \\frac{N_k+\\alpha_k-1}{N+\\sum^K_{k=1}\\alpha_k - K} θ^k​=N+∑k=1K​αk​−KNk​+αk​−1​ 同样的，如果使用最大似然估计，会退化为：θ^k=NkN\\hat \\theta_k = \\frac{N_k}{N}θ^k​=NNk​​ 生成一个骰子点数，其结果是j点的概率： P(xˉ=j∣D)=E(θj∣D)=αj+Nj∑k=1Kαk+NP(\\bar x=j|\\mathcal D) = E(\\theta_j|\\mathcal D) = \\frac{\\alpha_j+N_j}{\\sum^K_{k=1}\\alpha_k+N} P(xˉ=j∣D)=E(θj​∣D)=∑k=1K​αk​+Nαj​+Nj​​ 朴素贝叶斯分类器（NBC） 在第一章，我们说了 生成模型 包含 分类模型的所有功能。比如下面这个分类问题： 任务：将若干个由离散数据组成的向量进行分类。设变量 x=(x1,x2,...xD)Tx = (x_1,x_2,...x_D)^Tx=(x1​,x2​,...xD​)T是长度为D的特征向量，每个元素（特征）有KKK个可能的取值，希望将所有这样的向量分为C类： {1,2,…C} ，用 Y=c 来表示结果的类标号。于是我们想要知道，给定向量x和模型参数向量θ，分类的概率：Py=P(Y=c∣x,θ)P_y=P(Y=c|x,\\theta)Py​=P(Y=c∣x,θ)，只要对所有的类别c计算这个概率，取最大值作为最终分类。 我们尝试用贝叶斯生成模型的方法来解决这个问题。这涉及到刚才两节二项分布和狄利克雷多项分布方法的综合。 首先，我们考虑给定类别c，能生成特征向量x的概率： Px=P(x∣Y=c,θ)P_x = P(x|Y=c,\\theta)Px​=P(x∣Y=c,θ) 则由贝叶斯公式，要求的概率可以表示为： Py=P(Y=c∣x,θ)=P(x,Y=c∣θ)P(x∣θ)=P(x,Y=c∣θ)∑i∈C[P(x,Y=i∣θ)]=P(x∣Y=c,θ)P(Y=c∣θ)∑i∈C[P(x∣Y=i,θ)P(Y=i∣θ)]∝P(Y=c∣θ)PxP_y=P(Y=c|x,\\theta) = \\frac{P(x,Y=c|\\theta)}{P(x|\\theta)} = \\frac{P(x,Y=c|\\theta)}{\\sum_{i \\in C}[P(x,Y=i|\\theta)]} = \\frac{P(x|Y=c,\\theta)P(Y=c|\\theta)}{\\sum_{i \\in C}[P(x|Y=i,\\theta)P(Y=i|\\theta)]} \\propto {P(Y=c|\\theta)}P_x Py​=P(Y=c∣x,θ)=P(x∣θ)P(x,Y=c∣θ)​=∑i∈C​[P(x,Y=i∣θ)]P(x,Y=c∣θ)​=∑i∈C​[P(x∣Y=i,θ)P(Y=i∣θ)]P(x∣Y=c,θ)P(Y=c∣θ)​∝P(Y=c∣θ)Px​ 可以发现，因为现在问题空间包含两个随机变量x，Y，对于类别的变量Y，式子中也推出了先验概率。 P(Y=c∣θ)P(Y=c|\\theta)P(Y=c∣θ)就是分类先验（在训练的数据集中，最终被每一类被分入了多少个向量）。 这会使得模型的参数向量θ中还要包含分类先验的参数。如果将它们和向量x的参数混为一谈，这将导致无法更新参数。为了能将向量内的每个特征对应的参数拆分出来计算，朴素贝叶斯提出一个重要的假设： 给定同一个类别的标签，每一个特征的条件都是独立的 注意：真实世界中，这个假设是很难成立的，但是依然该方法依然可以用来估计分类器 则我们可以将特征向量在模型中的参数用θ表示，而分类先验的参数在下文将用π\\piπ来表示，它们将被分别估计。先来看特征向量： Px=P(x∣Y=c,θ)=Πi=1DP(xi∣Y=c,θic)=Πi=1DTiP_x = P(x|Y=c,\\theta) = \\Pi_{i=1}^{D}P(x_i|Y=c,\\theta_{ic}) = \\Pi_{i=1}^{D} T_i Px​=P(x∣Y=c,θ)=Πi=1D​P(xi​∣Y=c,θic​)=Πi=1D​Ti​ 我们用TiT_iTi​来标记以 向量中的每个特征 为随机变量的 分布。对于特征的定义不同，这个分布会表示成不同的形式： 如果特征是0-1编码的：值不是0就是1，那么TiT_iTi​是伯努利分布，参数θic\\theta_{ic}θic​要表示 第i个特征的值会使得整个向量有多大可能性落在分类c中 如果每个特征有KKK个可能的取值（原始的问题），那么TiT_iTi​是多项分布，参数θic\\theta_{ic}θic​要表示为一组向量。 如果特征是连续的实数来表示的，那么TiT_iTi​将是一个连续的高斯分布，参数也要表示为均值，方差的一组值。 回到问题的开始，为了下面的推导写的比较简单，我们将问题简化为只要0-1编码的两种特征 （它已经很复杂了QAQ，再加上K个类别我都不敢想……） 然后我们来看看如何使用贝叶斯学习来进行训练：假设训练数据集中包含了N个**向量x, 分类标签Y**的数据对，每个对用xi,Yi(i=1,2,...,n)x_i,Y_i(i=1,2,...,n)xi​,Yi​(i=1,2,...,n)来表示，那么分类的先验表示为：$$P(Y_i|\\pi) = \\Pi\\ \\pi_c^{if(Y_i =c)}$$ [...]if(Yi=c)[...]^{if(Y_i =c)}[...]if(Yi​=c)表示计数函数，即只有第i个分类标签为c时，才保留参数πc\\pi_cπc​【没错，还是统计方法】 注意这里作为条件概率的pi和下面的θ都是向量，而右侧式子表示取向量中的哪一个有效值（用符号π\\piπ来区分 分类的参数向量 和 特征的参数向量θ\\thetaθ） 同样，特征的先验计算为： P(xi∣Yi,θ)=Πj=1DP(xi[j] ∣Yi,θj)=Πj=1D(Π P(xi[j] ∣θjc)if(Yi=c))P(x_i|Y_i,\\theta) = \\Pi_{j=1}^DP(x_i[j]\\ |Y_i,\\theta_j) = \\Pi_{j=1}^D(\\Pi\\ P(x_i[j]\\ |\\theta_{jc})^{if(Y_i=c)}) P(xi​∣Yi​,θ)=Πj=1D​P(xi​[j] ∣Yi​,θj​)=Πj=1D​(Π P(xi​[j] ∣θjc​)if(Yi​=c)) 于是，推导整个数据集的后验概率函数，即将所有数据的结果相乘起来： P(D∣θ)=Πi=1N[P(Yi∣θ)P(xi∣Yi,θ)]P(\\mathcal D|\\theta) = \\Pi_{i=1}^N[P(Y_i|\\theta)P(x_i|Y_i,\\theta)] P(D∣θ)=Πi=1N​[P(Yi​∣θ)P(xi​∣Yi​,θ)] 尝试使用极大似然估计来直接计算参数（MLE），直接对两边取对数，得到： log[P(D∣θ)]=∑c=1CNc+∑j=1D(∑c=1C(∑Yi=clogP(xi[j]∣θjc)))log[P(\\mathcal D|\\theta)] = \\sum_{c=1}^C N_c + \\sum_{j=1}^D(\\sum_{c=1}^C(\\sum_{Y_i=c}logP(x_i[j]|\\theta_{jc}))) log[P(D∣θ)]=c=1∑C​Nc​+j=1∑D​(c=1∑C​(Yi​=c∑​logP(xi​[j]∣θjc​))) （其中xi[j]x_i[j]xi​[j]表示向量xix_ixi​中的第jjj个值）上面的式子看着非常复杂，其实本质上与上一节的二项分布估计相同： 分类的先验估计就是：从数据集N中统计出分类c出现次数NcN_cNc​，并计算比例： πc=NcN\\pi_{c} = \\frac{N_{c}}{N}πc​=NNc​​ 特征的似然参数估计就是：从每个已知类别的出现次数中，统计出是当前特征的比例，即： θjc=NjcNc\\theta_{jc} = \\frac{N_{jc}}{N_c}θjc​=Nc​Njc​​ （不是0-1编码的，NjcN_{jc}Njc​就是一个嵌套的向量了） 这个结果，与抛硬币和骰子一样，因为先验是完全按照统计结果的，容易出现过拟合的问题。另一种方法，可以使用刚才的贝叶斯方法，对pi和θ两个参数的分布假设先验： :::warning 假设P(Yi∣π)P(Y_i|\\pi)P(Yi​∣π)是一个狄利克雷分布，参数是α=(α0,α1,...αC)\\alpha = (\\alpha_0,\\alpha_1,...\\alpha_C)α=(α0​,α1​,...αC​) 假设每个 P(xi[j] ∣θjc)P(x_i[j]\\ |\\theta_{jc})P(xi​[j] ∣θjc​)都是一个贝塔分布，参数是(β0,β1)(\\beta_0,\\beta_1)(β0​,β1​) ::: 然后计算后验，形式是一样的，只不过丑陋的计数函数被替换成了先验分布的均值：代入最大后验概率估计MAP的估计结果是：如果有一条新的数据需要分类，那么对于每个类别c=1,2,...Cc=1,2,...Cc=1,2,...C，将MAP估计的参数带入P(Y=c∣xˉ,θ)P(Y=c|\\bar x,\\theta)P(Y=c∣xˉ,θ)，得到使得概率最大的c就是要求的分类。可以看出，朴素贝叶斯模型虽然推导起来比较复杂（其实也只是复杂在数据维度比较多，人脑CPU容易干烧，交给计算机来还是很快的），但是计算复杂度很低，所以在小场景中使用广泛。同时，我们也可以用这个模型生成新的数据：和二项分布生成模型一样，在生成新的数据时，代入模型的参数就是分布的均值：","tags":["生成模型","笔记"]},{"title":"【笔记】生成模型基础与应用 - 第1章","path":"/2023/10/28/生成模型基础与应用笔记-第1章/","content":"C1 概率论和统计基础 C1 概率论和统计基础 什么是概率？ 频数的概率解释：频率估计概率（小学就学过的） 贝叶斯概率解释：概率是某样事件的“不确定度”，是人们在多大程度上相信某件事情将会发生（似然likehood） 因为贝叶斯概率能适用更多更广泛的概率事件（例如：从未发生但可能的事件），因此以下讨论的概率偶都基于贝叶斯概率 事件、随机变量和概率分布： 事件：自然语言描述的具有随机性的事件 随机变量：可能从有限 或 可数的无限 集合X中随机取值的变量 概率的表示：P(事件A) = P(随机变量X=某值a) 【如果我们定义当X取a时事件A发生】 概率分布：表述随机变量X的 取值 的 概率规律 的集函数 离散随机变量要点回顾 事件合并概率： P(A∪B)=P(A)+P(B)−P(A∩B)P(A ∪ B) = P(A) + P(B) - P(A ∩ B)P(A∪B)=P(A)+P(B)−P(A∩B) 如果A和B是互斥事件，则：P(A∪B)=P(A)+P(B)P(A ∪ B) = P(A) + P(B)P(A∪B)=P(A)+P(B) 两个随机变量的联合概率： P(A,B)=P(A)P(B∣A)=P(B)P(A∣B)P(A,B) = P(A)P(B|A) = P(B)P(A|B)P(A,B)=P(A)P(B∣A)=P(B)P(A∣B) 联合概率分布的链式法则： P(X1,X2,...,XD)=P(X1)P(X2∣X1)P(X3∣X1,X2)...P(XD∣X1,X2...XD)P(X_1,X_2,...,X_D)=P(X_1)P(X_2|X_1)P(X_3|X_1,X_2)...P(X_D|X_1,X_2...X_D)P(X1​,X2​,...,XD​)=P(X1​)P(X2​∣X1​)P(X3​∣X1​,X2​)...P(XD​∣X1​,X2​...XD​) 两个随机变量的条件概率： P(B∣A)=P(B,A)P(A)=P(B)P(A∣B)P(A) ,P(A)0P(B|A) = \\frac{P(B,A)}{P(A)}=\\frac{P(B)P(A|B)}{P(A)} \\ ,P(A) 0P(B∣A)=P(A)P(B,A)​=P(A)P(B)P(A∣B)​ ,P(A)0 贝叶斯条件概率公式 重要名称： 先验：已知的，B事件发生的概率 似然：在B的条件下A发生的概率 与 A发生的概率 之比：P(A∣B)P(A)\\frac{P(A|B)}{P(A)}P(A)P(A∣B)​ 后验：在A的条件下，B发生的概率，即P(B|A) 两个随机变量的条件独立性： 如果两个变量的联合概率可以被拆分为各自概率的乘积，则称两个变量是 （无条件）独立的 P(X,Y)=P(X)P(Y)P(X,Y) = P(X)P(Y)P(X,Y)=P(X)P(Y) 而如果给定随机变量Z，在Z的条件概率下满足上述条件，则称两个变量X,Y是 条件独立的 P(X,Y∣Z)=P(X∣Z)P(Y∣Z)P(X,Y|Z) = P(X|Z)P(Y|Z)P(X,Y∣Z)=P(X∣Z)P(Y∣Z) 连续随机变量要点回顾 累计分布函数（cdf）：F(x) = P(X = x)当连续变量的取值小于x时，总计的概率 概率密度函数（pdf）：p(x) = F(x) dx 是累计分布函数的微分 计算连续随机变量在a,b区间上的概率：F(b) - F(a)，或是在p(x)上积分 连续分布的数学量：均值E、方差σ^2、中位数…… 协方差：对于两个随机变量X,Y，衡量它们的线性相关性： cov[X,Y]=E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y)cov[X,Y] = E[(X-E(X))(Y-E(Y))] = E(XY) - E(X)E(Y)cov[X,Y]=E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y) 相关系数：将协方差标准化后的数学量： corr[X,Y]=R=cov[X,Y]σ2(X)σ2(Y)corr[X,Y] = R= \\frac{cov[X,Y]}{\\sqrt{\\sigma^2(X) \\sigma^2(Y)}}corr[X,Y]=R=σ2(X)σ2(Y)​cov[X,Y]​ 独立的两个变量，它们不相关 但是，不相关的两个变量，可能互相独立 如果两个符合高斯分布的变量不相关，则它们一定独立 对于多元随机变量的联合分布，协方差和相关系数的计算将变为矩阵的形式： 常见概率分布回顾 【离散的】 经验分布（eCDF）：描述从抽样中得到的概率分布，经验分布的概率密度函数即为所有抽样的结果之和，其中抽样被定义为 狄利克雷函数：即抽样的结果只有0或者1. 二项分布：重复n次独立的 伯努利事件 实验，获得其中一种结果k次的概率分布 伯努利事件：某个事件只有两种可能的结果（布尔随机变量），其中一种的概率为p，另一个为1-p 经典的例子是抛硬币 多项分布：重复n次独立的 多重伯努利事件 实验，获得 每种结果的次数 的概率分布 多重伯努利事件：某个事件可能有k种不同的结果，并且每种结果具有固定的概率 经典的例子是投一个k面的骰子 【连续的】 均匀分布：在一个区间或域上，随机变量的取值为固定值 对于一维变量，随机分布的概率密度函数为：Unif(x)=1b−a(a≤x≤b)Unif(x) = \\frac{1}{b-a} (a \\leq x \\leq b )Unif(x)=b−a1​(a≤x≤b) 正态分布（高斯分布）：多个相互独立的随机变量之和 的分布 会趋近于这个分布，因此它被广泛使用 正态分布的概率密度函数和符合该分布的随机变量的均值和方差有关 N(x∣μ,σ2)=12πσ2e−12σ2(x−μ)2N(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}N(x∣μ,σ2)=2πσ2​1​e−2σ21​(x−μ)2 可以拓展为多元变量的正态分布，将均值修改为多元变量的数学期望，方差修改为多元变量的协方差 泊松分布：对二项分布的连续近似，在二项分布的实验次数n很大，单次概率p很小时，二项分布可被近似为泊松分布。 P(x∣λ)=e−λλxx!P(x|\\lambda) = e^{-\\lambda}\\frac{\\lambda^x}{x!}P(x∣λ)=e−λx!λx​, 其方差和均值都是 λ Student t 分布：基于正态分布，为了增强其抗干扰性而提出的分布，加入一个参数v： 拉普拉斯分布（双指数分布）：在均值的两侧，呈现对称分布规律的一种 指数分布 变种 Lap(x∣μ,b)=12be−∣x−μ∣bLap(x|\\mu,b) = \\frac{1}{2b}e^{-\\frac{|x-\\mu|}{b}}Lap(x∣μ,b)=2b1​e−b∣x−μ∣​，其均值为 μ\\muμ，方差为 2b22b^22b2 伽马分布：对正实数域上的随机变量建模的分布，是多个独立同分布的指数分布变量 和 的分布 Gamma(x∣a,b)=baγ(a)xa−1e−xbGamma(x|a,b) = \\frac{b^a}{\\gamma(a)}x^{a-1}e^{-xb}Gamma(x∣a,b)=γ(a)ba​xa−1e−xb，其中γ(a)是伽马函数：\\gamma(a) = \\int_0^\\inf t^{a-1}e^{-t}dt 参数a被称为shape，b被称为rate，该分布均值为 ab\\frac{a}{b}ba​，方差为ab2\\frac{a}{b^2}b2a​ 贝塔分布：对[0,1]区间上取值的随机变量建模的分布 Beta(x,∣a,b)=1β(a,b)xa−1(1−x)b−1Beta(x,|a,b) = \\frac{1}{\\beta(a,b)}x^{a-1}(1-x)^{b-1}Beta(x,∣a,b)=β(a,b)1​xa−1(1−x)b−1,其中 β(a,b)\\beta(a,b)β(a,b)是贝塔函数，它只是为了使得这个分布的概率密度积分等于1才加上的。 狄利克雷分布：将贝塔分布拓展到多元变量的泛化 【分布的变换】 若分布Y可以由服从分布X的随机变量，将每个取值用离散或连续的函数f变换得到，那么分布Y的均值和方差会遵循以下公式 线性变换： 通用变换： 离散变量： py(y)=∑x:f(x)=ypx(x)p_y(y)=\\sum_{x:f(x)=y}p_x(x)py​(y)=∑x:f(x)=y​px​(x) 连续变量： py(y)=px(x)∣dxdy∣p_y(y)=p_x(x)|\\frac{dx}{dy}|py​(y)=px​(x)∣dydx​∣ 其他重要概念 大数定律：随着样本规模的增加，样本均值对总体均值的估计越准确。 中心不变定理：多个随机变量样本的均值分布（随机变量和的分布）将近似于高斯分布。 蒙特卡洛近似：如果某随机变量X的分布未知，但可以对其进行抽样来实验，则可以使用经验分布来近似X的分布： 衡量两个分布的相似度（距离）：KL散度 先补充信息论的知识：信息熵 信息熵可以描述随机变量X在分布P上的不确定性的程度：H(X)=−∑Kk=1p(X=k)log2p(X=k)H(X) = -\\sum_K^{k=1}p(X=k)log_2p(X=k)H(X)=−∑Kk=1​p(X=k)log2​p(X=k) 均匀分布的信息熵最大 交叉熵：将服从分布P的变量转换到分布Q，需要提供额外信息（bits）的量，其中p和q代表P和Q的概率密度函数 H(p,q)=−∑kpklogqkH(p,q) = -\\sum_{k}p_klogq_kH(p,q)=−∑k​pk​logqk​ KL散度：描述两个分布的概率密度函数p和q的相似度： KL(p∣∣q)=∑k=1Kpklogpkqk=−H(p)+H(p,q)KL(p||q) = \\sum_{k=1}^Kp_klog\\frac{p_k}{q_k}=-H(p)+H(p,q)KL(p∣∣q)=∑k=1K​pk​logqk​pk​​=−H(p)+H(p,q) 互信息度：衡量两个分布的变量之间互相依赖的程度： II(X;Y)=KL(p(X,Y)∣∣p(X)p(Y)) =∑x∑yp(x,y)logp(x,y)p(x)p(y)=H(X)−H(X∣Y)=H(Y)−H(Y∣X)II(X;Y) = KL(p(X,Y)||p(X)p(Y))\\ = \\sum_x\\sum_yp(x,y)log\\frac{p(x,y)}{p(x)p(y)}\\\\=H(X)-H(X|Y)=H(Y)-H(Y|X)II(X;Y)=KL(p(X,Y)∣∣p(X)p(Y)) =∑x​∑y​p(x,y)logp(x)p(y)p(x,y)​=H(X)−H(X∣Y)=H(Y)−H(Y∣X)","tags":["生成模型","笔记"]},{"title":"2022影视游艺总结","path":"/2022/09/27/2022影视游艺总结/","content":"名称 类型 评分（满分10分+附加分2分） 备注 三角符文（Deltarune第二章） 游戏 10+2 JOJO的奇妙冒险 石之海（上部） 动漫 9+1 师父《Sifu》 游戏 8 《Elden Ring》艾尔登法环 游戏 9+0.5 史丹利的寓言 超级豪华版《The Stanley Parable Ultra Deluxe》 游戏 9+1.5 瞬息全宇宙《Everything.Everywhere.All.At.Once》 电影/剧集 9+1.5 爱，死亡和机器人 第三季《Love.Death.and.Robots.S03》 电影/剧集 9 林中小女巫《Little Witch in the Woods》 游戏 7.5+1 诡野西部《Weird West》 游戏 6 未完整体验 孤山难越《Insurmountable》 游戏 7 未完整体验 派对浪客诸葛孔明《Paripi Koumei》》 动漫 8+1 间谍过家家《Spy x Family》 动漫 9+0.5 迷失《Stray》 游戏 8.5+0.5 莉可丽丝《Lycoris Recoil》 动漫 8.5+1 哥布林弹球《Peglin》 游戏 7+1 独行月球 电影/剧集 5.5 Arcaea 4.0 游戏 8+1 咩咩启示录《Cult of the Lamb》 游戏 7+0.5 未完整体验 汉化日记 第三季 动漫 7.5+1 JOJO的奇妙冒险 石之海 Part.2（下部） 动漫 8+0.5","tags":["总结","评分表"]},{"title":"Nickbit's 2021影视游艺总结","path":"/2021/12/28/2021影视游艺总结/","content":"名称 类型 体验时间（估计） 体验地点 评分（满分10分+附加分1分） 备注 Alba: A Wildlife Adventure 游戏 2021.1 Home 7+1 虽然玩法、剧情都很简单，甚至说低幼，但仍让我感到十分放松。 咒术回战 动漫 2021.1 Home 9+1 奇蛋物语 动漫 2021.1 Home+寝室 8+1 特别篇是6月份在寝室看的，说实话没看懂。 唐人街探案3 电影 2021.1 电影院 7 没啥感觉 你好，李焕英 电影 2021.1 电影院 8 刺杀小说家 电影 2021.2 Home 6 人潮汹涌 电影 2021.2 Home 8 有点感觉 刺客五六七（第三季） 动漫 2021.4 寝室 8+1 汉化日记（第二季） 动漫 2021.1 Home 8+1 《玩梗王者》 工作细胞（第二季） 动漫 2021.2 Home 8+1 我的三体 张北海传 动漫 2021.1 Home 9+1 前进四！ The Unfinished Swan未完成的天鹅 游戏 2021.2 Home 8+0.5 很神，但是很难说出来在哪里 Spiritfarer 灵魂摆渡人 游戏 2021.2 Home 8.5+1 死亡教育喜+1 戴森球计划 游戏 2021.2 Home 10+1 戴森球，YYDS！ 双人成行 游戏 2021.3 寝室 10+1 GOTY！ 节奏医生 游戏 2021.2 Home+寝室 10+1 一二三四五六七！ 核聚变2021北京站 展会 2021.5 亦创国际会展中心 9 人人人从从从众众众 寻找李白 话剧 2021.5 科学会堂 8.5 《中轴线》 书籍 2021.5 寝室 8 冲动消费的绘本竟成为今年唯一纸质阅读入账，堕落啊！ 爱，死亡和机器人(第二季) 剧集 2021.5 寝室 7+0.5 年底想来，只记得冰面了 灵笼（下半部分及终章） 动漫 2021.5 寝室 6.5+0.5 星露谷物语 游戏 2021.8 寝室 【未完整体验】 但是真的太上头了 霓虹深渊 游戏 2021.7 寝室 【未完整体验】 Lacuna 游戏 2021.9 寝室 7.5+0.5 音乐好听 小林家的龙女仆S 动漫 2021.10 寝室 9.5+1 小林用的python，托儿他爸说python是龙族的语言：懂了，python天下第一！（ 瑞克和莫蒂（第五季） 剧集 2021.10 寝室 10+1 越来越费脑子了 空洞骑士 游戏 2021.10 工位+寝室 【未完整体验】 前半部分是真滴劝退，但是City of Tears还是让我有信心打下去的 DELTARUNE (第二章) 游戏 2021.10 寝室 【未完整体验】 中文估计已经有了，但是ddl人估计要鸽到明年玩了（） 影子工厂 游戏 2021.10 寝室 9+1 瞰哥，我的瞰哥（The Great Wild Unknow 循环中） 失控玩家 电影 2021.10 电影院 7.5 沙丘 电影 2021.10 寝室 8 有空去看看原著 双城之战 剧集 2021.11 寝室 9+1 看之前没想到这么帅 Inscryption 邪恶冥刻 游戏 2021.11 寝室 10+1 看看人家怎么做卡牌游戏的！ 光明记忆：无限 游戏 2021.11 寝室 7 JOJO的奇妙冒险：石之海 动漫 2021.12 寝室 9+1","tags":["总结","评分表"]},{"title":"使用Spring和JPA构建REST规范的后端API项目","path":"/2021/09/19/使用Spring和JPA构建REST规范的后端API项目/","content":"关于如何使用Spring和JPA的入门教程 注意：该文档最初创建于Sep 19, 2021，可能存在已过时的内容，仅供参考 建立Spring项目 使用Spring Initializr（Spring提供的依赖包生成网站） Dependencies中加入以下依赖 Spring Web Spring Data JPA 一个数据库驱动（使用你喜欢的数据库驱动，比如MySQL Driver） 设置好以后，点击Generate Project，就可以下载到对应的zip格式的空项目包 在IDE中可以使用Maven管理项目包（建立Maven项目），只要把对应的包版本按照xml的格式写到项目根目录的pom.xml中就可以了 ?xml version=1.0 encoding=UTF-8? project xmlns=http://maven.apache.org/POM/4.0.0 xmlns:xsi=http://www.w3.org/2001/XMLSchema-instance xsi:schemaLocation=http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd modelVersion4.0.0/modelVersion parent groupIdorg.springframework.boot/groupId artifactIdspring-boot-starter-parent/artifactId version2.5.4/version relativePath/ !-- lookup parent from repository -- /parent ...... dependencies dependency *添加你的依赖包* /dependency 配置程序主入口 一般来说，主入口文件都命名为XXXAplication.java，格式如下 import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class YourApplication public static void main(String... args) SpringApplication.run(YourApplication.class, args); 其中@SpringBootApplication标识服务端的入口类，SpringApplication.run(YourApplication.class, args)使用SpringBoot启动服务，一切配置都由其自动完成。 使用JPA链接模型类和数据库 JPA使用ORM（对象关系映射）来方便我们用代码链接数据库，简单来说，我们可以用一个类对应数据库中的一张表，类中的成员对应表中的属性，当类成员的值变化时，让JPA自动同步数据表中的值。这样的类一般称为模型类（MVC中的Model）或实体类，可以存放一些基础数据（比如用户类User） 语法说明 导入命名空间： import javax.persistence.Entity;import javax.persistence.GeneratedValue;import javax.persistence.Id; @Entityclass User\t... @Entity 注解，写在类前面，声明JPA托管该类 @Id @GeneratedValueprivate int id; @Id 和 @GeneratedValue：注解，写在类成员变量前面，声明该变量在数据表中所对应的属性是主关键字（PK） 其中@GeneratedValue注解向JPA声明主关键字的生成策略 @Id @GeneratedValueprivate long id;private String email; private String username; 对于非主关键字的属性，JPA会自动以类成员变量的命名，生成对应的属性名，无需再加入注解。 User() User(String email,String username) this.email = eamil;\tthis.username = username; 有参构造函数用来创建没有指定id（主关键字）的实例 配置数据库的加载 JPA对类数据的操作，都是通过自定义一个新的接口实现的。 建立一个新接口，继承JpaRepository接口： import org.springframework.data.jpa.repository.JpaRepository;interface UserRepository extends JpaRepositoryUser, long ... 其中，泛型参数User, long分别指JPA所链接的类的类型和其主关键字的类型. 接口类中可以自定义增删改查函数，如： // 返回一个对象User findByUsername(String username);// 根据pageable参数返回分页对象（一页的内容）PageUser findAllBy(Pageable pageable); 其中，findByXXXXX函数，Spring可以根据“XXXXX”的命名，自动对应模型类中同名（除了主键，不区分大小写）的数据变量，帮助你实现查找函数。即程序员只需声明该函数，并写对参数和返回类型。 findAllBy返回一个分页类型的所有数据成员。 然后，建立一个配置类，告诉Spring Boot加载接口对应数据类： import ...@Configuration public class Initialize @Bean CommandLineRunner init(UserRepository userRepository, ...) return args - userRepository.save(new User()...) ; 其中： @Configuration标识类是用来配置Spring Boot的加载项的 @Bean标识对象是一个Spring Bean CommandLineRunner接口：实现该接口的所有Bean都会在启动Spirng Boot时被加载。其中传入的参数即为JpaRepository类型的自定义接口对象（可以有多个，对应多个数据表），通过这些对象，即可以在服务加载时初始化数据库中内容。 在返回的匿名函数内用JpaRepository对象对数据表进行初始化操作： 使用xxxRepository.findByXXXX()函数，查找数据表的条目（前提：现在接口类中定义对应的函数） 使用xxxRepository.save()函数，修改并保存对应repository对应的模型类的成员数据，同时JPA会同步更新数据库的内容 注意：如果要操作模型类对象，使用其getter和setter函数，如 User user = userRepository.findByUsername(wht)String _info = user.getInfo();userRepository.save(new User().setUsername(admin) .setInfo(_info)) 在模型类中，注意封装好各个成员变量的getter和setter函数 实现HTTP控制模块 建立一个新类，用来处理前端请求，为了让Spring识别这个控制类，在类前加入注解@RestController 加入这些注解的目的是为了标记类为一个Spirng的Bean，Spring Boot帮我们配置好了扫描器，其会在启动时扫描全部带有@Component注解的类，将其注册为 Spring Bean。只有这样，Spring才会在有网络请求进入时，通过RequestMapping去索引 注册过的组件类 的相应函数。 @Component根据类的功能不同，其又分为以下三个子注解类型： @Controller：控制层，标注返回前端的控制组件 @Service：业务逻辑层，标注中间层的控制组件 @Repository：DAO层，标注数据库访问的组件 而@RestController是@Controller的一个升级版，其相当于@Controller + @ResponseBody，可以支持返回json格式的数据给前端。在REST模式设计的服务上，应该使用@RestController 在控制类中，在函数前加入注解@RequestMapping(url地址)，实现路由的作用，即用 URL请求的地址 对应 控制类调用相应的函数 根据REST规范，请求的类型可以分为 GET/POST/PUT/DELETE，因此可以直接使用以下子注解： @GetMapping(url) @PostMapping(url) @PutMapping(url) @DeleteMapping(url) // EmployeeController.javaimport java.util.List;// 导入RequestMapping使用的命名空间import org.springframework.web.bind.annotation.DeleteMapping;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.PutMapping;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RestController;@RestControllerclass EmployeeController // 引用接口服务，用来实现各种功能，记得要使用final静态变量 // 比如这里使用定义的JpaRepository接口对象，用来操作数据库 private final EmployeeRepository repository; EmployeeController(EmployeeRepository repository) this.repository = repository; // 如果用户请求GET，地址为/employees，调用该函数 @GetMapping(/employees) ListEmployee all() return repository.findAll(); // 如果用户请求GET，地址为/employees，调用该函数 @PostMapping(/employees) // @RequestBody 用于读取发来的请求中包含的参数数据，注意保持类型一致 Employee newEmployee(@RequestBody Employee newEmployee) return repository.save(newEmployee); @GetMapping(/employees/id) // @PathVariable 用于读取url地址中包含的参数数据，用包起参数名 Employee one(@PathVariable Long id) return repository.findById(id).orElseThrow(() - new EmployeeNotFoundException(id)); // 如果用户请求PUT（修改），地址为/employees/id，调用该函数 @PutMapping(/employees/id) Employee replaceEmployee(@RequestBody Employee newEmployee, @PathVariable Long id) return repository.findById(id) .map(employee - employee.setName(newEmployee.getName()); employee.setRole(newEmployee.getRole()); return repository.save(employee); ) .orElseGet(() - newEmployee.setId(id); return repository.save(newEmployee); ); // 如果用户请求DELETE（删除），地址为/employees/id，调用该函数 /employees/id @DeleteMapping(/employees/id) void deleteEmployee(@PathVariable Long id) repository.deleteById(id); 异常处理 在我们的后端服务中，一旦出现异常，光进行Java本身的异常处理是不够的，因为这样前端收不到对应的异常消息，没有办法知道接下来要干什么（前端：你这不是摆烂吗这是），因此Spring还提供ExceptionHandler反射机制，当后端产生异常时返回错误信息到前端。 主要步骤 先创建自己的异常类，继承RuntimeException，定义一种异常，如： XXXXException.java // EmployeeNotFoundException.javaclass EmployeeNotFoundException extends RuntimeException EmployeeNotFoundException(Long id) super(Could not find employee + id); 注意，这个异常类只抛出后端的错误信息，我们还需要让Spring返回前端信息 所以，在该类下再创建一个 XXXXAdvice.java // EmployeeNotFoundAdvice.java// 导入命名空间import org.springframework.http.HttpStatus;import org.springframework.web.bind.annotation.ControllerAdvice;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.bind.annotation.ResponseStatus;@ControllerAdviceclass EmployeeNotFoundAdvice @ResponseBody // 反射自定义异常类 @ExceptionHandler(EmployeeNotFoundException.class) // 注解，说明返回时向前端回复错误状态码（HttpStatus.NOT_FOUND 即 404） @ResponseStatus(HttpStatus.NOT_FOUND) String employeeNotFoundHandler(EmployeeNotFoundException ex) // 返回的错误信息 return ex.getMessage(); 定义XXXXAdvice类时，使用@ControllerAdvice注解，并且在其处理函数（通常命名为XXXXHandler）前，使用@ResponseBody、@ExceptionHandler、@ResponseStatus，它们的作用分别是： 注解 作用 @ResponseBody 说明返回给前端的消息采用json格式 @ExceptionHandler 说明这是一个处理异常的消息（参数填写对应异常类的反射） @ResponseStatus 说明返回的HTTP状态码（可以使用HttpStatus枚举） DTO（Data Transfer Object）支持 添加Spring HATEOAS 在pom.xml中加入以下依赖部分代码即可 dependency\tgroupIdorg.springframework.boot/groupId\tartifactIdspring-boot-starter-hateoas/artifactId/dependency HATEOAS是Spring中用来实现链接（Link）的工具，链接的存在使得客户端可以动态发现其所能执行的动作。 链接的作用 让我们来看一个例子。这是一个普通的json返回包： id: 1, name: Bilbo Baggins, role: burglar, 这是一个带Links的json返回包： id: 1, name: Bilbo Baggins, role: burglar, _links: self: href: http://localhost:8080/employees/1 , employees: href: http://localhost:8080/employees 返回的链接 “_links”部分将告诉前端，用户当前的操作（或是所在的位置）可以跳转到哪些可能的地址，因此前端可以根据这些链接为用户提供更方便的操作。 返回带链接的数据包 注意，以下代码可以简化，写这些的目的是为了更好的理解链接，如果要速成，请跳转到简化生成链接的步骤一节 以下是一个向前端返回带链接的数据的例子： @GetMapping(/employees/id)EntityModelEmployee one(@PathVariable Long id) Employee employee = repository.findById(id).orElseThrow( () - new EmployeeNotFoundException(id)); return EntityModel.of(employee, linkTo(methodOn(EmployeeController.class).one(id)).withSelfRel(), linkTo(methodOn(EmployeeController.class).all()).withRel(employees)); 这个Mapping函数和之前的相比，有下面这些不同的地方： 返回值类型由自定义的模型类Employee变为了EntityModelEmployee。 泛型EntityModelT是Spring HATEOAS设计的一种容器，其中不仅能存储数据，还可以存储一系列链接。 当你返回值时，不再只返回一个 Employee employee的模型对象，而是使用 EntityModel.of(employee,links) 返回一个EntityModel类型（包括employee对象和若干个链接） 使用linkTo()方法生成链接： 参数传入一个methodOn()函数，参数为你要链接的方法所在的类的反射，一般为控制类，即XXXXController.class，然后用.点出要连接到的类的方法，如： linkTo(methodOn(EmployeeController.class).one(id)).withSelfRel() 即会加入EmployeeController.one(id)方法所在Mapping的URL，即 localhost:8080/employees/id 可以理解为这是一种反向Mapping，即从调用的方法查找到对应的路由 linkTo()返回一个Link对象，可以用其.withSelfRel()和.withRel(Sting)来修饰其在返回包中的Rel标签 Rel 是 relation 的简写，用来说明链接自身和链接之间的关系 .withSelfRel()返回的链接的Rel标签为“self”：，这说明返回的是请求的链接本身。一般来说带链接的返回包都要返回自己，其目的大概和类里面为啥要有this这个对象一样。 .withRel(Sting)返回的链接带有一个用String参数修饰的标签，即“xxxx”：，这个标签有助于前端根据标签的名称所以链接。 Spring HATEOAS的各种容器（Model） EntityModel 用来容纳单个模型对象及其链接 CollectionModel用来容纳多个模型对象及其链接，一个Collection可以包含多个Entity，写为：CollectionModelEntityModelT 将多个实体模型打包成Collection时，最好将每个模型对象都包装好链接，再为整体的Collection（可以是List、Map或是Dict之类）包装链接，示例如下： @GetMapping(/employees)CollectionModelEntityModelEmployee all() // 这一行写了很多东西，但主要的步骤是：// find所有employee的对象 - 转换为java8流对象（stream）- 用map函数把每个// employee的对象换为EntityModel对象，打包链接 - 将所有单体对象用collect函数// 重新打包进列表\tListEntityModelEmployee employees = repository.findAll().stream() .map(employee - EntityModel.of(employee, linkTo(methodOn(EmployeeController.class).one(employee.getId())).withSelfRel(), linkTo(methodOn(EmployeeController.class).all()).withRel(employees))) .collect(Collectors.toList());//返回CollectionModel对象，打包整体的链接 return CollectionModel.of(employees, linkTo(methodOn(EmployeeController.class).all()).withSelfRel()); ​\t该代码最后返回的json包格式如下： _embedded: employeeList: [ id: 1, name: Bilbo Baggins, role: burglar, _links: self: href: http://localhost:8080/employees/1 , employees: href: http://localhost:8080/employees , id: 2, name: Frodo Baggins, role: thief, _links: self: href: http://localhost:8080/employees/2 , employees: href: http://localhost:8080/employees ] , _links: self: href: http://localhost:8080/employees 这就是一个非常符合REST规范的资源包了。 PagedModel也用来容纳多个模型对象及其链接，并且支持分页，生成PageModel必须使用Page类型的泛型对象，在生成PageModel时提供一个Pageable 的分页对象，用来记录分页信息： // PageModel.of()的函数重载类型：// 不传入Link参数，所有对象自动返回空Linkpublic static T PagedModelT model = PageModel.of (CollectionT content, @Nullable PagedModel.PageMetadata metadata);// 传入一组Link参数，每个Link与content一一对应public static T PagedModelT model = PageModel.of (CollectionT content, @Nullable PagedModel.PageMetadata metadata, Link... links); // 传入一个Link迭代对象，每个Link与content一一对应public static T PagedModelT model = PageModel.of (CollectionT content, @Nullable PagedModel.PageMetadata metadata, IterableLink); 带有分页信息的返回包格式如下，可以看到除了CollectionModel的信息外，还另外增加一个”Page“字段，存储分页信息： _embedded: userDtoList: [ email: admin@frogsoft.com, username: admin, roles: [ ROLE_USER, ROLE_ADMIN ], _links: self: href: http://127.0.0.1:8080/v1/users/admin , allUsers: href: http://127.0.0.1:8080/v1/users/?page=0size=10 ] , _links: self: href: http://127.0.0.1:8080/v1/users?page=0size=10 , page: size: 10, totalElements: 1, totalPages: 1, number: 0 可以使用Spring提供的PagedResourcesAssembler类，简化PagedModel的生成， 在阅读以下代码之前，先阅读下一节关于RepresentationModelAssembler接口的介绍。 // 使用单个EntityModel的RepresentationModelAssembler接口生成PageModel的函数。public PagedModelEntityModelT getPageModelT(Pageable pageable) // 获取实体类型T对应的PagedResourcesAssembler类型 private final PagedResourcesAssemblerT pagedResourcesAssembler; // 获取实体类型T的RepresentationModelAssembler（自行实现） private final XXXXModelAssembler tModelAssembler; // 将多个实体类型T打包为PageT类型（可以使用前面提到的JpaRepository中的findAllBy()实现） PageT contents = /* find the contents and packed in Page*/ // 调用.toModel返回分页模型，参数分别为打包的PageT类型 和 单个T类型对应的RepresentationModelAssembler对象 return pagedResourcesAssembler.toModel(contents, tModelAssembler); 简化生成链接的步骤 为了减少代码复用，不要在每次创建link时都重复写代码，我们可以写一个函数，将模型对象（比如 Employee）转换为对应的EntityModelT对象。这样当我们需要创建link时，只需简单的调用这个方法。 幸运的是，Spring HATEOAS帮你提供了RepresentationModelAssembler接口，通过这个接口，你可以快速创建转换类，而不用自己搭建框架。 实现RepresentationModelAssembler接口的方法如下： //EmployeeModelAssembler.javaimport static org.springframework.hateoas.server.mvc.WebMvcLinkBuilder.*;import org.springframework.hateoas.EntityModel;import org.springframework.hateoas.server.RepresentationModelAssembler;import org.springframework.stereotype.Component;@Componentclass EmployeeModelAssembler implements RepresentationModelAssemblerEmployee, EntityModelEmployee @Override public EntityModelEmployee toModel(Employee employee) return EntityModel.of(employee, linkTo(methodOn(EmployeeController.class).one(employee.getId())).withSelfRel(), linkTo(methodOn(EmployeeController.class).all()).withRel(employees)); 为你的接口实现类起一个 XXXModelAssembler的名字；加入Spring的注入注解@Component；实现接口的泛型参数Employee, EntityModelEmployee表明其是将模型对象XXX转换为EntityModelXXX 该接口只需要实现（重载）一个toModel函数，其内容和之前直接在控制器类的函数中创建链接的代码一模一样。 使用该接口对象的方法和repository接口对象的方法类似，只需在控制器中声明其静态引用： private final EmployeeModelAssembler assembler; 别忘了在有参构造函数中也为其添加参数，然后只要在返回函数中调用toModel函数即可： @GetMapping(/employees/id)EntityModelEmployee one(@PathVariable Long id) ... return assembler.toModel(employee); 之前打包Collection的代码也可以简化为： @GetMapping(/employees)CollectionModelEntityModelEmployee all() ListEntityModelEmployee employees = repository.findAll().stream() .map(assembler::toModel) .collect(Collectors.toList()); return CollectionModel.of(employees, linkTo(methodOn(EmployeeController.class).all()).withSelfRel()); 代码简洁了很多呢！ 使用ResponseEntity返回数据 另一个让你的代码符合REST规范的要点是永远返回合适的Respone 之前返回的json包中的HTTP状态码需要我们自行写入和处理，ResponseEntity则帮我们解决了这个问题，同时支持多种格式的返回包数据。 更新上面的POST函数，以便使用ResponseEntity类来处理返回值： @PostMapping(/employees)ResponseEntity? newEmployee(@RequestBody Employee newEmployee) EntityModelEmployee entityModel = assembler.toModel(repository.save(newEmployee)); return ResponseEntity // .created(entityModel.getRequiredLink(IanaLinkRelations.SELF).toUri()) // .body(entityModel); 和不使用ResponseEntity相比，返回值类型变为ResponseEntityEntityModelEmployee，也就是说，要在EntityModel外层，再包装一层ResponseEntity类型的泛型。在不至于混淆的情况下，可以使用ResponseEntity? 自动推导类型。 使用ResponseEntity.created()会返回HTTP 201 状态码，表示创建Employee的操作成功。当返回这个内容时，前端通常希望能同时获得该操作所对应的URL地址（Location），以便进行下一步操作。由于我们已经在返回包里打包了Link，因此添加响应头非常方便，代码中.created()的参数：entityModel.getRequiredLink(IanaLinkRelations.SELF).toUri()即是将Link的Self部分写入返回包的头部，最后得到的返回数据如下： Location: http://localhost:8080/employees/3 Content-Type: application/hal+json;charset=UTF-8 Transfer-Encoding: chunked Date: Fri, 10 Aug 2018 19:44:43 GMT id: 3, firstName: Samwise, lastName: Gamgee, role: gardener, name: Samwise Gamgee, _links: self: href: http://localhost:8080/employees/3 , employees: href: http://localhost:8080/employees ResponseEntity.created()只返回一个特定的状态（201）， 通常情况下，我们使用ResponseEntity.status()返回一个自定义状态码的数据包，状态码可以用HttpStatus枚举表示。 HTTP状态码 HttpStatus枚举 含义 100 HttpStatus.CONTINUE 继续 102 HttpStatus.PROCESSING 处理中 200 HttpStatus.OK 正常 201 HttpStatus.CREATED 已创建 202 HttpStatus.ACCEPTED 已接受 204 HttpStatus.NO_CONTENT 已无内容 302 HttpStatus.FOUND 已找到 400 HttpStatus.BAD_REQUEST 请求错误 401 HttpStatus.UNAUTHORIZED 未授权 403 HttpStatus.FORBIDDEN 禁止访问 404 HttpStatus.NOT_FOUND 无法找到 408 HttpStatus.REQUEST_TIMEOUT 请求超时 429 HttpStatus.TOO_MANY_REQUESTS 请求过多 500 HttpStatus.INTERNAL_SERVER_ERROR 内部服务错误 502 HttpStatus.BAD_GATEWAY 网关错误 503 HttpStatus.SERVICE_UNAVAILABLE 服务不可用 504 HttpStatus.GATEWAY_TIMEOUT 网关超时 ResponseEntity.status(/* http状态码 */HttpStatus.XXX).body(model); 其他一些常用的状态，ResponseEntity也为我们封装好了以下常用函数： // ok：返回状态码200ResponseEntity.ok().body(model);// badRequest：返回状态码400ResponseEntity.badRequest().body(model);// notFound：返回状态码404ResponseEntity.notFound().body(model); Controller、Service、Repository 三层结构 之前我们提到，Spring用来注册Bean的控制接口@Component根据名称不同又分为三种： @Controller：控制层，标注返回前端的控制组件 @Service：业务逻辑层，标注中间层的控制组件 @Repository：DAO层，标注数据库访问的组件 之所以这么设计，是由编写代码中分层规范决定的，通常Controller层最接近前端，Repository(DAO)层最接近后端（数据库），这样的结构最利于代码解耦化，也利于每个部分的代码更加简洁，接下来我们就来整理项目以适应三层结构。 @Controller控制层 在项目结构中建立一个Controller包，用来处理控制层。 控制层是最接近前端的部分，所以要适应可能随时改变的前端需求。因此最好做好版本控制，可以将Controller包命名为/Controller.v版本号。当需要修改需求时，可以保留旧版本，重新建立新版本的Controller包，以实现向后兼容。 控制层要实现的内容： 根据设计好的API，建立若干控制类：XXXXController，放置于/Controller/api包下。 若API需要接受前端请求的数据比较复杂，需要单独用一个数据类存储，可以建立对应的数据类XXXXRequest，放置于/Controller/request包下。 控制类中要做的事情： 路由处理：用@RequestMapping索引（这个模块的）根地址（可以写在class的定义前），然后用@Get/Post/Put/DeleteMapping索引子地址 权限处理：判断访问请求是否具有特殊的权限： token鉴权：从请求中获取username，与授权系统中的列表匹配 Roles判断：函数前加入注解@RolesAllowed(身份名)，自动拒绝没有对应身份权限的用户的访问，判断用户的身份在登陆时由鉴权系统实现 处理请求数据： 读请求数据：函数参数中配置@RequestParam和@PathVariable，获得请求或URL中的数据 读写数据：调用Service层的接口对象 注意，对于数据的具体处理要在Service层实现，Controller层只简单处理和传输参数，并读取结果（Entity对象） 返回状态码，用ResponseEntity.status()实现 @Service服务层 服务层，即作为一个承上启下的中间部分，用来处理数据、打包数据。 服务层的基本结构： 建立若干服务接口，在/service文件夹下，如果功能细分需要多个功能模块，也可以建立子文件夹 服务接口，以XXXXService命名，接口的实现类以XXXXServiceImpl命名，使用接口的目的是为了多个类访问同一个数据仓库时方便重载和复用。 服务实现类中要做的事情： 读写数据：调用Repository层的接口对象，处理数据 读数据：使用findBy系列方法（在Repository层定义） 写数据：使用save方法 打包数据：将要返回的数据通过RepresentationModelAssembler接口对象（这也属于持久层）打包为Entity对象，返回给控制层 @Repository持久层 持久层，用来和数据库进行直接连接。在这里我们使用JPA来建立持久层，数据库连接变得十分方便，但要满足DTO数据模型的要求，因此持久层分为Repository和Dto两个平行层 持久层的基本结构： 建立若干仓库接口，以XXXXRepository命名，接口要继承自JpaRepository，以实现数据库连接，接口无需实现（Spring金牌服务，帮您实现！） 建立Dto文件夹，其包含三个部分： model文件夹：建立Dto数据类，Dto数据类的命名在基本数据实体类后加Dto即可（如UserDto） Dto数据类的成员变量内容和基本数据类一模一样，但无需写与数据库连接的注解，数据的连通由mapper类搞定 mapper文件夹：对于每个Dto类，建立一个Mapper类，以XXXXMapper命名。 mapper类实现一个函数，将基本数据实体类（如User）转化为Dto数据类（如UserDto） mapper函数的实现方法：new一个Dto对象，然后将使用setter函数配置所有成员即可。之所以在mapper中new对象，也是处于方便解耦和类嵌套之类的目的 assemler文件夹，对于每个Dto类，建立一个Assembler类，以XXXXModelAssembler命名。该类实现RepresentationModelAssembler接口。 注意，这里实现RepresentationModelAssembler接口时，传入的实体对象要是已经打包好的Dto类对象，不然我费这么大劲打包Dto干啥（感觉在说废话） Assembler类即主要实现toModel函数，实现将Dto数据打包为Entity实体对象，这样，服务层只需调用对应的接口实现类，即可以获得符合REST规范的返回数据包 对于基本数据实体类，建立一个\\Model文件夹存储，使用@Entity实现类与数据库的ORM连接。这些即为持久层所需处理的部分。","tags":["java","Spring"]},{"title":"友链","path":"/friends/index.html","content":"See also my friends! 版本更新，施工中……"},{"title":"笔记分类页","path":"/notes/index.html","content":"正在施工中…"},{"title":"关于","path":"/about/index.html","content":"About ME 关于我： AI Robotics LLM相关方向 EHD在读 写过小说 第二名完美主义者 独立游戏爱好者 混剪爱好者 （非常业余的）铁道迷 好吧，其实我只是喜欢线路图和标志设计 音游人 主持过两次COC跑团 已经发表的论文 …… …… …… 什么？！居然一篇论文也没有？！！ 天哪我怎么这么菜 一些个人主页 Github：#Nick-bit233 bilibili：@赛因斯没有坦 Steam：@Nick_bit 做过的一些有意思的东西 音游成绩 我正在研究如何实时的更新这个东西，如果你有建议，欢迎写在评论区~ Game Ranking Last Update Date Arcaea 12.01 24/12/29 Lanota 16.05 24/12/29 Chunithm（CN） 15.37 24/12/29 maimaiDX（CN） 14100 24/12/29 Phigros 15.01 24/02/12 osu! Standard 803pp 22/08 有用的网站收藏夹 实用工具 寻找资源 不正经的部分"}]