[{"title":"【笔记】生成模型基础与应用 - 第3章","path":"/2023/11/27/生成模型基础与应用笔记-第3章/","content":"C3 主题模型：主题模型是一类用于文本分析的非监督学习方法，旨在从文本数据中发现隐藏的主题结构… C3 主题模型 Topic models：以非监督学习的方式对文本的隐含结构进行发现或生成的模型 主题模型是一类用于文本分析的非监督学习方法，旨在从文本数据中发现隐藏的主题结构。这些主题模型的目标是识别文本集合中的主题，而无需事先标记的主题标签或监督信息。主题模型最常见的应用之一是用于文本数据的主题建模，其中文档被看作是多个主题的混合，而每个主题又由一组词汇表示。 主题模型的发展： LSA PLSA LDA 2003 HDP 2005 单词向量空间 将文档中每个单词的出现的频数（或加权的频数）表示为向量 可以事先定义 有效单词的 语料库，只提取文档中有效单词的出现频率生成向量（也可以视为不在语料库中的单词其权值为0），所有的语料库中的有效单词用集合W表示 一个文档就表示成：d=(fw1,fw2,....,fwN)d=(f_{w_1},f_{w_2},....,f_{w_N})d=(fw1​​,fw2​​,....,fwN​​)，每个fif_ifi​都是有效单词iii出现的次数 一个文档集中的所有文档（设一共有N个文档组成了集合D）组成了一个向量集合，即 单词向量空间 对于两个不同的文档di, djd_i,\\ d_jdi​, dj​，可以使用它们之间的数学度量，来表示文本之间的语义相似度： 计算方法可以为 文档向量的 内积 或 标准化内积 单词向量空间的表示方法： 1、单词-文本矩阵：将单词在每个文档的出现频率向量作为列向量，组成的矩阵。通常为稀疏矩阵 单词-文本矩阵的例子： - 计算机处理稀疏矩阵，非常浪费算力，因此，需要一个等价的数据结构来表示相同给信息，于是提出了下面这种表示方法。 2、单词频率-逆文本频率（TF-IDF）： 统计单词wiw_iwi​在文本dkd_kdk​中出现的权值 ： TF−IDF(wi,dk)=f(wi,dk)len(dk)logNdf(wi)TF-IDF(w_i,d_k) = \\frac{f(w_i,d_k)}{len(d_k)}log\\frac{N}{df(w_i)}TF−IDF(wi​,dk​)=len(dk​)f(wi​,dk​)​logdf(wi​)N​ - tf(w,b)：单词w在文本b中的出现频数 - len(b)：文本b中的单词总数 - df(w)：整个文本集合中，含有单词w的文本数 - N：整个文本集合的大小（含有的文本数量） 单词向量空间的优缺点优点：模型简单、计算效率高缺点：内积计算的相似度不一定能准确表达两个文档间的相似度 话题向量空间 定义话题：假设所有的文本中一共含有K个话题 假设每个话题都是一个M维度的向量ttt，ttt由语料库（集合W）中的M个单词组成。 t=(w1,w2,...,wM)Tt = (w_1,w_2,...,w_M)^T t=(w1​,w2​,...,wM​)T 这样，一共K个话题就组成了一个话题向量空间，记为T=[t1,t2,...,tk]=[w11 w12... wMK]T = [t_1,t_2,...,t_k]=[w_{11}\\ w_{12} ... \\ w_{MK}]T=[t1​,t2​,...,tk​]=[w11​ w12​... wMK​]【单词-话题矩阵】 根据单词向量空间的定义，可以推导出： 假设一篇文本xxx在单词向量空间中被表示为：x=(fw1,fw2,....,fwN)x = (f_{w_1},f_{w_2},....,f_{w_N})x=(fw1​​,fw2​​,....,fwN​​)，而在话题空间中被表示为：$$y = g(x) = (f_{t_1},f_{t_2},…,f_{t_k})$$ 为了方便，我们将话题的出现频率 记为ft1=y1,ft2=y2...f_{t_1} = y_1, f_{t_2} = y_2...ft1​​=y1​,ft2​​=y2​...这样，一个文档x，它可以表示为所有话题和单词的线性组合： x=t1y1+t2y2+...tkykx = t_1y_1 + t_2y_2 + ... t_ky_k x=t1​y1​+t2​y2​+...tk​yk​ 如果我们定义话题-文本矩阵Y，即将每个话题表示的文本yyy组合起来，则由上述的线性组合关系，对整个文档集合D，即可由话题向量空间的 话题矩阵T 来表示。 因此，单词-文本矩阵、单词-话题矩阵和话题-文本矩阵具有如下的关系： XMN=TMKYKNX_{MN} = T_{MK}Y_{KN} XMN​=TMK​YKN​ 潜在语义分析即是 将文本在 单词向量空间的表示 通过 线性变换 转换为** 在话题向量空间中的表示**的方法 期望最大化算法 在讲潜在语义分析之前，必须要了解一下什么是贝叶斯概率模型的潜变量，以及对潜变量模型的估计算法。期望最大化算法（Exception Maximization Algorithm，简称EM算法）是一种启发式的迭代算法，用于对含有隐变量的概率模型 的参数做 极大似然估计 假设我们有这样一个概率模型： 有三枚硬币A,B,CA,B,CA,B,C，抛掷它们正面朝上的概率不同，记为r,p,qr,p,qr,p,q 重复执行以下试验（Ex(X)表示对事件X做一次实验，1表示正面向上，0表示反面向上）： Ex(Y)=Ex(B) if Ex(A)==1 else Ex(C)Ex(Y)= Ex(B) \\ if \\ Ex(A)==1 \\ else \\ Ex(C) Ex(Y)=Ex(B) if Ex(A)==1 else Ex(C) 即抛硬币A，如果是正面抛硬币B，否则抛硬币C，你只能观测到最后一次抛掷硬币的结果。 那么如何估计三个参数r,p,qr,p,qr,p,q呢？ 因为这个随机事件包含两个概率模型的 复合。因此我们无法直接对结果建模，数学家们由此提出，能否加入“隐变量”ZZZ： 用它来表示试验中抛掷A硬币的中间结果（虽然我们并不能直接观测到），假设共抛了n次，第i次观测结果的值记作ziz_izi​ -假设我们只看最终结果Y，将参数表示为向量θ=(r,p,q)\\theta = (r,p,q)θ=(r,p,q)，则实际要观测的事件Y的似然可以表示为： P(Y∣θ)=P(Y,Z∣θ)=∑i=1nP(yi,zi∣θ)=∑i=1nP(zi∣θ)P(yi∣zi,θ)=rp∑(yi=1)(1−p)∑(yi=0)+(1−r)q∑(yi=1)(1−q)∑(yi=0)P(Y|\\theta) = P(Y,Z|\\theta)= \\sum_{i=1}^{n}P(y_i,z_i|\\theta) = \\sum_{i=1}^{n}P(z_i|\\theta)P(y_i|z_i,\\theta) \\\\= rp^{\\sum(y_i=1)}(1-p)^{\\sum(y_i=0)}+(1-r)q^{\\sum(y_i=1)}(1-q)^{\\sum(y_i=0)} P(Y∣θ)=P(Y,Z∣θ)=i=1∑n​P(yi​,zi​∣θ)=i=1∑n​P(zi​∣θ)P(yi​∣zi​,θ)=rp∑(yi​=1)(1−p)∑(yi​=0)+(1−r)q∑(yi​=1)(1−q)∑(yi​=0) 直接使用MLE估计这个似然函数？不行，包含三个未知变量，且需要求log和的极大值，算不出解析解。 既然没有准确的解析解，数学家于是提出了用迭代法逼近最大值的求解办法，这就是EM算法。我们先定义好两种情况的似然函数，既然要求极大似然，我们对似然函数直接取log，定义对数似然函数： 不将隐变量Z视为随机变量的情况【称为：不完全数据】 LY(θ)=log[∏i=1nP(yi∣θ)]=∑i=1nlog[∑zi∈ Ex(Z)P(yi,zi∣θ)]L_Y(\\theta) = log[\\prod_{i=1}^nP(y_i|\\theta)] = \\sum_{i=1}^nlog[\\sum_{z_i \\in \\ Ex(Z)}P(y_i,z_i|\\theta)] LY​(θ)=log[i=1∏n​P(yi​∣θ)]=i=1∑n​log[zi​∈ Ex(Z)∑​P(yi​,zi​∣θ)] 将隐变量Z视为随机变量的情况【称为：完全数据】 LY,Z(θ)=log[∏i=1nP(yi,zi∣θ)]=∑i=1nlog[P(yi,zi∣θ)]L_{Y,Z}(\\theta) = log[\\prod_{i=1}^nP(y_i,z_i|\\theta)] = \\sum_{i=1}^nlog[P(y_i,z_i|\\theta)] LY,Z​(θ)=log[i=1∏n​P(yi​,zi​∣θ)]=i=1∑n​log[P(yi​,zi​∣θ)] 我们假设θ\\thetaθ从一个初始值θ0\\theta_0θ0​开始，每次迭代的时候更新（下标+1），那么我们希望 在已知观测结果Y的情况下，让完全数据的对数似然函数LY,Z(θ)L_{Y,Z}(\\theta)LY,Z​(θ)尽可能取得最大的期望，由此定义Q函数： Q(θ,θt)=E[LY,Z(θ)∣Y,θt]=∑zi∈ Ex(Z)P(Z∣Y,θt)log[P(Y,Z∣θ)]Q(\\theta,\\theta_t) = E[L_{Y,Z}(\\theta)|Y,\\theta_t] = \\sum_{z_i \\in \\ Ex(Z)}{P(Z|Y,\\theta_t)log[P(Y,Z|\\theta)]} Q(θ,θt​)=E[LY,Z​(θ)∣Y,θt​]=zi​∈ Ex(Z)∑​P(Z∣Y,θt​)log[P(Y,Z∣θ)] 其中，θt\\theta_tθt​是第t步更新的参数，Q是关于θ\\thetaθ的函数，让它最大，则需要对θ\\thetaθ求偏导，并取导数为0。此时θt\\theta_tθt​取一个新值，即： \\theta_{t+1} = argmax_\\theta{Q(\\theta,\\theta_t)} $$【MLE】 这是采用极大似然估计的方法，如果你说要考虑先验概率（即MAP方法），那么每次更新是需要加上先验函数的对数值： \\theta_{t+1} = argmax_\\theta{Q(\\theta,\\theta_t)} + logP(\\theta) 【MAP】 &gt; 现在我们试着对刚才的问题应用EM算法： &gt; - 先假设$\\theta^{(0)} = (0.5,0.5,0.5)$（为了下标不打架，我们用上括号标记步数t） &gt; - 则$P(Z|Y,\\theta_t)$表示： &gt; - 假设第$i$次的最终**观测结果是**$y_i(=1 \\ or\\ 0)$**，这个结果是由 抛硬币B**$(z_i=1)$** 还是 抛硬币C**$(z_i=0)$** 得到的概率，将其记为**$p_i$**，**我们可用直接以伯努利实验的结果计算： &gt; $p_i = \\frac{P(y_i,z_i=1)}{P(y_i,z_i=0)+P(y_i,z_i=1)}=\\frac{rp}{rp+(1-r)q}(y_i=1) \\ or \\ \\frac{r(1-p)}{r(1-p)+(1-r)(1-q)}(y_i=0)$**【对于每一步i】** &gt; - **然后求其关于Z的期望，即：** &gt; $$Q(\\theta) = \\sum_{i=1}^{n}p_i\\log[P(y_i=1,Z|\\theta)] + (1-p_i)\\log[P(y_i=0,Z|\\theta)] 注意：pip_ipi​与每一步要更新的θt\\theta_tθt​有关，因此其表示为的r,p,qr,p,qr,p,q**也需要每一步更新。**而log[P(Y,Z∣θ)]log[P(Y,Z|\\theta)]log[P(Y,Z∣θ)]则直接表示为上述的似然函数，注意它与要逼近的θt\\theta_tθt​无关，可以直接表示为r,p,qr,p,qr,p,q的式子，故： Q(θ,θt)=∑i=1npi(t−1)log⁡[rpyi(1−p)1−yi]+(1−pi(t−1))log⁡[(1−r)qyi(1−q)1−yi]Q(\\theta,\\theta_t)=\\sum_{i=1}^{n}p_i^{(t-1)}\\log[rp^{y_i}(1-p)^{1-y_i}] + (1-p_i^{(t-1)})\\log[(1-r)q^{y_i}(1-q)^{1-y_i}] Q(θ,θt​)=i=1∑n​pi(t−1)​log[rpyi​(1−p)1−yi​]+(1−pi(t−1)​)log[(1−r)qyi​(1−q)1−yi​] 对该函数的r,p,qr,p,qr,p,q分别求偏导，即可得到： r(t+1)=∑i=1npi(t)nr^{(t+1)} = \\frac{\\sum_{i=1}^{n}p_i^{(t)}}{n} r(t+1)=n∑i=1n​pi(t)​​ p(t+1)=∑i=1npi(t)yi∑i=1npi(t)p^{(t+1)} = \\frac{\\sum_{i=1}^{n}p_i^{(t)}y_i}{\\sum_{i=1}^{n}p_i^{(t)}} p(t+1)=∑i=1n​pi(t)​∑i=1n​pi(t)​yi​​ q(t+1)=∑i=1n(1−pi(t))yi∑i=1n(1−pi(t))q^{(t+1)} = \\frac{\\sum_{i=1}^{n}(1-p_i^{(t)})y_i}{\\sum_{i=1}^{n}(1-p_i^{(t)})} q(t+1)=∑i=1n​(1−pi(t)​)∑i=1n​(1−pi(t)​)yi​​ 问题： 为什么EM算法能近似实现对观测数据 的极大似然估计？ 答：可以通过数学证明：可以通过L(θ)L(\\theta)L(θ)函数构造Q函数，当作为中间函数时，可以证明在迭代过程中极大对数似然函数L(θ)L(\\theta)L(θ)的下界单调增加，即：L(θt+1)≥L(θt)L(\\theta_{t+1}) \\geq L(\\theta_t)L(θt+1​)≥L(θt​) 证明就省略啦~ 概率潜在语义分析（PLSA） 是一种利用概率生成模型 对 文本集合进行话题分析的 无监督学习方法 假设： 每个文本由一个话题的分布决定 每个话题由一个单词的分布决定 因此，从给定文本的表示到生成单词，就是一个概率模型，其中话题是隐变量 举个例子：假设一个离散空间中，【话题】和其对应的单词的概率分布如下： 【教育】 = {大学：0.5，老师：0.3，课程：0.2 } 【经济】 = {市场：0.4，企业：0.2，金融：0.4} 【交通】 = {高铁：0.5，汽车：0.2，飞机：0.3} 而对于一个挖了空位的文本，其每个位置对应话题的概率分布如下： D(xx场景下如何看待xx和xx的关系) = {教育：0.5，经济：0.3，交通：0.2} 那么，生成 “大学场景下如何看待大学与企业的关系”的概率 是： [P(t=教育)P(w=大学∣t=教育)]2×P(t=经济)P(w=企业∣t=经济)=0.00375[P( t = 教育 )P(w = 大学|t = 教育) ]^2× \\\\P(t = 经济) P(w = 企业 |t = 经济 ) =0.00375[P(t=教育)P(w=大学∣t=教育)]2×P(t=经济)P(w=企业∣t=经济)=0.00375 假设文本的集合是D={d1,d2,...,dN}D=\\{d_1,d_2,...,d_N\\}D={d1​,d2​,...,dN​}单词的集合是：W={w1,w2,...,wM}W=\\{w_1,w_2,...,w_M\\}W={w1​,w2​,...,wM​}话题的集合是：Z={z1,z2,...,zK}Z=\\{z_1,z_2,...,z_K\\}Z={z1​,z2​,...,zK​}我们可以看到，在求文本的生成分布时，涉及到以下的条件概率：P(z∣d)P(z|d)P(z∣d): 已知文本d，生成话题z的概率，是一个多项分布P(w∣z)P(w|z)P(w∣z): 已知话题z，生成单词w的概率，也是一个多项分布而P(d)P(d)P(d)表示从所有文本集合中，随机选取一个文本的d的概率 现在我们希望知道，给定(单词，文本)对，它在这个空间中的生成的概率，即： P(X)=∏(w,d) ∈ W×DP(w,d)cnt(w,d)P(X) = \\prod_{(w,d)\\ \\in \\ W×D} P(w,d)^{cnt(w,d)} P(X)=(w,d) ∈ W×D∏​P(w,d)cnt(w,d) ，其中所有的(w,d)(w,d)(w,d)对应该有N×L（文本数×每个文本中要填空的单词数）个根据上面例子的思想，对于每个(w,d)(w,d)(w,d)对，其生成概率又可以用隐变量写为： P(w,d)=P(d)P(w∣d)=P(d)∑z ∈ ZP(w,z∣d)=P(d)∑z ∈ ZP(z∣d)P(w∣z)P(w,d) = P(d)P(w|d) = P(d) \\sum_{z\\ \\in \\ Z}P(w,z|d) = P(d) \\sum_{z \\ \\in \\ Z}P(z|d)P(w|z) P(w,d)=P(d)P(w∣d)=P(d)z ∈ Z∑​P(w,z∣d)=P(d)z ∈ Z∑​P(z∣d)P(w∣z) 于是我们可以建立模型，对P(z∣d)P(z|d)P(z∣d)和P(w∣z)P(w|z)P(w∣z)分别进行估计，得到了这样的单层隐变量网络，即PLSA：其中左侧的参数量是 NK，右侧为MK 现实中K远小于M，所以PLSA通过话题 对数据进行了更简洁地表示，减少了学习过程中过拟合的可能性 继续使用极大似然估计的方法，试图求出让P(X)最大时的参数，取对数似然（下面的公式推导中，我们省略了长度是O(NK+MK)O(NK+MK)O(NK+MK)的参数向量θ，不然就太长了太难读了QAQ）： LW,Z,D=log∏(w,d) ∈ W×DP(w,d)cnt(w,d)=log∏i=1M∏j=1NP(wi,dj)cnt(wi,dj)=∑i=1M∑j=1Ncnt(wi,dj)logP(wi,dj)=∑i=1M∑j=1Ncnt(wi,dj)[logP(dj)+log∑k=1KP(wi∣zk)P(zk∣dj)]L_{W,Z,D} = log\\prod_{(w,d)\\ \\in \\ W×D} P(w,d)^{cnt(w,d)} \\\\ = log \\prod_{i=1}^M \\prod_{j=1}^NP(w_i,d_j)^{cnt(w_i,d_j)} \\\\ = \\sum_{i=1}^M\\sum_{j=1}^Ncnt(w_i,d_j)logP(w_i,d_j) \\\\ = \\sum_{i=1}^M\\sum_{j=1}^Ncnt(w_i,d_j)[logP(d_j) + log \\sum_{k=1}^KP(w_i|z_k)P(z_k|d_j)] LW,Z,D​=log(w,d) ∈ W×D∏​P(w,d)cnt(w,d)=logi=1∏M​j=1∏N​P(wi​,dj​)cnt(wi​,dj​)=i=1∑M​j=1∑N​cnt(wi​,dj​)logP(wi​,dj​)=i=1∑M​j=1∑N​cnt(wi​,dj​)[logP(dj​)+logk=1∑K​P(wi​∣zk​)P(zk​∣dj​)] 仍然是非常难计算，因此要使用上一节介绍的EM估计算法，为此我们计算Q函数。上面的似然函数LW,Z,DL_{W,Z,D}LW,Z,D​已经是一个【完全数据】，隐藏变量Z被视为与W,D并列的随机变量。因此Q函数是每个(w,d)(w,d)(w,d)对为条件下，LW,Z,DL_{W,Z,D}LW,Z,D​的期望，即： Q=P(zk∣wi,dj)[∑k=1K∑i=1M∑j=1Ncnt(wi,dj)logP(wi,dj,zk)]Q = P(z_k|w_i,d_j)[\\sum_{k=1}^K\\sum_{i=1}^M\\sum_{j=1}^Ncnt(w_i,d_j)logP(w_i,d_j,z_k)] Q=P(zk​∣wi​,dj​)[k=1∑K​i=1∑M​j=1∑N​cnt(wi​,dj​)logP(wi​,dj​,zk​)] 其中，$$P(w_i,d_j,z_k) = P(d_j)P(w_i|z_k)P(z_k|d_j)$$ 就是按照“链式法则”生成一堆单词文本的联合概率，而P(zk∣wi,dj)P(z_k|w_i,d_j)P(zk​∣wi​,dj​)则是隐藏变量Z不被视为条件的分布概率【不完全数据】，即： P(zk∣wi,dj)=P(wi∣zk)P(zk∣dj)∑k=1KP(wi∣zk)P(zk∣dj)P(z_k|w_i,d_j) = \\frac{P(w_i|z_k)P(z_k|d_j)}{\\sum_{k=1}^KP(w_i|z_k)P(z_k|d_j)} P(zk​∣wi​,dj​)=∑k=1K​P(wi​∣zk​)P(zk​∣dj​)P(wi​∣zk​)P(zk​∣dj​)​ 这样，我们可以发现，要每一步更新的参数其实是P(wi∣zk)P(w_i|z_k)P(wi​∣zk​)和P(zk∣dj)P(z_k|d_j)P(zk​∣dj​) 因为我们已经有一个数据集，P(dj)P(d_j)P(dj​)的值（即先验）可以直接由统计方法估计出来，cnt函数（即哪些对实际在数据集中同时出现了也是可以统计的已知量）。因此，EM近似的步骤为： 对于每个下标对(i,k)和(k,j)，分别求P(wi∣zk)P(w_i|z_k)P(wi​∣zk​)和P(zk∣dj)P(z_k|d_j)P(zk​∣dj​)的偏导数（第一步时为每个概率取初始值（如均匀分布），只要满足概率和为1即可） 然后将导数为0的值求出来，更新(i,k)和(k,j)对应参数向量的元素 取 k=k+1，求下一步的不完全数据，直到收敛 在用统计估计P(dj)P(d_j)P(dj​)后，局部最优解的结果经过推导是：PLSA的方法仍然胜在离散空间，计算复杂度低，可以快速迭代，但存在一个很重要的缺点：因为所学习的参数不仅依赖于单词库W，还依赖于文档数据集D，所有其可以生成其所在数据集的文档的模型，但却不能生成新文档的模型 潜在狄利克雷分布（LDA） 在对PLSA建模时，我们发现一个规律： 文本由话题的一个多项分布表示 ， 话题也由单词的一个多项分布表示 以防你忘了我们在C1中介绍的多项分布，补充一下完整定义： 这不巧了嘛，我们在C2中提到，多项分布的贝叶斯生成模型，可以用 狄利克雷分布 作为先验。因此，可以假设 话题分布 和 单词分布 的先验都是 狄利克雷分布： 对于每个文本dmd_mdm​，假设d中包含的单词组成了向量dm=wm (m=1,2,...,M)d_m = \\bold w_m \\ (m=1,2,...,M)dm​=wm​ (m=1,2,...,M),认为生成该文本话题的概率P(z∣wm)=θm∼Dir(α)P(z|\\bold w_m) = \\theta_m \\sim Dir(\\alpha)P(z∣wm​)=θm​∼Dir(α) θm\\theta_mθm​参数向量有K个值，每个值表示文本dmd_mdm​能生成对应话题z1,z2,...,zkz_1,z_2,...,z_kz1​,z2​,...,zk​的概率 因此，超参数α\\alphaα也是一个K维向量 注意，在这里我们为了对每个单词写出一个话题分布，将文本的定义改成了单词组成的向量。因此代表文本集中的文本数量的常量不再是NNN，而是MMM。用Ni(i=1,2,...,M)N_i(i=1,2,...,M)Ni​(i=1,2,...,M)来表示一个文本中的单词总数。而下面的部分中，我们假设整个单词库中的单词总数是VVV 对于每个话题zk (k=1,2,...,K)z_k\\ (k=1,2,...,K)zk​ (k=1,2,...,K)，认为生成相应单词的概率P(w∣zk)=ϕk∼Dir(β)P(w|z_k) = \\phi_k \\sim Dir(\\beta)P(w∣zk​)=ϕk​∼Dir(β) 而对于每个单词wm[n]∈wm (n=1,2,...,Nm)w_m[n] \\in \\bold w_m \\ (n=1,2,...,N_m)wm​[n]∈wm​ (n=1,2,...,Nm​)，其对应的话题和单词都链式地服从多项分布： zm[n]∼Mult(θm)z_m[n] \\sim Mult(\\theta_m)zm​[n]∼Mult(θm​)：先随机根据概率生成一个话题序列 wm[n]∼Mult(ϕzm[n])w_m[n] \\sim Mult(\\phi_{z_m[n]})wm​[n]∼Mult(ϕzm​[n]​)，再对每个话题，随机生成一个单词序列，共生成m个 在这个定义下，我们可以直接得到所有先验的表示，因此，可以使用最大后验概率估计MAP来求解参数，记住在这里w是观测量，z是隐变量，因此后验是已知观测量算隐变量的概率：文本w的后验=P(θ,z∣w,α,β)=P(w,z,θ,ϕ∣α,β)P(w∣α,β)文本w的后验=P(\\bold{\\theta}, \\bold z|\\bold w, \\alpha,\\beta) = \\frac{P(\\bold w,\\bold z,\\theta,\\phi|\\alpha,\\beta)}{P(\\bold w|\\alpha,\\beta)}文本w的后验=P(θ,z∣w,α,β)=P(w∣α,β)P(w,z,θ,ϕ∣α,β)​然后，就像PLSA方法一样，根据向量的嵌套定义把概率计算拆成每个元素概率的乘积，式子很长，直接贴一下结果吧：自然，想对这种玩意求解析最大值，和自杀没什么区别。不过，既然这次我们估计的是后验概率分布，概率论中可以以 变分推断 的方法来处理 变分推断 所谓变分推断，是取一个用隐变量z的分布q(z)q(z)q(z)来近似后验概率的条件分布p(z∣x)p(z|x)p(z∣x)的方法。为了比较二者的分布相似度，使用KL散度来计量： 如果能找到与p(z∣x)p(z|x)p(z∣x)在KL散度意义下最近的分布 q^(z)\\hat q(z)q^​(z)，则可以用这个分布近似后验概率分布 带入KL散度的公式： 则，如果想要KL散度更小，必有 log p(x)≥Eq[log p(x,z)]−Eq[log q(z)]log\\ p(x) \\geq E_q[log\\ p(x,z)] - E_q[log\\ q(z)] log p(x)≥Eq​[log p(x,z)]−Eq​[log q(z)] 因为先验x的分布log p(x)log\\ p(x)log p(x)可以被视为常量，因此右侧关于q分布的期望式越大，KL散度就越小。数学上把右侧称为证据下界。因此，下面问题就变为求证据下界的最大化。还有一个假设，是对于隐变量z（向量），假设分布q(z)q(z)q(z)对 z 的所有分量都是独立的，即： q(z)=q(z1)q(z2),...,q(zm)q(z) = q(z_1)q(z_2),...,q(z_m) q(z)=q(z1​)q(z2​),...,q(zm​) 称其为平均场。 现在让我们回到LDA模型的最大后验概率估计，现在我们有了变分推断算法，可以定义“文本w的后验” 的证据下界： L(r,t,α,ϕ)=Eq[log p(z,w)]−Eq[log q(z)]=Eq[log p(θ,z,w∣α,ϕ)]−Eq[log q(θ,z∣r,t)]L(r,t,\\alpha,\\phi) = E_q[log\\ p(\\bold z, \\bold w)] -E_q[log\\ q(\\bold z)]\\\\ = E_q[log\\ p(\\theta,\\bold z, \\bold w| \\alpha,\\phi)] -E_q[log\\ q(\\theta,\\bold z|r,t)] L(r,t,α,ϕ)=Eq​[log p(z,w)]−Eq​[log q(z)]=Eq​[log p(θ,z,w∣α,ϕ)]−Eq​[log q(θ,z∣r,t)] 其中，向量r和t是变分参数，r来估计隐变量z在单词向量中的分布参数θ\\thetaθ，t来估计话题向量中的分布参数(z1,z2,...,zn)(z_1,z_2,...,z_n)(z1​,z2​,...,zn​)有了平均场假设，就可以对每个文本分来计算，得到所有文本的证据下界： L′(r,t,α,ϕ)=∑m=1MEq[log p(θm,zm,wm∣αm,ϕm)]−Eq[log q(θm,zm∣rm,tm)]L&#x27;(r,t,\\alpha,\\phi) =\\sum_{m=1}^M{ E_q[log\\ p(\\theta_m,\\bold z_m, \\bold w_m| \\alpha_m,\\phi_m)] -E_q[log\\ q(\\theta_m,\\bold z_m|r_m,t_m)]} L′(r,t,α,ϕ)=m=1∑M​Eq​[log p(θm​,zm​,wm​∣αm​,ϕm​)]−Eq​[log q(θm​,zm​∣rm​,tm​)] 此时我们发现它也是一个 离散的期望最大化估计 问题了，可以使用第二节提到的 EM算法来迭代更新参数，此时有四个参数向量，两个是为了变分推断引入的，另外两个则为模型建立的狄利克雷分布参数。 注意，实际上狄利克雷分布的参数是α、β\\alpha 、\\betaα、β，不过因为话题的分布参数ϕ\\phiϕ可以直接由参数β\\betaβ根据话题数 k 得到，因此这里简化一下模型，直接估计参数向量ϕ\\phiϕ 这种方法被综合称为 **变分EM算法** LDA和PLSA的比较 相同点：都将 话题建模为单词的多项分布，文本建模为话题的多项分布 不同点： PLSA没有使用先验分布（ 或者说假设先验分布是均匀分布 ），使用MLE估计。 而LDA假设了狄利克雷分布作为先验分布，且使用MAP估计。 有先验分布的好处和C2中提到的一样，可以防止过拟合问题","tags":["生成模型","笔记"]},{"title":"【笔记】生成模型基础与应用 - 第2章","path":"/2023/11/27/生成模型基础与应用笔记-第2章/","content":"C2 离散数据的生成模型 C2 离散数据的生成模型 生成模型的目的：学习联合概率分布 P(X,Y)结合第一节课的知识，接下来我们会介绍一些比较传统的生成模型，它们不会涉及DL和神经网络，但是是后续生成模型的数学基础。 贝叶斯概念学习（以离散模型举例） 只提供正向的样本，让模型学习正向的特征，然后判断输入是否属于要训练的类别（概念） 与二分类任务的原理相同，但是训练二分类模型会提供 both 正向样本和负向样本 **以学习一个猜数字的模型为例：**问题：有一个由数字组成的数据集D\\mathcal {D}D，问如何学习一个概念C（C是未知的，描述数据集D\\mathcal {D}D中数字应该服从的分布），这里的数字都在0-100之间。 假设空间：人为地提出“假设”，用来猜测概念C。根据某一个概念假设（如“偶数”），所有可能生成的元素组成h的集合。比如“偶数”，则对应的假设数据集h={2,4,6,...,100}h=\\{2,4,6,...,100\\}h={2,4,6,...,100}。所有可以提出假设组成假设空间 版本空间：即所以“有效的”假设组成的空间。即满足：数据集D中的所有元素在假设的集合h中，这些假设组成的空间。 注意：一般来说，数据集D中包含的元素数量越多，版本空间就会越小，因为会有很多假设连数据集中的采样数据都过不了关，就被筛除了。 似然（likehood）：从h中独立采样N次，从假设能生成 数据集D中数据的概率 P(D∣h)P(\\mathcal {D}|h)P(D∣h) P(D∣h)=[1∣h∣]NP(\\mathcal {D}|h) = [\\frac{1}{|h|}]^NP(D∣h)=[∣h∣1​]N, 这里|h|表示h的集合大小 考虑假设1：h=“偶数”，假设2：h=“2的幂” 若 D = {16}，则假设1的概率P(D|h) = 1/6，则假设2的概率P(D|h) = 1/50 若 D = {2,8,16,64} 此时，假设2 的 似然概率是 假设1 的4812.5倍 奥卡姆剃刀原则：选择似然相近的假设时，假设h中包含的数据数量越少越好 先验概率：当提出假设时，为其中的数据赋予先验概率P(h)P(h)P(h) 。当提出的假设在数据集中对应的实际意义不太自然时，为其赋予低的先验概率 比如，D = {50,60,70,90}，给与两个假设，分别包含数据 67 和 210 如果数据集描述的是自然数，那么包含210的假设拥有更高的P(h) ，因为210和D中数据都是10的倍数 如果数据集描述的人的体重数据(kg)，那么包含67的假设拥有更高的P(h)，因为210在不太可能是符合概念的数据 后验：判断版本空间中的概念谁更接近要学习的概念。根据贝叶斯公式，后验概率与 先验概率和似然的乘积成正比：$$P(h|\\mathcal {D}) \\propto P(\\mathcal {D}|h) P(h)$$，后验概率最大的假设可以被认为是最优假设。 如果用H标记假设空间中所有假设i组成的集合，那么将数据集D与每种假设的联合概率求和，就是后验概率的基数了，即： P(h∣D)=P(D∣h)P(h)∑i∈HP(D,i)P(h|\\mathcal {D}) = \\frac{P(\\mathcal {D}|h)P(h)}{\\sum_{i \\in H}P(\\mathcal {D},i)} P(h∣D)=∑i∈H​P(D,i)P(D∣h)P(h)​ 注意：预先定义的假设空间不一定是完备的，真·概念C不一定在你的空间中，如果这样（现实中大多数情况也是如此），找到的最优假设只能说是最接近C的。 接下来我们尝试直接用数学推导出求最大后验概率的公式： 最大后验概率估计 MAP 我们要求一个h，其会使得后验概率最大，数学中我们用argmax函数来描述使得一个函数取最大值时，自变量的值，因此： h^map=argmaxh(P(h∣D))=argmaxh(P(D∣h)P(h)∑i∈HP(D,i))\\hat h^{map} = argmax_h(P(h|D)) = argmax_h(\\frac{P(\\mathcal {D}|h)P(h)}{\\sum_{i \\in H}P(\\mathcal {D},i)}) h^map=argmaxh​(P(h∣D))=argmaxh​(∑i∈H​P(D,i)P(D∣h)P(h)​) 分母对所有假设的联合概率求和，这与h无关，因此我们可以认为：h^map=argmaxhP(D∣h)P(h)=argmaxh[log([1∣h∣]N)+log(P(h))]\\hat{h}^{map} = argmax_h P(\\mathcal {D}|h)P(h) = argmax_h [log([\\frac{1}{|h|}]^N) + log(P(h))]h^map=argmaxh​P(D∣h)P(h)=argmaxh​[log([∣h∣1​]N)+log(P(h))] 右边我们取对数，在不破坏函数的单调性的同时将乘积的概率变为加法，然后我们会发现： 常量N会影响最大函数的取值 当N越大，即数据足够多时，P(D∣h)P(\\mathcal {D}|h)P(D∣h)即log([1∣h∣]N)log([\\frac{1}{|h|}]^N)log([∣h∣1​]N)的对数增长速率远大于P(h)P(h)P(h) 因此在计算出最大概率的h时，后者影响越来越小，直到可以忽略不计，因此，在实际建模中，我们常常只估计似然P(D∣h)P(\\mathcal {D}|h)P(D∣h)，我们将这种近似的方法称为：极大似然估计 MLE : \\hat{h}^{mle}\\ \\~= \\ argmax_h \\ P(\\mathcal {D}|h) = argmax_h \\ log([\\frac{1}{|h|}]^N) 于是，当N足够大时，将每个H空间中的假设h代入上式，使得函数取值最大的h可以被视为最优假设。 生成新数据：已知最优假设h后，便可以直接以该估计作为条件，求出生成各种元素（x）的概率： p(xˉ∣D)=∑h∈HP(h∣D)P(xˉ∣h)≈P(xˉ∣h^map)p(\\bar x| \\mathcal {D} ) = \\sum_{h\\in H}{P(h|\\mathcal {D})P(\\bar x|h)} \\approx P(\\bar x|\\hat{h}^{map}) p(xˉ∣D)=h∈H∑​P(h∣D)P(xˉ∣h)≈P(xˉ∣h^map) 以上是一个离散分布的贝叶斯概念模型学习的例子，根据概率论，这个方法也可以推广到的连续随机变量的情况，比如服从二项分布的概率模型。相应地，我们需要将计算似然和概率估计的概率P(X)改为概率密度p(x)，而将求和计算转为积分。下面会介绍两个连续分布的贝叶斯概念模型 Beta二项分布生成模型 现在来看连续随机变量的情况，以抛硬币的二项分布为例： 抛一个硬币N次，记录有N0N_0N0​次正面朝上，N1N_1N1​次反面朝上 假设每次抛硬币的结果为随机变量x，x服从二项分布，正面朝上记为1，反面朝上记为0 那么，二项分布的参数θ\\thetaθ（正面朝上的概率）在[0,1]之间，是一个连续变量，也是我们要估计的 注意：我们的目标是估计θ\\thetaθ的分布，而不是直接求出一个结果（不然直接频率作为概率不就完了，不行，你这不贝叶斯啊）。 因为有很多可能的θ\\thetaθ，都可以使得我们的抛硬币实验得到上述的结果。 我们现在已知N次独立实验的结果，N0N_0N0​次正面朝上，N1N_1N1​次反面朝上就是“数据集”的数据，因此可以写出已知采样与我们实际要求的“抛硬币哪一面向上”分布的似然： likehood：$$P(\\mathcal {D} | \\theta) = \\theta{N_1}(1-\\theta){N_0}$$ 记住我们要求后验概率，它等价于先验概率和似然的乘积。现在的问题在于：我们怎么知道先验概率p(θ)p(\\theta)p(θ)？我们可以用贝塔分布来作为我们的先验概率模型，之所以是它的理由如下： 贝塔分布常用于估计[0,1]分布上，缺少足够先验样本的分布 贝塔分布有一个重要的特性：共轭先验，即将其带入贝叶斯模型，得到的后验概率的函数形式与先验相同。而如果我们将独立实验的结果N0N_0N0​或N1N_1N1​次作为随机变量计算先验，其服从伯努利分布，似然函数的形式也与贝塔分布的先验相同。 我们假设先验服从贝塔分布：p(θ)∼Beta(θ∣a,b)p(\\theta) \\sim Beta(\\theta|a,b)p(θ)∼Beta(θ∣a,b)于是我们现在可以计算后验： P(θ∣D)∝[θN1(1−θ)N0][θa−1(1−θ)b−1]=[θN1+a−1(1−θ)N0+b−1]=Beta(θ∣N1+a,N0+b)P(\\theta|\\mathcal {D}) \\propto [\\theta^{N_1}(1-\\theta)^{N_0}][ \\theta^{a-1}(1-\\theta)^{b-1}] = [\\theta^{N_1+a-1}(1-\\theta)^{N_0+b-1}]\\\\ = Beta(\\theta|N_1+a,N_0+b) P(θ∣D)∝[θN1​(1−θ)N0​][θa−1(1−θ)b−1]=[θN1​+a−1(1−θ)N0​+b−1]=Beta(θ∣N1​+a,N0​+b) 现在，我们只需估计后验函数取尽可能大值时，参数θ的分布，根据贝塔分布的特性， Beta(a,b) 分布的随机变量取最大值时，参数为a−1a+b−2\\frac{a-1}{a+b-2}a+b−2a−1​ 得到最大后验概率估计：θ^map=a−1+N1a+b−2+N\\hat \\theta^{map} = \\frac{a-1+N_1}{a+b-2+N}θ^map=a+b−2+Na−1+N1​​，根据共轭先验的性质，我们要估计的θ\\thetaθ分布也是一个贝塔分布，其具体形状与参数a，b有关。 如果我们使用极大似然估计，即不考虑先验，则结果退化为：θ^mle=N1N\\hat\\theta^{mle} = \\frac{N_1}{N}θ^mle=NN1​​，即直接用频率估计概率的结果，如果我们取上述贝塔分布的a=b=1（均匀分布，认为先验的结果硬币正面朝上的概率就是五五开的），那么也会得到这个结果。 使用MAP的结果，我们可以生成下一次抛硬币正面朝上的结果估计： P(xˉ=1∣D)=∫01P(xˉ=1∣θ)P(θ∣D)dθ=∫01θBeta(θ∣a+N1,b+N0)dθ=E[θ∣D]=a+N1a+b+NP(\\bar x=1|\\mathcal D) = \\int_0^1P(\\bar x =1 |\\theta)P(\\theta|\\mathcal D)d\\theta = \\int_0^1\\theta Beta(\\theta|a+N_1,b+N_0) d\\theta \\\\= E[\\theta|D] = \\frac{a+N_1}{a+b+N} P(xˉ=1∣D)=∫01​P(xˉ=1∣θ)P(θ∣D)dθ=∫01​θBeta(θ∣a+N1​,b+N0​)dθ=E[θ∣D]=a+b+Na+N1​​ 可以发现，结果正好是我们的贝塔分布模型的均值。有趣的是，只要我们使用MAP的生成结果，即使取a=b=1的均匀分布，也会使得估计正面朝上的概率中，分子至少为1，不会出现 _因为实验数据集中没有正面朝上，就将其判定为不可能事件 _的情况。因此，这种方法也被称为**”（拉普拉斯）+1平滑“** 狄利克雷多项分布生成模型 再来看一个复杂一点的情况：随机变量有K个可能的取值，比如说，估计一个有k面的骰子，每个面朝上的概率。我们假设做了N次实验，每个面朝上的次数分别为：N1,N2,...NkN_1,N_2,...N_kN1​,N2​,...Nk​和伯努利实验类似，可以这样定义： 似然函数： p(D∣θ)=θ1N1θ2N2...θkNkp(\\mathcal D|\\theta) = \\theta_1^{N_1}\\theta_2^{N_2}...\\theta_k^{N_k} p(D∣θ)=θ1N1​​θ2N2​​...θkNk​​ 先验函数，将之前贝塔分布的共轭先验推广到多个变量，这就是狄利克雷多项分布： p(θ)=Dir(θ∣α)p(\\theta) = Dir(\\theta|\\alpha) p(θ)=Dir(θ∣α) 后验计算： 最大后验概率估计： θ^k=Nk+αk−1N+∑k=1Kαk−K\\hat \\theta_k = \\frac{N_k+\\alpha_k-1}{N+\\sum^K_{k=1}\\alpha_k - K} θ^k​=N+∑k=1K​αk​−KNk​+αk​−1​ 同样的，如果使用最大似然估计，会退化为：θ^k=NkN\\hat \\theta_k = \\frac{N_k}{N}θ^k​=NNk​​ 生成一个骰子点数，其结果是j点的概率： P(xˉ=j∣D)=E(θj∣D)=αj+Nj∑k=1Kαk+NP(\\bar x=j|\\mathcal D) = E(\\theta_j|\\mathcal D) = \\frac{\\alpha_j+N_j}{\\sum^K_{k=1}\\alpha_k+N} P(xˉ=j∣D)=E(θj​∣D)=∑k=1K​αk​+Nαj​+Nj​​ 朴素贝叶斯分类器（NBC） 在第一章，我们说了 生成模型 包含 分类模型的所有功能。比如下面这个分类问题： 任务：将若干个由离散数据组成的向量进行分类。设变量 x=(x1,x2,...xD)Tx = (x_1,x_2,...x_D)^Tx=(x1​,x2​,...xD​)T是长度为D的特征向量，每个元素（特征）有KKK个可能的取值，希望将所有这样的向量分为C类： {1,2,…C} ，用 Y=c 来表示结果的类标号。于是我们想要知道，给定向量x和模型参数向量θ，分类的概率：Py=P(Y=c∣x,θ)P_y=P(Y=c|x,\\theta)Py​=P(Y=c∣x,θ)，只要对所有的类别c计算这个概率，取最大值作为最终分类。 我们尝试用贝叶斯生成模型的方法来解决这个问题。这涉及到刚才两节二项分布和狄利克雷多项分布方法的综合。 首先，我们考虑给定类别c，能生成特征向量x的概率： Px=P(x∣Y=c,θ)P_x = P(x|Y=c,\\theta)Px​=P(x∣Y=c,θ) 则由贝叶斯公式，要求的概率可以表示为： Py=P(Y=c∣x,θ)=P(x,Y=c∣θ)P(x∣θ)=P(x,Y=c∣θ)∑i∈C[P(x,Y=i∣θ)]=P(x∣Y=c,θ)P(Y=c∣θ)∑i∈C[P(x∣Y=i,θ)P(Y=i∣θ)]∝P(Y=c∣θ)PxP_y=P(Y=c|x,\\theta) = \\frac{P(x,Y=c|\\theta)}{P(x|\\theta)} = \\frac{P(x,Y=c|\\theta)}{\\sum_{i \\in C}[P(x,Y=i|\\theta)]} = \\frac{P(x|Y=c,\\theta)P(Y=c|\\theta)}{\\sum_{i \\in C}[P(x|Y=i,\\theta)P(Y=i|\\theta)]} \\propto {P(Y=c|\\theta)}P_x Py​=P(Y=c∣x,θ)=P(x∣θ)P(x,Y=c∣θ)​=∑i∈C​[P(x,Y=i∣θ)]P(x,Y=c∣θ)​=∑i∈C​[P(x∣Y=i,θ)P(Y=i∣θ)]P(x∣Y=c,θ)P(Y=c∣θ)​∝P(Y=c∣θ)Px​ 可以发现，因为现在问题空间包含两个随机变量x，Y，对于类别的变量Y，式子中也推出了先验概率。 P(Y=c∣θ)P(Y=c|\\theta)P(Y=c∣θ)就是分类先验（在训练的数据集中，最终被每一类被分入了多少个向量）。 这会使得模型的参数向量θ中还要包含分类先验的参数。如果将它们和向量x的参数混为一谈，这将导致无法更新参数。为了能将向量内的每个特征对应的参数拆分出来计算，朴素贝叶斯提出一个重要的假设： 给定同一个类别的标签，每一个特征的条件都是独立的 注意：真实世界中，这个假设是很难成立的，但是依然该方法依然可以用来估计分类器 则我们可以将特征向量在模型中的参数用θ表示，而分类先验的参数在下文将用π\\piπ来表示，它们将被分别估计。先来看特征向量： Px=P(x∣Y=c,θ)=Πi=1DP(xi∣Y=c,θic)=Πi=1DTiP_x = P(x|Y=c,\\theta) = \\Pi_{i=1}^{D}P(x_i|Y=c,\\theta_{ic}) = \\Pi_{i=1}^{D} T_i Px​=P(x∣Y=c,θ)=Πi=1D​P(xi​∣Y=c,θic​)=Πi=1D​Ti​ 我们用TiT_iTi​来标记以 向量中的每个特征 为随机变量的 分布。对于特征的定义不同，这个分布会表示成不同的形式： 如果特征是0-1编码的：值不是0就是1，那么TiT_iTi​是伯努利分布，参数θic\\theta_{ic}θic​要表示 第i个特征的值会使得整个向量有多大可能性落在分类c中 如果每个特征有KKK个可能的取值（原始的问题），那么TiT_iTi​是多项分布，参数θic\\theta_{ic}θic​要表示为一组向量。 如果特征是连续的实数来表示的，那么TiT_iTi​将是一个连续的高斯分布，参数也要表示为&lt;均值，方差&gt;的一组值。 回到问题的开始，为了下面的推导写的比较简单，我们将问题简化为只要0-1编码的两种特征 （它已经很复杂了QAQ，再加上K个类别我都不敢想……） 然后我们来看看如何使用贝叶斯学习来进行训练：假设训练数据集中包含了N个**&lt;向量x, 分类标签Y&gt;**的数据对，每个对用&lt;xi,Yi&gt;(i=1,2,...,n)&lt;x_i,Y_i&gt;(i=1,2,...,n)&lt;xi​,Yi​&gt;(i=1,2,...,n)来表示，那么分类的先验表示为：$$P(Y_i|\\pi) = \\Pi\\ \\pi_c^{if(Y_i =c)}$$ [...]if(Yi=c)[...]^{if(Y_i =c)}[...]if(Yi​=c)表示计数函数，即只有第i个分类标签为c时，才保留参数πc\\pi_cπc​【没错，还是统计方法】 注意这里作为条件概率的pi和下面的θ都是向量，而右侧式子表示取向量中的哪一个有效值（用符号π\\piπ来区分 分类的参数向量 和 特征的参数向量θ\\thetaθ） 同样，特征的先验计算为： P(xi∣Yi,θ)=Πj=1DP(xi[j] ∣Yi,θj)=Πj=1D(Π P(xi[j] ∣θjc)if(Yi=c))P(x_i|Y_i,\\theta) = \\Pi_{j=1}^DP(x_i[j]\\ |Y_i,\\theta_j) = \\Pi_{j=1}^D(\\Pi\\ P(x_i[j]\\ |\\theta_{jc})^{if(Y_i=c)}) P(xi​∣Yi​,θ)=Πj=1D​P(xi​[j] ∣Yi​,θj​)=Πj=1D​(Π P(xi​[j] ∣θjc​)if(Yi​=c)) 于是，推导整个数据集的后验概率函数，即将所有数据的结果相乘起来： P(D∣θ)=Πi=1N[P(Yi∣θ)P(xi∣Yi,θ)]P(\\mathcal D|\\theta) = \\Pi_{i=1}^N[P(Y_i|\\theta)P(x_i|Y_i,\\theta)] P(D∣θ)=Πi=1N​[P(Yi​∣θ)P(xi​∣Yi​,θ)] 尝试使用极大似然估计来直接计算参数（MLE），直接对两边取对数，得到： log[P(D∣θ)]=∑c=1CNc+∑j=1D(∑c=1C(∑Yi=clogP(xi[j]∣θjc)))log[P(\\mathcal D|\\theta)] = \\sum_{c=1}^C N_c + \\sum_{j=1}^D(\\sum_{c=1}^C(\\sum_{Y_i=c}logP(x_i[j]|\\theta_{jc}))) log[P(D∣θ)]=c=1∑C​Nc​+j=1∑D​(c=1∑C​(Yi​=c∑​logP(xi​[j]∣θjc​))) （其中xi[j]x_i[j]xi​[j]表示向量xix_ixi​中的第jjj个值）上面的式子看着非常复杂，其实本质上与上一节的二项分布估计相同： 分类的先验估计就是：从数据集N中统计出分类c出现次数NcN_cNc​，并计算比例： πc=NcN\\pi_{c} = \\frac{N_{c}}{N}πc​=NNc​​ 特征的似然参数估计就是：从每个已知类别的出现次数中，统计出是当前特征的比例，即： θjc=NjcNc\\theta_{jc} = \\frac{N_{jc}}{N_c}θjc​=Nc​Njc​​ （不是0-1编码的，NjcN_{jc}Njc​就是一个嵌套的向量了） 这个结果，与抛硬币和骰子一样，因为先验是完全按照统计结果的，容易出现过拟合的问题。另一种方法，可以使用刚才的贝叶斯方法，对pi和θ两个参数的分布假设先验： :::warning 假设P(Yi∣π)P(Y_i|\\pi)P(Yi​∣π)是一个狄利克雷分布，参数是α=(α0,α1,...αC)\\alpha = (\\alpha_0,\\alpha_1,...\\alpha_C)α=(α0​,α1​,...αC​) 假设每个 P(xi[j] ∣θjc)P(x_i[j]\\ |\\theta_{jc})P(xi​[j] ∣θjc​)都是一个贝塔分布，参数是(β0,β1)(\\beta_0,\\beta_1)(β0​,β1​) ::: 然后计算后验，形式是一样的，只不过丑陋的计数函数被替换成了先验分布的均值：代入最大后验概率估计MAP的估计结果是：如果有一条新的数据需要分类，那么对于每个类别c=1,2,...Cc=1,2,...Cc=1,2,...C，将MAP估计的参数带入P(Y=c∣xˉ,θ)P(Y=c|\\bar x,\\theta)P(Y=c∣xˉ,θ)，得到使得概率最大的c就是要求的分类。可以看出，朴素贝叶斯模型虽然推导起来比较复杂（其实也只是复杂在数据维度比较多，人脑CPU容易干烧，交给计算机来还是很快的），但是计算复杂度很低，所以在小场景中使用广泛。同时，我们也可以用这个模型生成新的数据：和二项分布生成模型一样，在生成新的数据时，代入模型的参数就是分布的均值：","tags":["生成模型","笔记"]},{"title":"【笔记】生成模型基础与应用 - 第1章","path":"/2023/10/28/生成模型基础与应用笔记-第1章/","content":"C1 概率论和统计基础 C1 概率论和统计基础 什么是概率？ 频数的概率解释：频率估计概率（小学就学过的） 贝叶斯概率解释：概率是某样事件的“不确定度”，是人们在多大程度上相信某件事情将会发生（似然likehood） 因为贝叶斯概率能适用更多更广泛的概率事件（例如：从未发生但可能的事件），因此以下讨论的概率偶都基于贝叶斯概率 事件、随机变量和概率分布： 事件：自然语言描述的具有随机性的事件 随机变量：可能从有限 或 可数的无限 集合X中随机取值的变量 概率的表示：P(事件A) = P(随机变量X=某值a) 【如果我们定义当X取a时事件A发生】 概率分布：表述随机变量X的 取值 的 概率规律 的集函数 离散随机变量要点回顾 事件合并概率： P(A∪B)=P(A)+P(B)−P(A∩B)P(A ∪ B) = P(A) + P(B) - P(A ∩ B)P(A∪B)=P(A)+P(B)−P(A∩B) 如果A和B是互斥事件，则：P(A∪B)=P(A)+P(B)P(A ∪ B) = P(A) + P(B)P(A∪B)=P(A)+P(B) 两个随机变量的联合概率： P(A,B)=P(A)P(B∣A)=P(B)P(A∣B)P(A,B) = P(A)P(B|A) = P(B)P(A|B)P(A,B)=P(A)P(B∣A)=P(B)P(A∣B) 联合概率分布的链式法则： P(X1,X2,...,XD)=P(X1)P(X2∣X1)P(X3∣X1,X2)...P(XD∣X1,X2...XD)P(X_1,X_2,...,X_D)=P(X_1)P(X_2|X_1)P(X_3|X_1,X_2)...P(X_D|X_1,X_2...X_D)P(X1​,X2​,...,XD​)=P(X1​)P(X2​∣X1​)P(X3​∣X1​,X2​)...P(XD​∣X1​,X2​...XD​) 两个随机变量的条件概率： P(B∣A)=P(B,A)P(A)=P(B)P(A∣B)P(A) ,P(A)&gt;0P(B|A) = \\frac{P(B,A)}{P(A)}=\\frac{P(B)P(A|B)}{P(A)} \\ ,P(A) &gt; 0P(B∣A)=P(A)P(B,A)​=P(A)P(B)P(A∣B)​ ,P(A)&gt;0 贝叶斯条件概率公式 重要名称： 先验：已知的，B事件发生的概率 似然：在B的条件下A发生的概率 与 A发生的概率 之比：P(A∣B)P(A)\\frac{P(A|B)}{P(A)}P(A)P(A∣B)​ 后验：在A的条件下，B发生的概率，即P(B|A) 两个随机变量的条件独立性： 如果两个变量的联合概率可以被拆分为各自概率的乘积，则称两个变量是 （无条件）独立的 P(X,Y)=P(X)P(Y)P(X,Y) = P(X)P(Y)P(X,Y)=P(X)P(Y) 而如果给定随机变量Z，在Z的条件概率下满足上述条件，则称两个变量X,Y是 条件独立的 P(X,Y∣Z)=P(X∣Z)P(Y∣Z)P(X,Y|Z) = P(X|Z)P(Y|Z)P(X,Y∣Z)=P(X∣Z)P(Y∣Z) 连续随机变量要点回顾 累计分布函数（cdf）：F(x) = P(X &lt;= x)当连续变量的取值小于x时，总计的概率 概率密度函数（pdf）：p(x) = F(x) dx 是累计分布函数的微分 计算连续随机变量在a,b区间上的概率：F(b) - F(a)，或是在p(x)上积分 连续分布的数学量：均值E、方差σ^2、中位数…… 协方差：对于两个随机变量X,Y，衡量它们的线性相关性： cov[X,Y]=E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y)cov[X,Y] = E[(X-E(X))(Y-E(Y))] = E(XY) - E(X)E(Y)cov[X,Y]=E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y) 相关系数：将协方差标准化后的数学量： corr[X,Y]=R=cov[X,Y]σ2(X)σ2(Y)corr[X,Y] = R= \\frac{cov[X,Y]}{\\sqrt{\\sigma^2(X) \\sigma^2(Y)}}corr[X,Y]=R=σ2(X)σ2(Y)​cov[X,Y]​ 独立的两个变量，它们不相关 但是，不相关的两个变量，可能互相独立 如果两个符合高斯分布的变量不相关，则它们一定独立 对于多元随机变量的联合分布，协方差和相关系数的计算将变为矩阵的形式： 常见概率分布回顾 【离散的】 经验分布（eCDF）：描述从抽样中得到的概率分布，经验分布的概率密度函数即为所有抽样的结果之和，其中抽样被定义为 狄利克雷函数：即抽样的结果只有0或者1. 二项分布：重复n次独立的 伯努利事件 实验，获得其中一种结果k次的概率分布 伯努利事件：某个事件只有两种可能的结果（布尔随机变量），其中一种的概率为p，另一个为1-p 经典的例子是抛硬币 多项分布：重复n次独立的 多重伯努利事件 实验，获得 每种结果的次数 的概率分布 多重伯努利事件：某个事件可能有k种不同的结果，并且每种结果具有固定的概率 经典的例子是投一个k面的骰子 【连续的】 均匀分布：在一个区间或域上，随机变量的取值为固定值 对于一维变量，随机分布的概率密度函数为：Unif(x)=1b−a(a≤x≤b)Unif(x) = \\frac{1}{b-a} (a \\leq x \\leq b )Unif(x)=b−a1​(a≤x≤b) 正态分布（高斯分布）：多个相互独立的随机变量之和 的分布 会趋近于这个分布，因此它被广泛使用 正态分布的概率密度函数和符合该分布的随机变量的均值和方差有关 N(x∣μ,σ2)=12πσ2e−12σ2(x−μ)2N(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}N(x∣μ,σ2)=2πσ2​1​e−2σ21​(x−μ)2 可以拓展为多元变量的正态分布，将均值修改为多元变量的数学期望，方差修改为多元变量的协方差 泊松分布：对二项分布的连续近似，在二项分布的实验次数n很大，单次概率p很小时，二项分布可被近似为泊松分布。 P(x∣λ)=e−λλxx!P(x|\\lambda) = e^{-\\lambda}\\frac{\\lambda^x}{x!}P(x∣λ)=e−λx!λx​, 其方差和均值都是 λ Student t 分布：基于正态分布，为了增强其抗干扰性而提出的分布，加入一个参数v： 拉普拉斯分布（双指数分布）：在均值的两侧，呈现对称分布规律的一种 指数分布 变种 Lap(x∣μ,b)=12be−∣x−μ∣bLap(x|\\mu,b) = \\frac{1}{2b}e^{-\\frac{|x-\\mu|}{b}}Lap(x∣μ,b)=2b1​e−b∣x−μ∣​，其均值为 μ\\muμ，方差为 2b22b^22b2 伽马分布：对正实数域上的随机变量建模的分布，是多个独立同分布的指数分布变量 和 的分布 Gamma(x∣a,b)=baγ(a)xa−1e−xbGamma(x|a,b) = \\frac{b^a}{\\gamma(a)}x^{a-1}e^{-xb}Gamma(x∣a,b)=γ(a)ba​xa−1e−xb，其中γ(a)是伽马函数：\\gamma(a) = \\int_0^\\inf t^{a-1}e^{-t}dt 参数a被称为shape，b被称为rate，该分布均值为 ab\\frac{a}{b}ba​，方差为ab2\\frac{a}{b^2}b2a​ 贝塔分布：对[0,1]区间上取值的随机变量建模的分布 Beta(x,∣a,b)=1β(a,b)xa−1(1−x)b−1Beta(x,|a,b) = \\frac{1}{\\beta(a,b)}x^{a-1}(1-x)^{b-1}Beta(x,∣a,b)=β(a,b)1​xa−1(1−x)b−1,其中 β(a,b)\\beta(a,b)β(a,b)是贝塔函数，它只是为了使得这个分布的概率密度积分等于1才加上的。 狄利克雷分布：将贝塔分布拓展到多元变量的泛化 【分布的变换】 若分布Y可以由服从分布X的随机变量，将每个取值用离散或连续的函数f变换得到，那么分布Y的均值和方差会遵循以下公式 线性变换： 通用变换： 离散变量： py(y)=∑x:f(x)=ypx(x)p_y(y)=\\sum_{x:f(x)=y}p_x(x)py​(y)=∑x:f(x)=y​px​(x) 连续变量： py(y)=px(x)∣dxdy∣p_y(y)=p_x(x)|\\frac{dx}{dy}|py​(y)=px​(x)∣dydx​∣ 其他重要概念 大数定律：随着样本规模的增加，样本均值对总体均值的估计越准确。 中心不变定理：多个随机变量样本的均值分布（随机变量和的分布）将近似于高斯分布。 蒙特卡洛近似：如果某随机变量X的分布未知，但可以对其进行抽样来实验，则可以使用经验分布来近似X的分布： 衡量两个分布的相似度（距离）：KL散度 先补充信息论的知识：信息熵 信息熵可以描述随机变量X在分布P上的不确定性的程度：H(X)=−∑Kk=1p(X=k)log2p(X=k)H(X) = -\\sum_K^{k=1}p(X=k)log_2p(X=k)H(X)=−∑Kk=1​p(X=k)log2​p(X=k) 均匀分布的信息熵最大 交叉熵：将服从分布P的变量转换到分布Q，需要提供额外信息（bits）的量，其中p和q代表P和Q的概率密度函数 H(p,q)=−∑kpklogqkH(p,q) = -\\sum_{k}p_klogq_kH(p,q)=−∑k​pk​logqk​ KL散度：描述两个分布的概率密度函数p和q的相似度： KL(p∣∣q)=∑k=1Kpklogpkqk=−H(p)+H(p,q)KL(p||q) = \\sum_{k=1}^Kp_klog\\frac{p_k}{q_k}=-H(p)+H(p,q)KL(p∣∣q)=∑k=1K​pk​logqk​pk​​=−H(p)+H(p,q) 互信息度：衡量两个分布的变量之间互相依赖的程度： II(X;Y)=KL(p(X,Y)∣∣p(X)p(Y)) =∑x∑yp(x,y)logp(x,y)p(x)p(y)=H(X)−H(X∣Y)=H(Y)−H(Y∣X)II(X;Y) = KL(p(X,Y)||p(X)p(Y))\\ = \\sum_x\\sum_yp(x,y)log\\frac{p(x,y)}{p(x)p(y)}\\\\=H(X)-H(X|Y)=H(Y)-H(Y|X)II(X;Y)=KL(p(X,Y)∣∣p(X)p(Y)) =∑x​∑y​p(x,y)logp(x)p(y)p(x,y)​=H(X)−H(X∣Y)=H(Y)−H(Y∣X)","tags":["生成模型","笔记"]},{"title":"Test Article","path":"/2023/10/28/Test-Article/","content":"Hello world! Hi there! 如果你看到这个，说明这个博客框架似乎运行正常。 This marks the blog is running well. 下面的部分用于测试hexo和stellar主题扩展的相关功能，请忽略其中的内容。 表情 行内文本修饰 美化文本格式 这是 你知道的太多了 隐藏标签 这是 下划线 标签 这是 着重号 标签 这是 波浪线 标签 这是 删除线 标签 这是 X2 上标标签 这是 X2 下标标签 这是 键盘样式 标签，试一试：Ctrl + D mark 标记 这是彩色标记：默认 红 橙 黄 绿 青 蓝 紫 这是底色标记：浅 深 这是特殊标记： 警告 错误 一共 12 种颜色。 tag标签 和mark类似，但是可以高亮和添加链接，使用属性color指定颜色，不指定为随机 Stellar Hexo MyGitHub 使用标签插入图片 这将比使用默认markdown格式插入图片要好 src: 图片地址 description: 图片描述 width:和padding:可以对不同的尺寸做适应 bg:可以添加背景颜色，使用bg:var(–card)使得背景颜色适配全局配色 download: 设置为true可以增加一个下载图片的按钮，但这只会从src中下载图片，如果（对于大图）图片的下载地址和src的预览地址不同，可以将下载地址写在这里 点击放大功能：在任意 image 标签中增加 fancybox:true 参数即可为特定图片开启缩放功能 1&#123;% image src [description] [download:bool/string] [width:px] [padding:px] [bg:hex] %&#125; 点击下载你可以获得另一张来自Apple的图片 文本美化块 美化的引用 适合居中且醒目的引用：这句话不是我说的 ——鲁迅 支持自定义引号：话题001 诗词文本展示 游山西村陆游莫笑农家腊酒浑，丰年留客足鸡豚。山重水复疑无路，柳暗花明又一村。箫鼓追随春社近，衣冠简朴古风存。从今若许闲乘月，拄杖无时夜叩门。诗词节选 备注块 正式内容中，第一个空格前面的是标题，后面的是正文。如果标题中需要显示空格，请使用&amp;nbsp;代替。 无色备注块：和代码块一样的展示方式 你&nbsp;标题&nbsp;中有&nbsp;空&nbsp;格正文部分可以随便空格。 彩色备注块：使用color:&lt;color&gt;设置背景颜色，颜色标签和行内文本一致 可用颜色：color = red、orange、yellow、green、cyan、blue、purple、light、dark、warning、error 只使用标题的备注块，使得文本默认加粗： 这个只有标题，没有正文。 外链卡片 12345&#123;% link href [title] [icon:src] [desc:true/false] %&#125;href: 链接title: 可选，手动设置标题（为空时会自动抓取页面标题）icon: 可选，手动设置图标（为空时会自动抓取页面图标）desc: 可选，是否显示摘要描述，为true时将会显示页面描述 https://www.bilibili.com/video/BV1ic411R77s/Nickbit的bilibili投稿 mermaid代码语言绘图 需要安装插件，目前还没有支持 使用前需要在 Markdown 文件开头加入 123---mermaid: true--- 然后使用代码块，在语言中指定mermaid即可 frame插入一个移动设备UI框架 可用插选择参数img（图片）和video（视频） iframe插入视频 静态时间线 2021 年 6 月BT夏季版banner2021 年 2 月过年放假 友情链接 要现在source/_data/links.yml中加入链接的静态数据： 123456789101112&#x27;链接组名称&#x27;: - title: 某某某 url: https:// screenshot: avatar: description: - title: 某某某2 url: https:// screenshot: avatar: description: ... 然后这样写： 1&#123;% friends 链接组名称 %&#125; 一个链接组可以有多个链接，它们会自适应排布到页面。 网站卡片链接 同样要先在source/_data/links.yml中加入链接的静态数据，写法和友链一样。 写法是： 1&#123;% sites 分组名 %&#125; 616.sb纯净的音游下载站BEMANICNBEMANICN 论坛BestdoriBanG Dream! 资源库和社区 github card卡片 填写github仓库的名称后缀即可 容器标签 支持更加丰富的分块文本，note等标签是由容器标签简化实现的。 容器标签需要多行来写 标准容器块 这是标题这是容器块中的内容插入一个链接：#172 可折叠标题的容器块 容器块中可用嵌套其他块，包括另一个容器块。使用child属性设置嵌入块的属性 open属性设置块是否默认打开 默认打开的代码折叠框代码块 平铺折叠列表块 #1这是答案1#2这是答案2#3这是答案3 分栏tab容器 方便地切换展示的内容，也可以嵌套 图片代码块表格12let x = 123print(&quot;hello world&quot;) a b c a1 b1 c1 a2 b2 c2 轮播容器 适用于轮播图片，默认一张图片是 50% 宽度，通过设置 width:min 设置为 25% 宽度，width:max 设置为 100% 宽度。 添加参考引用列表，效果如下： references: title: ‘使用 Stellar 主题的博客’ url: https://xaoxuu.com/wiki/stellar/examples/ title: ‘Hexo Stellar 和 Next 主题支持 KaTex 公式与 Markdown 复杂表格’ url: https://www.panoshu.top/blog/2c3f9e38/"},{"title":"Nickbit's 2021影视游艺总结","path":"/2021/12/28/2021影视游艺总结/","content":"名称 类型 体验时间（估计） 体验地点 评分（满分10分+附加分1分） 备注 Alba: A Wildlife Adventure 游戏 2021.1 Home 7+1 虽然玩法、剧情都很简单，甚至说低幼，但仍让我感到十分放松。 咒术回战 动漫 2021.1 Home 9+1 奇蛋物语 动漫 2021.1 Home+寝室 8+1 特别篇是6月份在寝室看的，说实话没看懂。 唐人街探案3 电影 2021.1 电影院 7 没啥感觉 你好，李焕英 电影 2021.1 电影院 8 刺杀小说家 电影 2021.2 Home 6 人潮汹涌 电影 2021.2 Home 8 有点感觉 刺客五六七（第三季） 动漫 2021.4 寝室 8+1 汉化日记（第二季） 动漫 2021.1 Home 8+1 《玩梗王者》 工作细胞（第二季） 动漫 2021.2 Home 8+1 我的三体 张北海传 动漫 2021.1 Home 9+1 前进四！ The Unfinished Swan未完成的天鹅 游戏 2021.2 Home 8+0.5 很神，但是很难说出来在哪里 Spiritfarer 灵魂摆渡人 游戏 2021.2 Home 8.5+1 死亡教育喜+1 戴森球计划 游戏 2021.2 Home 10+1 戴森球，YYDS！ 双人成行 游戏 2021.3 寝室 10+1 GOTY！ 节奏医生 游戏 2021.2 Home+寝室 10+1 一二三四五六七！ 核聚变2021北京站 展会 2021.5 亦创国际会展中心 9 人人人从从从众众众 寻找李白 话剧 2021.5 科学会堂 8.5 《中轴线》 书籍 2021.5 寝室 8 冲动消费的绘本竟成为今年唯一纸质阅读入账，堕落啊！ 爱，死亡和机器人(第二季) 剧集 2021.5 寝室 7+0.5 年底想来，只记得冰面了 灵笼（下半部分及终章） 动漫 2021.5 寝室 6.5+0.5 星露谷物语 游戏 2021.8 寝室 【未完整体验】 但是真的太上头了 霓虹深渊 游戏 2021.7 寝室 【未完整体验】 Lacuna 游戏 2021.9 寝室 7.5+0.5 音乐好听 小林家的龙女仆S 动漫 2021.10 寝室 9.5+1 小林用的python，托儿他爸说python是龙族的语言：懂了，python天下第一！（ 瑞克和莫蒂（第五季） 剧集 2021.10 寝室 10+1 越来越费脑子了 空洞骑士 游戏 2021.10 工位+寝室 【未完整体验】 前半部分是真滴劝退，但是City of Tears还是让我有信心打下去的 DELTARUNE (第二章) 游戏 2021.10 寝室 【未完整体验】 中文估计已经有了，但是ddl人估计要鸽到明年玩了（） 影子工厂 游戏 2021.10 寝室 9+1 瞰哥，我的瞰哥（The Great Wild Unknow 循环中） 失控玩家 电影 2021.10 电影院 7.5 沙丘 电影 2021.10 寝室 8 有空去看看原著 双城之战 剧集 2021.11 寝室 9+1 看之前没想到这么帅 Inscryption 邪恶冥刻 游戏 2021.11 寝室 10+1 看看人家怎么做卡牌游戏的！ 光明记忆：无限 游戏 2021.11 寝室 7 JOJO的奇妙冒险：石之海 动漫 2021.12 寝室 9+1","tags":["总结","评分表"]},{"title":"使用Spring和JPA构建REST规范的后端API项目","path":"/2021/09/19/使用Spring和JPA构建REST规范的后端API项目/","content":"关于如何使用Spring和JPA的入门教程 注意：该文档最初创建于Sep 19, 2021，可能存在已过时的内容，仅供参考 建立Spring项目 使用Spring Initializr（Spring提供的依赖包生成网站） Dependencies中加入以下依赖 Spring Web Spring Data JPA 一个数据库驱动（使用你喜欢的数据库驱动，比如MySQL Driver） 设置好以后，点击Generate Project，就可以下载到对应的zip格式的空项目包 在IDE中可以使用Maven管理项目包（建立Maven项目），只要把对应的包版本按照xml的格式写到项目根目录的pom.xml中就可以了 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; ...... &lt;dependencies&gt; &lt;dependency&gt; *添加你的依赖包* &lt;/dependency&gt; 配置程序主入口 一般来说，主入口文件都命名为XXXAplication.java，格式如下 12345678910import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class YourApplication &#123; public static void main(String... args) &#123; SpringApplication.run(YourApplication.class, args); &#125;&#125; 其中@SpringBootApplication标识服务端的入口类，SpringApplication.run(YourApplication.class, args)使用SpringBoot启动服务，一切配置都由其自动完成。 使用JPA链接模型类和数据库 JPA使用ORM（对象关系映射）来方便我们用代码链接数据库，简单来说，我们可以用一个类对应数据库中的一张表，类中的成员对应表中的属性，当类成员的值变化时，让JPA自动同步数据表中的值。这样的类一般称为模型类（MVC中的Model）或实体类，可以存放一些基础数据（比如用户类User） 语法说明 导入命名空间： 123import javax.persistence.Entity;import javax.persistence.GeneratedValue;import javax.persistence.Id; 1234@Entityclass User&#123;\t...&#125; @Entity 注解，写在类前面，声明JPA托管该类 123@Id @GeneratedValueprivate int id; @Id 和 @GeneratedValue：注解，写在类成员变量前面，声明该变量在数据表中所对应的属性是主关键字（PK） 其中@GeneratedValue注解向JPA声明主关键字的生成策略 123456@Id @GeneratedValueprivate long id;private String email; private String username; 对于非主关键字的属性，JPA会自动以类成员变量的命名，生成对应的属性名，无需再加入注解。 12345User() &#123;&#125;User(String email,String username) &#123;\tthis.email = eamil;\tthis.username = username;&#125; 有参构造函数用来创建没有指定id（主关键字）的实例 配置数据库的加载 JPA对类数据的操作，都是通过自定义一个新的接口实现的。 建立一个新接口，继承JpaRepository接口： 12345import org.springframework.data.jpa.repository.JpaRepository;interface UserRepository extends JpaRepository&lt;User, long&gt; &#123;\t...&#125; 其中，泛型参数&lt;User, long&gt;分别指JPA所链接的类的类型和其主关键字的类型. 接口类中可以自定义增删改查函数，如： 1234// 返回一个对象User findByUsername(String username);// 根据pageable参数返回分页对象（一页的内容）Page&lt;User&gt; findAllBy(Pageable pageable); 其中，findByXXXXX函数，Spring可以根据“XXXXX”的命名，自动对应模型类中同名（除了主键，不区分大小写）的数据变量，帮助你实现查找函数。即程序员只需声明该函数，并写对参数和返回类型。 findAllBy返回一个分页类型的所有数据成员。 然后，建立一个配置类，告诉Spring Boot加载接口对应数据类： 12345678910111213import ...@Configuration public class Initialize &#123; @Bean CommandLineRunner init(UserRepository userRepository, ...) &#123; return args -&gt; &#123; userRepository.save(new User()...) &#125;; &#125; 其中： @Configuration标识类是用来配置Spring Boot的加载项的 @Bean标识对象是一个Spring Bean CommandLineRunner接口：实现该接口的所有Bean都会在启动Spirng Boot时被加载。其中传入的参数即为JpaRepository类型的自定义接口对象（可以有多个，对应多个数据表），通过这些对象，即可以在服务加载时初始化数据库中内容。 在返回的匿名函数内用JpaRepository对象对数据表进行初始化操作： 使用xxxRepository.findByXXXX()函数，查找数据表的条目（前提：现在接口类中定义对应的函数） 使用xxxRepository.save()函数，修改并保存对应repository对应的模型类的成员数据，同时JPA会同步更新数据库的内容 注意：如果要操作模型类对象，使用其getter和setter函数，如 12345User user = userRepository.findByUsername(&quot;wht&quot;)String _info = user.getInfo();userRepository.save(new User().setUsername(&quot;admin&quot;) .setInfo(_info)) 在模型类中，注意封装好各个成员变量的getter和setter函数 实现HTTP控制模块 建立一个新类，用来处理前端请求，为了让Spring识别这个控制类，在类前加入注解@RestController 加入这些注解的目的是为了标记类为一个Spirng的Bean，Spring Boot帮我们配置好了扫描器，其会在启动时扫描全部带有@Component注解的类，将其注册为 Spring Bean。只有这样，Spring才会在有网络请求进入时，通过RequestMapping去索引 注册过的组件类 的相应函数。 @Component根据类的功能不同，其又分为以下三个子注解类型： @Controller：控制层，标注返回前端的控制组件 @Service：业务逻辑层，标注中间层的控制组件 @Repository：DAO层，标注数据库访问的组件 而@RestController是@Controller的一个升级版，其相当于@Controller + @ResponseBody，可以支持返回json格式的数据给前端。在REST模式设计的服务上，应该使用@RestController 在控制类中，在函数前加入注解@RequestMapping(&#123;url地址&#125;)，实现路由的作用，即用 URL请求的地址 对应 控制类调用相应的函数 根据REST规范，请求的类型可以分为 GET/POST/PUT/DELETE，因此可以直接使用以下子注解： @GetMapping(&quot;url&quot;) @PostMapping(&quot;url&quot;) @PutMapping(&quot;url&quot;) @DeleteMapping(&quot;url&quot;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// EmployeeController.javaimport java.util.List;// 导入RequestMapping使用的命名空间import org.springframework.web.bind.annotation.DeleteMapping;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.PutMapping;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RestController;@RestControllerclass EmployeeController &#123; // 引用接口服务，用来实现各种功能，记得要使用final静态变量 // 比如这里使用定义的JpaRepository接口对象，用来操作数据库 private final EmployeeRepository repository; EmployeeController(EmployeeRepository repository) &#123; this.repository = repository; &#125; // 如果用户请求GET，地址为/employees，调用该函数 @GetMapping(&quot;/employees&quot;) List&lt;Employee&gt; all() &#123; return repository.findAll(); &#125; // 如果用户请求GET，地址为/employees，调用该函数 @PostMapping(&quot;/employees&quot;) // @RequestBody 用于读取发来的请求中包含的参数数据，注意保持类型一致 Employee newEmployee(@RequestBody Employee newEmployee) &#123; return repository.save(newEmployee); &#125; @GetMapping(&quot;/employees/&#123;id&#125;&quot;) // @PathVariable 用于读取url地址中包含的参数数据，用&#123;&#125;包起参数名 Employee one(@PathVariable Long id) &#123; return repository.findById(id).orElseThrow(() -&gt; new EmployeeNotFoundException(id)); &#125; // 如果用户请求PUT（修改），地址为/employees/&#123;id&#125;，调用该函数 @PutMapping(&quot;/employees/&#123;id&#125;&quot;) Employee replaceEmployee(@RequestBody Employee newEmployee, @PathVariable Long id) &#123; return repository.findById(id) .map(employee -&gt; &#123; employee.setName(newEmployee.getName()); employee.setRole(newEmployee.getRole()); return repository.save(employee); &#125;) .orElseGet(() -&gt; &#123; newEmployee.setId(id); return repository.save(newEmployee); &#125;); &#125; // 如果用户请求DELETE（删除），地址为/employees/&#123;id&#125;，调用该函数 /employees/&#123;id&#125; @DeleteMapping(&quot;/employees/&#123;id&#125;&quot;) void deleteEmployee(@PathVariable Long id) &#123; repository.deleteById(id); &#125;&#125; 异常处理 在我们的后端服务中，一旦出现异常，光进行Java本身的异常处理是不够的，因为这样前端收不到对应的异常消息，没有办法知道接下来要干什么（前端：你这不是摆烂吗这是），因此Spring还提供ExceptionHandler反射机制，当后端产生异常时返回错误信息到前端。 主要步骤 先创建自己的异常类，继承RuntimeException，定义一种异常，如： XXXXException.java 1234567// EmployeeNotFoundException.javaclass EmployeeNotFoundException extends RuntimeException &#123; EmployeeNotFoundException(Long id) &#123; super(&quot;Could not find employee &quot; + id); &#125;&#125; 注意，这个异常类只抛出后端的错误信息，我们还需要让Spring返回前端信息 所以，在该类下再创建一个 XXXXAdvice.java 123456789101112131415161718192021// EmployeeNotFoundAdvice.java// 导入命名空间import org.springframework.http.HttpStatus;import org.springframework.web.bind.annotation.ControllerAdvice;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.bind.annotation.ResponseStatus;@ControllerAdviceclass EmployeeNotFoundAdvice &#123; @ResponseBody // 反射自定义异常类 @ExceptionHandler(EmployeeNotFoundException.class) // 注解，说明返回时向前端回复错误状态码（HttpStatus.NOT_FOUND 即 404） @ResponseStatus(HttpStatus.NOT_FOUND) String employeeNotFoundHandler(EmployeeNotFoundException ex) &#123;\t// 返回的错误信息 return ex.getMessage(); &#125;&#125; 定义XXXXAdvice类时，使用@ControllerAdvice注解，并且在其处理函数（通常命名为XXXXHandler）前，使用@ResponseBody、@ExceptionHandler、@ResponseStatus，它们的作用分别是： 注解 作用 @ResponseBody 说明返回给前端的消息采用json格式 @ExceptionHandler 说明这是一个处理异常的消息（参数填写对应异常类的反射） @ResponseStatus 说明返回的HTTP状态码（可以使用HttpStatus枚举） DTO（Data Transfer Object）支持 添加Spring HATEOAS 在pom.xml中加入以下依赖部分代码即可 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-hateoas&lt;/artifactId&gt;&lt;/dependency&gt; HATEOAS是Spring中用来实现链接（Link）的工具，链接的存在使得客户端可以动态发现其所能执行的动作。 链接的作用 让我们来看一个例子。这是一个普通的json返回包： 12345&#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;Bilbo Baggins&quot;, &quot;role&quot;: &quot;burglar&quot;,&#125; 这是一个带Links的json返回包： 12345678910111213&#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;Bilbo Baggins&quot;, &quot;role&quot;: &quot;burglar&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees/1&quot; &#125;, &quot;employees&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125;&#125; 返回的链接 “_links”部分将告诉前端，用户当前的操作（或是所在的位置）可以跳转到哪些可能的地址，因此前端可以根据这些链接为用户提供更方便的操作。 返回带链接的数据包 注意，以下代码可以简化，写这些的目的是为了更好的理解链接，如果要速成，请跳转到简化生成链接的步骤一节 以下是一个向前端返回带链接的数据的例子： 12345678910@GetMapping(&quot;/employees/&#123;id&#125;&quot;)EntityModel&lt;Employee&gt; one(@PathVariable Long id) &#123; Employee employee = repository.findById(id).orElseThrow( () -&gt; new EmployeeNotFoundException(id)); return EntityModel.of(employee, linkTo(methodOn(EmployeeController.class).one(id)).withSelfRel(), linkTo(methodOn(EmployeeController.class).all()).withRel(&quot;employees&quot;));&#125; 这个Mapping函数和之前的相比，有下面这些不同的地方： 返回值类型由自定义的模型类Employee变为了EntityModel&lt;Employee&gt;。 泛型EntityModel&lt;T&gt;是Spring HATEOAS设计的一种容器，其中不仅能存储数据，还可以存储一系列链接。 当你返回值时，不再只返回一个 Employee employee的模型对象，而是使用 1EntityModel.of(employee,&#123;links&#125;) 返回一个EntityModel类型（包括employee对象和若干个链接） 使用linkTo()方法生成链接： 参数传入一个methodOn()函数，参数为你要链接的方法所在的类的反射，一般为控制类，即XXXXController.class，然后用.点出要连接到的类的方法，如： linkTo(methodOn(EmployeeController.class).one(id)).withSelfRel() 即会加入EmployeeController&#123;&#125;.one(id)方法所在Mapping的URL，即 &quot;localhost:8080/employees/&#123;id&#125;&quot; 可以理解为这是一种反向Mapping，即从调用的方法查找到对应的路由 linkTo()返回一个Link对象，可以用其.withSelfRel()和.withRel(Sting)来修饰其在返回包中的Rel标签 Rel 是 relation 的简写，用来说明链接自身和链接之间的关系 .withSelfRel()返回的链接的Rel标签为“self”：&#123;&#125;，这说明返回的是请求的链接本身。一般来说带链接的返回包都要返回自己，其目的大概和类里面为啥要有this这个对象一样。 .withRel(Sting)返回的链接带有一个用String参数修饰的标签，即“xxxx”：&#123;&#125;，这个标签有助于前端根据标签的名称所以链接。 Spring HATEOAS的各种容器（Model） EntityModel&lt;&gt; 用来容纳单个模型对象及其链接 CollectionModel&lt;&gt;用来容纳多个模型对象及其链接，一个Collection可以包含多个Entity，写为：CollectionModel&lt;EntityModel&lt;T&gt;&gt; 将多个实体模型打包成Collection时，最好将每个模型对象都包装好链接，再为整体的Collection（可以是List、Map或是Dict之类）包装链接，示例如下： 12345678910111213141516@GetMapping(&quot;/employees&quot;)CollectionModel&lt;EntityModel&lt;Employee&gt;&gt; all() &#123;// 这一行写了很多东西，但主要的步骤是：// find所有employee的对象 -&gt; 转换为java8流对象（stream）-&gt; 用map函数把每个// employee的对象换为EntityModel对象，打包链接 -&gt; 将所有单体对象用collect函数// 重新打包进列表\tList&lt;EntityModel&lt;Employee&gt;&gt; employees = repository.findAll().stream() .map(employee -&gt; EntityModel.of(employee, linkTo(methodOn(EmployeeController.class).one(employee.getId())).withSelfRel(), linkTo(methodOn(EmployeeController.class).all()).withRel(&quot;employees&quot;))) .collect(Collectors.toList());//返回CollectionModel对象，打包整体的链接 return CollectionModel.of(employees, linkTo(methodOn(EmployeeController.class).all()).withSelfRel());&#125; ​\t该代码最后返回的json包格式如下： 12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;_embedded&quot;: &#123; &quot;employeeList&quot;: [ &#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;Bilbo Baggins&quot;, &quot;role&quot;: &quot;burglar&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees/1&quot; &#125;, &quot;employees&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125; &#125;, &#123; &quot;id&quot;: 2, &quot;name&quot;: &quot;Frodo Baggins&quot;, &quot;role&quot;: &quot;thief&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees/2&quot; &#125;, &quot;employees&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125; &#125; ] &#125;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125;&#125; 这就是一个非常符合REST规范的资源包了。 PagedModel&lt;&gt;也用来容纳多个模型对象及其链接，并且支持分页，生成PageModel必须使用Page&lt;&gt;类型的泛型对象，在生成PageModel时提供一个Pageable 的分页对象，用来记录分页信息： 1234567891011// PageModel.of()的函数重载类型：// 不传入Link参数，所有对象自动返回空Linkpublic static &lt;T&gt; PagedModel&lt;T&gt; model = PageModel.of (Collection&lt;T&gt; content, @Nullable PagedModel.PageMetadata metadata);// 传入一组Link参数，每个Link与content一一对应public static &lt;T&gt; PagedModel&lt;T&gt; model = PageModel.of (Collection&lt;T&gt; content, @Nullable PagedModel.PageMetadata metadata, Link... links); // 传入一个Link迭代对象，每个Link与content一一对应public static &lt;T&gt; PagedModel&lt;T&gt; model = PageModel.of (Collection&lt;T&gt; content, @Nullable PagedModel.PageMetadata metadata, Iterable&lt;Link&gt;); 带有分页信息的返回包格式如下，可以看到除了CollectionModel的信息外，还另外增加一个”Page“字段，存储分页信息： 123456789101112131415161718192021222324252627282930313233&#123; &quot;_embedded&quot;: &#123; &quot;userDtoList&quot;: [ &#123; &quot;email&quot;: &quot;admin@frogsoft.com&quot;, &quot;username&quot;: &quot;admin&quot;, &quot;roles&quot;: [ &quot;ROLE_USER&quot;, &quot;ROLE_ADMIN&quot; ], &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://127.0.0.1:8080/v1/users/admin&quot; &#125;, &quot;allUsers&quot;: &#123; &quot;href&quot;: &quot;http://127.0.0.1:8080/v1/users/?page=0&amp;size=10&quot; &#125; &#125; &#125; ] &#125;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://127.0.0.1:8080/v1/users?page=0&amp;size=10&quot; &#125; &#125;, &quot;page&quot;: &#123; &quot;size&quot;: 10, &quot;totalElements&quot;: 1, &quot;totalPages&quot;: 1, &quot;number&quot;: 0 &#125;&#125; 可以使用Spring提供的PagedResourcesAssembler类，简化PagedModel&lt;&gt;的生成， 在阅读以下代码之前，先阅读下一节关于RepresentationModelAssembler接口的介绍。 12345678910111213141516// 使用单个EntityModel的RepresentationModelAssembler接口生成PageModel的函数。public PagedModel&lt;EntityModel&lt;T&gt;&gt; getPageModelT(Pageable pageable) &#123; // 获取实体类型T对应的PagedResourcesAssembler类型 private final PagedResourcesAssembler&lt;T&gt; pagedResourcesAssembler; // 获取实体类型T的RepresentationModelAssembler（自行实现） private final XXXXModelAssembler tModelAssembler; // 将多个实体类型T打包为Page&lt;T&gt;类型（可以使用前面提到的JpaRepository中的findAllBy()实现） Page&lt;T&gt; contents = /* find the contents and packed in Page*/ // 调用.toModel返回分页模型，参数分别为打包的Page&lt;T&gt;类型 和 单个T类型对应的RepresentationModelAssembler对象 return pagedResourcesAssembler.toModel(contents, tModelAssembler); &#125; 简化生成链接的步骤 为了减少代码复用，不要在每次创建link时都重复写代码，我们可以写一个函数，将模型对象（比如 Employee）转换为对应的EntityModel&lt;T&gt;对象。这样当我们需要创建link时，只需简单的调用这个方法。 幸运的是，Spring HATEOAS帮你提供了RepresentationModelAssembler接口，通过这个接口，你可以快速创建转换类，而不用自己搭建框架。 实现RepresentationModelAssembler接口的方法如下： 123456789101112131415161718//EmployeeModelAssembler.javaimport static org.springframework.hateoas.server.mvc.WebMvcLinkBuilder.*;import org.springframework.hateoas.EntityModel;import org.springframework.hateoas.server.RepresentationModelAssembler;import org.springframework.stereotype.Component;@Componentclass EmployeeModelAssembler implements RepresentationModelAssembler&lt;Employee, EntityModel&lt;Employee&gt;&gt; &#123; @Override public EntityModel&lt;Employee&gt; toModel(Employee employee) &#123; return EntityModel.of(employee, linkTo(methodOn(EmployeeController.class).one(employee.getId())).withSelfRel(), linkTo(methodOn(EmployeeController.class).all()).withRel(&quot;employees&quot;)); &#125;&#125; 为你的接口实现类起一个 XXXModelAssembler的名字；加入Spring的注入注解@Component；实现接口的泛型参数&lt;Employee, EntityModel&lt;Employee&gt;&gt;表明其是将模型对象XXX转换为EntityModel&lt;XXX&gt; 该接口只需要实现（重载）一个toModel函数，其内容和之前直接在控制器类的函数中创建链接的代码一模一样。 使用该接口对象的方法和repository接口对象的方法类似，只需在控制器中声明其静态引用： 1private final EmployeeModelAssembler assembler; 别忘了在有参构造函数中也为其添加参数，然后只要在返回函数中调用toModel函数即可： 1234567@GetMapping(&quot;/employees/&#123;id&#125;&quot;)EntityModel&lt;Employee&gt; one(@PathVariable Long id) &#123; ... return assembler.toModel(employee);&#125; 之前打包Collection的代码也可以简化为： 123456789@GetMapping(&quot;/employees&quot;)CollectionModel&lt;EntityModel&lt;Employee&gt;&gt; all() &#123; List&lt;EntityModel&lt;Employee&gt;&gt; employees = repository.findAll().stream() .map(assembler::toModel) .collect(Collectors.toList()); return CollectionModel.of(employees, linkTo(methodOn(EmployeeController.class).all()).withSelfRel());&#125; 代码简洁了很多呢！ 使用ResponseEntity返回数据 另一个让你的代码符合REST规范的要点是永远返回合适的Respone 之前返回的json包中的HTTP状态码需要我们自行写入和处理，ResponseEntity则帮我们解决了这个问题，同时支持多种格式的返回包数据。 更新上面的POST函数，以便使用ResponseEntity类来处理返回值： 123456789@PostMapping(&quot;/employees&quot;)ResponseEntity&lt;?&gt; newEmployee(@RequestBody Employee newEmployee) &#123; EntityModel&lt;Employee&gt; entityModel = assembler.toModel(repository.save(newEmployee)); return ResponseEntity // .created(entityModel.getRequiredLink(IanaLinkRelations.SELF).toUri()) // .body(entityModel);&#125; 和不使用ResponseEntity相比，返回值类型变为ResponseEntity&lt;EntityModel&lt;Employee&gt;&gt;，也就是说，要在EntityModel外层，再包装一层ResponseEntity类型的泛型。在不至于混淆的情况下，可以使用ResponseEntity&lt;?&gt; 自动推导类型。 使用ResponseEntity.created()会返回HTTP 201 状态码，表示创建Employee的操作成功。当返回这个内容时，前端通常希望能同时获得该操作所对应的URL地址（Location），以便进行下一步操作。由于我们已经在返回包里打包了Link，因此添加响应头非常方便，代码中.created()的参数：entityModel.getRequiredLink(IanaLinkRelations.SELF).toUri()即是将Link的Self部分写入返回包的头部，最后得到的返回数据如下： 1234567891011121314151617181920&lt; Location: http://localhost:8080/employees/3&lt; Content-Type: application/hal+json;charset=UTF-8&lt; Transfer-Encoding: chunked&lt; Date: Fri, 10 Aug 2018 19:44:43 GMT&lt;&#123; &quot;id&quot;: 3, &quot;firstName&quot;: &quot;Samwise&quot;, &quot;lastName&quot;: &quot;Gamgee&quot;, &quot;role&quot;: &quot;gardener&quot;, &quot;name&quot;: &quot;Samwise Gamgee&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees/3&quot; &#125;, &quot;employees&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125;&#125; ResponseEntity.created()只返回一个特定的状态（201）， 通常情况下，我们使用ResponseEntity.status()返回一个自定义状态码的数据包，状态码可以用HttpStatus枚举表示。 HTTP状态码 HttpStatus枚举 含义 100 HttpStatus.CONTINUE 继续 102 HttpStatus.PROCESSING 处理中 200 HttpStatus.OK 正常 201 HttpStatus.CREATED 已创建 202 HttpStatus.ACCEPTED 已接受 204 HttpStatus.NO_CONTENT 已无内容 302 HttpStatus.FOUND 已找到 400 HttpStatus.BAD_REQUEST 请求错误 401 HttpStatus.UNAUTHORIZED 未授权 403 HttpStatus.FORBIDDEN 禁止访问 404 HttpStatus.NOT_FOUND 无法找到 408 HttpStatus.REQUEST_TIMEOUT 请求超时 429 HttpStatus.TOO_MANY_REQUESTS 请求过多 500 HttpStatus.INTERNAL_SERVER_ERROR 内部服务错误 502 HttpStatus.BAD_GATEWAY 网关错误 503 HttpStatus.SERVICE_UNAVAILABLE 服务不可用 504 HttpStatus.GATEWAY_TIMEOUT 网关超时 1ResponseEntity.status(/* http状态码 */HttpStatus.XXX).body(model); 其他一些常用的状态，ResponseEntity也为我们封装好了以下常用函数： 123456// ok：返回状态码200ResponseEntity.ok().body(model);// badRequest：返回状态码400ResponseEntity.badRequest().body(model);// notFound：返回状态码404ResponseEntity.notFound().body(model); Controller、Service、Repository 三层结构 之前我们提到，Spring用来注册Bean的控制接口@Component根据名称不同又分为三种： @Controller：控制层，标注返回前端的控制组件 @Service：业务逻辑层，标注中间层的控制组件 @Repository：DAO层，标注数据库访问的组件 之所以这么设计，是由编写代码中分层规范决定的，通常Controller层最接近前端，Repository(DAO)层最接近后端（数据库），这样的结构最利于代码解耦化，也利于每个部分的代码更加简洁，接下来我们就来整理项目以适应三层结构。 @Controller控制层 在项目结构中建立一个Controller包，用来处理控制层。 控制层是最接近前端的部分，所以要适应可能随时改变的前端需求。因此最好做好版本控制，可以将Controller包命名为/Controller.v&#123;版本号&#125;。当需要修改需求时，可以保留旧版本，重新建立新版本的Controller包，以实现向后兼容。 控制层要实现的内容： 根据设计好的API，建立若干控制类：XXXXController，放置于/Controller/api包下。 若API需要接受前端请求的数据比较复杂，需要单独用一个数据类存储，可以建立对应的数据类XXXXRequest，放置于/Controller/request包下。 控制类中要做的事情： 路由处理：用@RequestMapping索引（这个模块的）根地址（可以写在class的定义前），然后用@Get/Post/Put/DeleteMapping索引子地址 权限处理：判断访问请求是否具有特殊的权限： token鉴权：从请求中获取username，与授权系统中的列表匹配 Roles判断：函数前加入注解@RolesAllowed(&quot;身份名&quot;)，自动拒绝没有对应身份权限的用户的访问，判断用户的身份在登陆时由鉴权系统实现 处理请求数据： 读请求数据：函数参数中配置@RequestParam和@PathVariable，获得请求或URL中的数据 读写数据：调用Service层的接口对象 注意，对于数据的具体处理要在Service层实现，Controller层只简单处理和传输参数，并读取结果（Entity对象） 返回状态码，用ResponseEntity.status()实现 @Service服务层 服务层，即作为一个承上启下的中间部分，用来处理数据、打包数据。 服务层的基本结构： 建立若干服务接口，在/service文件夹下，如果功能细分需要多个功能模块，也可以建立子文件夹 服务接口，以XXXXService命名，接口的实现类以XXXXServiceImpl命名，使用接口的目的是为了多个类访问同一个数据仓库时方便重载和复用。 服务实现类中要做的事情： 读写数据：调用Repository层的接口对象，处理数据 读数据：使用findBy系列方法（在Repository层定义） 写数据：使用save方法 打包数据：将要返回的数据通过RepresentationModelAssembler接口对象（这也属于持久层）打包为Entity对象，返回给控制层 @Repository持久层 持久层，用来和数据库进行直接连接。在这里我们使用JPA来建立持久层，数据库连接变得十分方便，但要满足DTO数据模型的要求，因此持久层分为Repository和Dto两个平行层 持久层的基本结构： 建立若干仓库接口，以XXXXRepository命名，接口要继承自JpaRepository，以实现数据库连接，接口无需实现（Spring金牌服务，帮您实现！） 建立Dto文件夹，其包含三个部分： model文件夹：建立Dto数据类，Dto数据类的命名在基本数据实体类后加Dto即可（如UserDto） Dto数据类的成员变量内容和基本数据类一模一样，但无需写与数据库连接的注解，数据的连通由mapper类搞定 mapper文件夹：对于每个Dto类，建立一个Mapper类，以XXXXMapper命名。 mapper类实现一个函数，将基本数据实体类（如User）转化为Dto数据类（如UserDto） mapper函数的实现方法：new一个Dto对象，然后将使用setter函数配置所有成员即可。之所以在mapper中new对象，也是处于方便解耦和类嵌套之类的目的 assemler文件夹，对于每个Dto类，建立一个Assembler类，以XXXXModelAssembler命名。该类实现RepresentationModelAssembler接口。 注意，这里实现RepresentationModelAssembler接口时，传入的实体对象要是已经打包好的Dto类对象，不然我费这么大劲打包Dto干啥（感觉在说废话） Assembler类即主要实现toModel函数，实现将Dto数据打包为Entity实体对象，这样，服务层只需调用对应的接口实现类，即可以获得符合REST规范的返回数据包 对于基本数据实体类，建立一个\\Model文件夹存储，使用@Entity实现类与数据库的ORM连接。这些即为持久层所需处理的部分。","tags":["java","Spring"]},{"title":"关于","path":"/about/index.html","content":"这是一个关于页面 TODO list: 自我介绍、收藏夹、语录集"},{"title":"友链","path":"/friends/index.html","content":"See also my friends! Geeks RiddNOAM CHI Artists Waldo Yang"},{"title":"笔记分类页","path":"/notes/index.html","content":"正在施工中…"}]