[{"title":"MuG Diffusion v1.0 源码解析","path":"/2023/12/27/MuG-Diffusion-v1-0-源码解析/","content":"警告，包含大量GPT4分析和代写内容，可能存在谬误 原仓库地址：https://github.com/Keytoyze/Mug-Diffusion （1）数据集准备部分 数据准备过程概况如下： 把下载好的数据集（osu谱面）放在data/文件夹下，可以准备一个子文件夹 数据集文件夹下新建一个beatmap.txt，记录谱面元数据，用于下一步 按照configs/mug/mania_beatmap_features.yaml中的配置，抽取谱面的特征 谱面特征需要转化为数据表，存放与beatmap.txt同一目录下的features.db数据库中 至少准备两个这样的数据集：一个训练集一个验证集 [mug/data] mug/data/convertor.py: 工具类，用来解析单个.osu谱面文件 mug/data/dataset.py: 构建用于pytorch训练的数据集格式，在此之前，请确保已经抽取了谱面特征，并存放在features.db中 mug/data/utils.py: （待分析）一些用于对齐的工具函数 准备好的数据集条目示例： 123456789101112131415161718192021222324252627&#123; &quot;meta&quot;: &#123; &quot;path&quot;: &quot;data/beatmap_4k/1469980 Silentroom vs. Frums - Aegleseeker/Silentroom vs. Frums - Aegleseeker ([Crz]Alleyne) [The Last Observation].osu&quot;, &quot;audio&quot;: &quot;data/beatmap_4k/1469980 Silentroom vs. Frums - Aegleseeker/audio.ogg&quot;, &quot;game_mode&quot;: 3, &quot;cs&quot;: 4.0, &quot;version&quot;: &quot;The Last Observation&quot;, &quot;set_id&quot;: 1469980 &#125;, &quot;convertor&quot;: &#123; &quot;frame_ms&quot;: 46.439909297052154, &quot;max_frame&quot;: 4096, &quot;mirror&quot;: False, &quot;random&quot;: False, &quot;mirror_at_interval_prob&quot;: 0, &quot;offset_ms&quot;: 0, &quot;rate&quot;: 1.0 &#125;, &quot;note&quot;: array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), &#x27;valid_flag&#x27;: array([1., 1., 1., ..., 0., 0., 0.]), &#x27;audio&#x27;: array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), &#x27;feature&#x27;: array([ 24, 39, 41, 53, 55, 58, 81, 110, 124, 142, 157, 179, 192, 212, 225, 237, 259, 275, 293, 314, 327])&#125; note：谱面音符的数据（文本 -&gt; 向量） feature：谱面特征的数据，osu元数据和ett分数等 （文本 -&gt; 向量） audio：音频的张量数据 （有一个问题是，模型是怎么处理长度不同的音频的？） valid_flag：？一个标记向量，用于标记谱面中的音符是否有效，即是否在音频中有对应的音符？ 12345# 数据集中各张量的shape[&#x27;note&#x27;].shape: (16, 4096)[&#x27;audio&#x27;].shape: (128, 32768)[&#x27;feature&#x27;].shape: (21,)[&#x27;valid flag&#x27;].shape: (4096,) 处理音频的缓存.npz文件放置于data/audio_cache文件夹中。 [scripts] 如果我的判断没错，准备数据这个阶段时完全手动的，看到这个文件夹下的脚本都没有被引用过…… scripts/prepare_beatmap.py: 用本地下载的osu谱面于制作beatmap.txt，给定输入输出文件夹 scripts/prepare_ranked_beatmap.py: 从osu网站上爬取谱面，制作beatmap.txt，需要用到kuit自己的网站服务 scripts/prepare_beatmap_from_ranking_mapper.py: 同上，不过似乎是从特定的谱师id中获取 scripts/prepare_beatmap_features.py: 从beatmap.txt中读取谱面元数据，抽取谱面特征，并提交到features.db中 抽取特征需要以下参数： 1234567beat_map_path = beatmap.txt 文件的路径，记得铺面文件的存放位置与该txt位于同一目录features_yaml = 配置feature文件的路径，如&quot;configs/mug/mania_beatmap_features.yaml&quot;osu_tools = 一个osu官方开源的计算osu谱面信息的工具，在这里下载：https://github.com/ppy/osu-tools/tree/master/PerformanceCalculator获得源码后，需要使用dotnet编译（参见仓库readme），编译完成后，在\\PerformanceCalculator\\bin\\Debug et6.0\\下可以找到PerformanceCalculator.dll文件，使用该文件的路径作为参数ranked_map_path = 存储每个谱面的rank（谱面审核）信息，该信息需要通过预处理获取，如果没有，可以指定为Nonedotnet_path = 为了运行PerformanceCalculator.dll，需要安装.NET6.0，安装并设置完成环境变量后，可直接使用填入&quot;dotnet&quot; scripts/filter_beatmap.py: （待分析）似乎是用于过滤谱面中的变速 …… 其他脚本似乎是用于处理Etterna谱面的，或者是一些用于算法测试的（破案了，是原来stable diffusion的一些测试代码），跳过这一部分 数据集配置 如果你要训练，必须准备基本的两个数据集文件夹： mug.data.dataset.OsuTrainDataset mug.data.dataset.OsuVaildDataset 将数据集路径指定在训练用的配置文件中，目前的路径是configs/mug/mug_diffusion.yaml， 同时最好有一个mug.data.dataset.OsuTestDataset数据集，用于测试. 训练数据集示例结构： TODO （2）VAE编解码部分 这部分的目的是为了得到两个模型： Encoder：将谱面信息编码为一个向量 Decoder：将一个向量解码为谱面信息 [mug/firststage] 这部分用于实现AutoEncoder，自动编码器是一种无监督学习的神经网络模型，主要用于学习输入数据的压缩表示 AutoEncoder的结构如下： 其中包含两个torch.nn.Module类的组件：编码器（Encoder）和解码器（Decoder）。编码器的作用是将输入数据压缩为一个低维度的表示，而解码器则将这个低维度的表示恢复为原始的高维度数据 注意：mug/firststage/autoencoder.py中的部分模型使用的网络模块定义在mug/model/modules.py中 Encoder模型结构： 输入通道数C=16 经过一个3*3的卷积层，输出中间通道数64 经过一系列下采样操作：在每次下采样之前，先经过一个残差块，并让每两次下采样后，网络的通道数翻倍 即: 64 * [1,1,2,2,4,4] 下采样网络：可选 卷积 或 平均池化 卷积：3*3卷积层，stride=2 平均池化：2*2平均池化层，stride=2 中间层：使用2个残差块，每个残差块包含两个3*3卷积层，通道数不变 输出层：首先通过一个归一化层对数据进行归一化处理，然后通过一个33卷积层将数据的通道数转换为编码器的输出通道数，输出 目标输出通道数2 的向量表示 如：目标输出通道数为32，则输出64维的向量表示 Decoder模型结构： 与Encoder模型结构正好相反，先将输入向量卷积到中间层，然后再经过一系列上采样操作，最后经过归一化和一个3*3卷积层，输出目标输出通道数的向量表示 损失函数：AutoEncoderKL类使用了两个损失函数，分别是重构损失和KL散度损失 重构损失：在mug/firststage/losses.py中定义，计算重构的谱面数据与原始谱面数据的误差，包括多种误差计算方式 KL损失：计算编码器输出的均值和方差与标准正态分布的KL散度 KL损失在加入总损失时，需要乘以一个权重系数self.kl_weight，默认为0（不考虑KL损失） AutoEncoder模型的使用： 参见下面diffusion的部分 （3）Diffusion训练部分 条件嵌入模型 [mug/cond] mug/cond这个文件夹里实现的都是条件编码模型，即将谱面的特征提示信息和音频信息一起编码，然后输入Diffusion模型的去噪过程，用来控制输出结果的条件向量。 特征输入：特征定义在符合yaml格式的文本上，里面的信息包括如 难度sr，long note比例，键形特征等，训练时特征存储在数据集的database里，推理是可以手动输入，或是从一张想要参考的谱面中提取出来 音频输入：通过频谱变换离散化后的音频数据，有多种处理方式 noise level：[不知道这个是在哪里被嵌入的……] feature_cond.py - BeatmapFeatureEmbedder 谱面特征（提示）嵌入 主要模型为BeatmapFeatureEmbedder的类，该类继承自torch.nn.Module，用于实现特征编码和嵌入。 这里嵌入的特征是谱面元数据，即configs/mug/mania_beatmap_features.yaml中定义的特征。该文件路径需要作为参数传入的构造函数中。 读入mania_beatmap_features.yaml文件的信息后，会使用mug/util.py中的count_beatmap_features函数，计算每个特征需要在嵌入向量中用几个值来表示，例如，一个bool类型的特征需要3个值（表示True/False或者没有这个特征）。 使用 torch.nn.Embedding创建一个嵌入层self.embedding，其维度为传入的参数的embed_dim。 12345def __init__(self, path_to_yaml, embed_dim): super().__init__() with open(path_to_yaml) as f: self.feature_dicts = yaml.safe_load(f) self.embedding = torch.nn.Embedding(count_beatmap_features(self.feature_dicts), embed_dim) forward方法是模型的前向传播过程。它接收一个输入x，将其转换为长整型，然后通过嵌入层进行转换。最后，使用rearrange函数调整张量的维度顺序。 pattern解释： 输入时二维向量，其形状为[B, F]，其中B代表批次大小，F代表特征数量。 经过嵌入层后，输出的形状为[B, F, H]，其中H代表嵌入向量的维度，即参数embed_dim。 123def forward(self, x): x = rearrange(self.embedding(x.long()), &quot;b f h -&gt; b h f&quot;) # [B, H, F] return x summary方法使用torchsummary库的summary函数，打印出模型的概要信息，包括输出大小、参数数量、内核大小等。 123456def summary(self): import torchsummary torchsummary.summary(self, input_data=[5, ], dtypes=[torch.long], col_names=(&quot;output_size&quot;, &quot;num_params&quot;, &quot;kernel_size&quot;), depth=10, device=torch.device(&quot;cpu&quot;)) 测试：输入一个特征配置文件，打印出模型的概要信息。 123if __name__ == &#x27;__main__&#x27;: BeatmapFeatureEmbedder(path_to_yaml=&quot;configs/mug/mania_beatmap_features.yaml&quot;, embed_dim=128).summary() wave.py - 音频数据嵌入 这里定义了STFTEncoder，MelspectrogramEncoder等模型，主要实现将音频信息编码为嵌入向量 STFTEncoder类是一个音频编码器，使用了短时傅里叶变换（STFT）进行音频编码。 MelspectrogramEncoder和MelspectrogramEncoder1D类是另外两种音频编码器，使用了梅尔频谱图（Melspectrogram）进行音频编码。 S4BidirectionalLayer类是一个双向S4模型层，它包含了一个归一化层和一个双向S4模型。 TimingDecoder类是一个解码器，它用于将编码后的音频数据解码回原始数据 MelspectrogramScaleEncoder1D类是一个一维的梅尔频谱图编码器，它使用了注意力机制进行音频编码。 实际模型训练时，通过指定yaml配置文件中的wave_stage_config字段选择音频编码模型。 在已知的两种的配置中，编码音频分别使用了STFTEncoder和MelspectrogramScaleEncoder1D模型，分别对应配置文件mug_diffusion_stft.yaml 和mug_diffusion.yaml LDM/DDPM模型 - [mug/diffusion] 模型训练使用的是Pytorch-Lightning框架，这是一个pytorch深度神经网络api的抽象和包装。它的好处是可复用性强，易维护，逻辑清晰等。 主要的代码结构(本项目主要在mug目录下)： 12345678-mug/ |-data/ |-... 数据集的中间接口 |-model/ |-... 模型的中间接口 -lr_scheduler.py -util.py-main.py 定义命令行参数，实例化数据集、模型、Trainer，开始训练 diffusion.py - DDPM模型 mug/diffusion/diffusion.py是扩散模型的核心文件，根据DDPM论文的方法实现了整个框架模型，因此类名也叫DDPM： DDPM继承了pl.LightningModule类。一个继承了pl.LightningModule的类被称为Lightning Module，需要实现以下三个核心的组件： 1 模型：self.model，训练用的模型网络，可嵌套torch.nn.Module类型 2 优化器：实现configure_optimizers()函数，其返回值可以是一个优化器或数个优化器，又或是两个List（优化器，Scheduler） 3 训练步骤：实现以下函数： forward()函数，定义前向转播过程，返回如何计算损失，这是用来做梯度下降的 training_step()函数，这个函数其实是包装了一个pytorch的训练过程（就是一个for循环，如果你学过基础的pytorch神经网络实现的话） 其接收一个batch的来自训练数据集的数据，然后，通常这个函数会调用类自身（self(batch)），来通过已定义的模型对整个批量进行一步推理， 然后根据forward()中的定义计算loss损失。 需要返回一个字典，包含loss和log两个字段。loss返回损失，log则是可选的，记录一些需要记录的信息，loss字段一定要存在， 否则pytorch lightning会认为此batch是无效的，从而被跳过训练部分。 validation_step()函数：[可选]。功能和training_step()函数一样，不过是在验证过程中调用的，不会去更新模型的权重，只输出log信息， 主要是给你一个工具，用来自定义输出验证过程中要观察的变量的。 training_epoch_end()函数：[可选]。这个函数是在每个epoch结束时调用的，用于记录一些信息，比如训练集的平均损失等。 回到DDPM类本身，在定义模型时，使用的语句是： 12# mug/diffusion/diffusion.pyself.model = MugDiffusionWrapper(unet_config, first_stage_config, wave_stage_config, cond_stage_config) 这里，额外定义了一个MugDiffusionWrapper类，用来包装MuG Diffusion模型中包含的四个子模型，分别是： cond_stage_model：前文所述的BeatmapFeatureEmbedder模型，用来得到【谱面提示】的压缩表示（由可读数据压缩到向量） 谱面数据包含 osu的谱面元数据（难度等）和 通过minacalc计算得到的谱面提示（各个键形的分布、键形难度等），不包含note数据 wave_model：前文所述的STFTEncoder或MelspectrogramScaleEncoder1D模型，用来编码和解码【音频波形】（由音频数据到条件向量？） first_stage_model：前文所述的AutoEncoderKL模型，用来编码和解码 【谱面note数据】（从 向量 编码到 潜空间向量 和 从 潜空间向量 解码） unet_model：与Stable Diffusion类似的U-Net结构，用来学习反向扩散去噪（Denosing）的步骤 即，unet_model接受潜空间的两种输入：1、【潜空间的谱面向量数据】2、【条件嵌入向量，包括音频和谱面特征（即“提示 prompt”）】， 通过多步扩散迭代，学习正向扩散参数，或是反向扩散输新的谱面向量表示。 在训练时，unet_model的输入是一个【训练数据集的谱面潜空间向量】和【该谱面对应的音频向量、提示向量】，输出是一个【潜空间向量】， 每一次迭代，都会使得【潜空间向量】接近于从高斯分布采样的【噪声数据】，并同时更新网络权重 在推理生成新谱面时，unet_model的输入是一个【从高斯分布种采样的噪声谱面向量】和【用户想要的谱面音频和特征的输入向量】，通过多次反向扩散， 得到生成的【谱面潜空间向量】，通过first_stage_model解码得到可读的【谱面note数据】 每个模型的详细结构在下一个小节描述 对比：Stable Diffusion（v1）的模型模块和 MuG Diffusion的模型模块 输入编码器（条件嵌入） 潜空间扩散网络 输出解码器 Stable Diffusion 基于 CLIP 模型的文本编码器 U-Net AutoEncoder（图像） MuG Diffusion 基于STFT/Mel的音频编码器 + 谱面提示嵌入 U-Net（魔改） AutoEncoder（谱面Note） DDPM使用的优化器是AdamW，如果yaml配置文件中定义了学习率调度器，还会使用Pytorch的LambdaLR创建一个学习了调度器。 1234567# mug/diffusion/diffusion.pyopt = torch.optim.AdamW(params, lr=lr)...if self.use_scheduler: assert &#x27;target&#x27; in self.scheduler_config scheduler = instantiate_from_config(self.scheduler_config)... 在MuG Diffusion的两种配置文件里，mug_diffusion.yaml中定义了学习率调度器为Stable Diffusion使用的 ldm.lr_scheduler.LambdaLinearScheduler，而mug_diffusion_stft.yaml中没有定义学习率调度器 unet.py - U-Net模型 mug/diffusion/unet.py中定义了U-Net模型，这个模型是DDPM的核心，用于进行潜空间的扩散和反向扩散。 wave_model、cond_stage_model和first_stage_model（即VAE）的输出都将作为U-Net的输入，下面从模型输入和输出的shape简要描述一下模型。 1234567891011# mug/diffusion/diffusion.py ... def forward(self, x, t, c, w): &quot;&quot;&quot; x 对应 UNetModel 的 x 参数，表示输入的特征张量。 t 对应 UNetModel 的 timesteps 参数，表示时间步长。 c 对应 UNetModel 的 context 参数，表示上下文信息[谱面提示]，用于条件生成 *w 对应 UNetModel 的 *audios 参数，表示音频数据，用于音频处理 &quot;&quot;&quot; return self.unet_model(x, t, c, *w) ... 整个网络的结构遵循UNet的典型编码器-解码器结构，通过逐渐增加和减少特征图的尺寸来学习丰富的特征表示，最终输出与输入相同尺寸的张量。 U-Net模仿了Stable Diffusion中的U-Net结构，包括注意力层等，但是做了一些改动，主要是为了适应【音频数据具有天然的时序性】。 Stable Diffusion U-Net的结构参见（图源水印）： 因此，U-Net模型中可以选择加入下面的部分来增强模型处理序列数据的能力。 LSTM层，使用LSTM模型处理时序信息，这也是参考已有生成谱面的工作，大多采用了LSTM块来设计网络。 S4层：使用Structured State Space（S4）模型，是一种基于状态空间模型（SSM）的新型序列建模方法。它本质上是连续时间的，并且在建模数据中的长期依赖性方面表现出色。 模型结构流程描述： [ i ] 首先进行一个卷积，将输入张量数据（谱面note数据）的通道数转换为model_channels （model_channels是配置文件中定义的参数(128)，代表模型的通道数）。 [ ii ] Downstamps（input_blocks） 下采样:&quot;U“形状的左半边，每次通过两个网络模块： 1、AudioConcatBlock，将当前分辨率等级的音频通道数直接加到到模型的通道数中，并连接音频数据和输入数据。 音频通道数对于不同的采样分辨率分别是[256,512,512,512] 2、一个ResBlock（残差块），其中包含三层： 第一 TimestepResBlock，和Stable Diffusion的方法一样，将时间步数据嵌入到残差块中（没细看，不知道这里有没有魔改…… 第二 ContextualTransformer（自定义注意力层）， 第三：按照配置中的指定，在注意力层中后面额外添加一个LSTM层或S4层。 每个TimestepResBlock都会从外部连接一个timestep_emb，作为时间信息的编码。这里采用了和Stable Diffusion一样的策略， timestep_emb的长度为model_channels的4倍。 每个ContextualTransformer层都会从外部连接一个相同的context张量，这个就是之前传入的c，从BeatmapFeatureEmbedder获得的谱面特征提示数据 在原版Stable Diffusion里，注意力块名称是SpatialTransformer，因为生成图像的文本提示contex是不定长的，而这里生成谱面的提示是定长的， 且不需要进行2D卷积。因此这里重新写了一个ContextualTransformer，二者都是从BasicTransformerBlock继承而来。 每次通过完整的input_block以后，进行一次下采样，倍增模型的通道数（按照配置，倍率为[ 1,2,4,4 ]），压缩数据的维度和注意力层的分辨率 [ iii ] middle_block: &quot;U&quot;形状的底部，也是模型的中间层，与Stable Diffusion的结构仍然相同，通过了一个三明治形状的TimestepResBlock-ContextualTransformer-TimestepResBlock网络块。 [ iv ] Upstamps（output_blocks） 上采样：&quot;U&quot;形状的右半边，老样子，与下采样相反即可，每次通过相同的TimestepResBlock-ContextualTransformer-LSTM/S4层后进行一次上采样。 [ v ] 输出层：上采样结果先通过一个归一化层和一个SiLU激活函数，然后最后通过一个卷积层将特征映射到目标输出通道数，over。 好奇：这里和原版SD一样，用了个zero_module来初始化卷积层，不知是什么优化技巧…… 各个模型的input size和Output size： 下表默认省略了batch size，即应该添加在各个input size前面的批量维度B 模型 Input size Output size 备注 (VAE)Encoder (16, 4096) (64, 128) 16是谱面note数据的输入通道数，它是由“4K”按键的四个位置，每个位置分配4个通道构成的。经过多次下采样后得到形状256*128，然后通过卷积压缩到潜空间的形状为64*128 (VAE)Decoder (64, 128) (16, 4096) 与上面相反，多次上采样后得到形状64*4096, 然后通过卷积恢复到谱面形状16*4096 BeatmapFeatureEmbedder (f) (f, 128) f是feature的数量？（数据集中一个谱面的feature长度为21） MelspectrogramScaleEncoder1D (128, 32768) (512, 64) 32768代表最大的序列长度（梅尔频谱图的长度），和下面的傅里叶变换相比，是将两个音频通道混合到一起，数量*2 STFTEncoder (2, 1025, 16384) (32, 256) 2=输入通数道（对应复数的实部和虚部），1025=频率分辨率，16384=最大的序列长度，输出形状中的32是输出通道数 U-Net(使用STFT) (544, 4096) (32, 4096) 在这个方法中，音频通道和输入通道（32）一开始就通过直接连接合并到一起，544 = 512+32 (我也不知道为什么使用STFT的通道数多一倍，对不起我没学过信号处理QAQ) U-Net(使用MelScaleEncoder1D) (16, 4096) (16, 4096) x是从噪声分布中采样的，与谱面note数据形状相同的随机输入。 而提示数据、时间步数据和音频数据都是从外部嵌入的。 ddim.py - DDIM模型 DDIM是DDPM的一个改进模型，用于推理时快速采样x，貌似训练时没有用到。 训练入口[main.py] 加载模型：model = instantiate_from_config(config.model),config里配置的模型即为DDPM模型 加载数据： 123456data = instantiate_from_config(config.data)data.prepare_data()data.setup()print(&quot;#### Data #####&quot;)for k in data.datasets: print(f&quot;&#123;k&#125;, &#123;data.datasets[k].__class__.__name__&#125;, &#123;len(data.datasets[k])&#125;&quot;) 开始训练，使用trainer.fit()函数，传入模型和数据集 12345678if opt.train: try: trainer.fit(model, data) except Exception: melk() raiseif not opt.no_test and not trainer.interrupted: trainer.test(model, data) （4）WebUI和推理部分 交互式推理入口[webui.py] TODO"},{"title":"生成模型基础与应用笔记-第4章","path":"/2023/12/19/生成模型基础与应用笔记-第4章/","content":"C4 连续数据的生成模型 本文的部分公式格式未校对，正在施工中…… C4 连续数据的生成模型 生成分类器 在C2中，我们提到，生成模型也是一个分类模型，通过对离散变量的贝叶斯建模，可以得到 朴素贝叶斯分类器（NBC），事实上，这一点也可以推广到连续数据。只需将概率建模改为“类条件密度函数”： 记好这个结论，在后面两节我们会使用其来训练一个** 多元高斯分布** 分类器 单一正态分布的高斯模型 这一节我们专注于高斯分布（正态分布），因为它是连续变量中的最常见的建模。 我们先一句带过 一元正态分布，对没错就是这个： 它的概率密度函数是N(x∣μ,σ2)=12μσ2e−(x−μ)22σ2N(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\mu\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}N(x∣μ,σ2)=2μσ2​1​e−2σ2(x−μ)2​，大家应该都知道吧。 实际上，多元连续变量也是具有正态分布的（概率论里其实也学过，不过大概大家都忘了）： 其中，我们把 (x−μ)TΣ−1(x−μ)(x-\\mu)^T\\bold \\Sigma^{-1}(x-\\mu)(x−μ)TΣ−1(x−μ) 称为** 马氏距离 DMD_MDM​，这是欧式距离在多元空间中的推广：** DM=(x−μ)TΣ−1(x−μ)D_M = (\\bold x-\\mu)^T\\Sigma^{-1}(\\bold x-\\mu)DM​=(x−μ)TΣ−1(x−μ) 下面我们提这样一个问题：给定N个独立同分布的样本xi∼N(μ,Σ)\\bold x_i \\sim N(\\bold \\mu, \\bold \\Sigma)xi​∼N(μ,Σ)，如何估计该多元高斯分布的参数呢？ 同样，可以使用MLE和MAP两种估计方法。 极大似然估计 似然函数： L(μ,Σ)=logP(x∣μ,Σ)=∑i=1NlogP(xi∣μ,Σ)=∑i=1Nlog1(2π)D/26∣Σ∣1/2e−12∑i=1NDM(xi)L(\\mu,\\Sigma) = log P(\\bold x|\\mu,\\Sigma) \\\\ = \\sum_{i=1}^NlogP(\\bold x_i|\\mu,\\Sigma) \\\\ = \\sum_{i=1}^Nlog\\frac{1}{(2\\pi)^{D/2}6|\\Sigma|^{1/2}}e^{-\\frac{1}{2}\\sum_{i=1}^N D_M(x_i)}L(μ,Σ)=logP(x∣μ,Σ)=∑i=1N​logP(xi​∣μ,Σ)=∑i=1N​log(2π)D/26∣Σ∣1/21​e−21​∑i=1N​DM​(xi​) 最大化时的参数结论： 最大后验概率估计 似然函数 L(μ,Σ)=logP(x∣μ,Σ)=(2π)−ND2∣Σ∣−N2e−12∑i=1NDM(xi)L(\\mu,\\Sigma) = log P(\\bold x|\\mu,\\Sigma) = (2\\pi)^{-\\frac{ND}{2}}|\\Sigma|^{-\\frac{N}{2}}e^{-\\frac{1}{2}\\sum_{i=1}^N D_M(x_i)}L(μ,Σ)=logP(x∣μ,Σ)=(2π)−2ND​∣Σ∣−2N​e−21​∑i=1N​DM​(xi​) 其中，对于∑i=1NDM(xi)=∑i=1N(xi−μ)TΣ−1(xi−μ)\\sum_{i=1}^N D_M(x_i) = \\sum_{i=1}^N (x_i-\\mu)^T\\Sigma^{-1}(x_i-\\mu)∑i=1N​DM​(xi​)=∑i=1N​(xi​−μ)TΣ−1(xi​−μ)，可以用采样数据x的均值xˉ\\bar xxˉ和散度SxˉS_{\\bar x}Sxˉ​表示为： ∑i=1NDM(xi)=tr(Σ−1Sxˉ)+N(xˉ−μ)TΣ−1(xˉ−μ)\\sum_{i=1}^N D_M(x_i) = tr(\\Sigma^{-1}S_{\\bar x}) + N(\\bar x - \\mu)^T \\Sigma^{-1} (\\bar x - \\mu)∑i=1N​DM​(xi​)=tr(Σ−1Sxˉ​)+N(xˉ−μ)TΣ−1(xˉ−μ) ,其中Sxˉ=∑i=1N(xi−xˉ)(xi−xˉ)TS_{\\bar x} = \\sum_{i=1}^N(x_i - \\bar x)(x_i - \\bar x)^TSxˉ​=∑i=1N​(xi​−xˉ)(xi​−xˉ)T 最大化时的参数结论： 因为是MAP，需要引入先验的参数： m0=xˉ\\bold m_0 = \\bar xm0​=xˉ，即先验均值 k0k_0k0​：对m0\\bold m_0m0​的信任程度 ，为0或一个极小值 S0=diag(Sxˉ)N\\bold S_0 = \\frac{diag(S_{\\bar x})}{N}S0​=Ndiag(Sxˉ​)​，协方差的先验，并取平均 v0v_0v0​：对S0\\bold S_0S0​的信任程度 ，v0=D+2v_0 = D+2v0​=D+2，D是维度 然后对先验均值和极大似然估计的结果做** 凸组合 ，定义：** \\bold m_N = \\frac{k_0}{k_0+N}\\bold m_0 + \\frac{N}{k_0+N}\\bar x \\\\ \\ \\\\ \\bold S_N = \\bold S_0 + S_{\\bar x} + \\frac{k_0 N}{k_0+N}(\\bar x - \\bold m_0)(\\bar x - \\bold m_0)^T \\\\ = \\bold S_0 + S_{\\bar x} + k_0\\bold m_0\\bold m_0^T-K_N \\bold m_N\\bold m_N^T \\\\ \\ \\\\ 其中： k_N = k_0 + N \\\\ v_N = v_0 + N 则估计结果为： - &#123;% image https://cdn.nlark.com/yuque/0/2023/png/23169257/1699340522524-968b4f6e-de54-4446-a436-bef4db536cf0.png#averageHue=%23fbf8f7&clientId=u0e63a603-818a-4&from=paste&height=246&id=u220176e4&originHeight=472&originWidth=919&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=165465&status=done&style=none&taskId=u9c62d9f6-5344-4492-a011-0e9d87c8828&title=&width=479.66668701171875 %&#125; 高斯判别分析 上一节我们已经学习了如何以 多元高斯分布 为随机变量建模贝叶斯生成模型，结合第一节所说，下面我们尝试利用其生成一个分类器。 假设我们有多个训练样本数据x1,x2,...x_1,x_2,...x1​,x2​,...，它们对对应的类别分别是 y=1,2,...Cy=1,2,...Cy=1,2,...C 那么可以定义当条件为“随机变量被分为类别c”的条件密度函数： p(x∣y=c,θ)=N(x∣μc,Σc)p(\\bold x|y=c, \\theta) = N(\\bold x |\\mu_c, \\Sigma_c)p(x∣y=c,θ)=N(x∣μc​,Σc​) 其中μc,Σc\\mu_c, \\Sigma_cμc​,Σc​是 所有被分为类别c的训练样本数据构成的多元高斯分布 的 均值 和 协方差矩阵 按照生成分类器的 后验估计最大 为原则，和离散生成分类器的输出类似，可以得到分类的计算公式： 其中，π\\piπ是分类先验的分布 如果对于类别c，先验分布πc\\pi_cπc​是均匀的 ，则推导公式为： 如果 x 是二元变量，那么y(x)的分类结果（决策边界）在平面图上会呈现一条二次曲线，因此，也将这种方法称为“二次判别分析”）下图是一个这样的例子： 在这个图示中，样本变量x是一个二元的变量，因此被绘制在平面上，每个点还对应一个y值，图中的圆圈表示的是y值的等高线。 x相对y的值服从正态分布，直观地绘制出三维和二维的对比图，其实是这样的： 如果对于任意的类别c，有Σc=Σ\\Sigma_c = \\SigmaΣc​=Σ，则决策边界恰好是一条直线，此时“二次判别分析”退化为“ 线性判别分析 ”,可以直接写出决策线的表达式： logP(y=c∣x,θ)∝βcx+γclogP(y=c|\\bold x,\\theta) \\propto \\beta_c \\bold x + \\gamma_clogP(y=c∣x,θ)∝βc​x+γc​ 其中参数矩阵βc=(Σ−1μc)T\\beta_c = (\\Sigma^{-1}\\mu_c)^Tβc​=(Σ−1μc​)T,γc=−12μcTΣ−1μc+logπc\\gamma_c = -\\frac{1}{2}\\mu_c^T\\Sigma^{-1}\\mu_c+log\\pi_cγc​=−21​μcT​Σ−1μc​+logπc​，下图是一个这样的二元变量线性分类的例子： 我们可以看到，多元高斯分布分类器（判别器）的模型参数就是高斯分布的参数： θc=(μc,Σc)\\theta_c = (\\mu_c, \\Sigma_c)θc​=(μc​,Σc​)，基于上一节的结论，直接使用极大似然估计来计算参数。 先计算对数似然函数，其即是 对每个分类的多元高斯分布 做 加权平均（没错这和离散情况的下的贝叶斯分类器其实是一样的）： logP(D∣θ)=∑i=1N∑c=1C(logπc)if(Yi=c)+∑c=1C[∑i:yi=clog N(x∣μc,Σc)]log P(D|\\theta) = \\sum_{i=1}^N \\sum_{c=1}^C (log \\pi_c)^{if(Y_i =c)} + \\sum_{c=1}^C[\\sum_{i: y_i=c}log \\ N(\\bold x| \\mu_c, \\Sigma_c)]logP(D∣θ)=∑i=1N​∑c=1C​(logπc​)if(Yi​=c)+∑c=1C​[∑i:yi​=c​log N(x∣μc​,Σc​)] 右侧的N即为类别c对应的多元高斯分布的概率密度函数。 而πc\\pi_cπc​表示第c个类的先验，其满足以下条件： 0≤πc≤1, ∑c=1Cπc=10 \\leq \\pi_c \\leq 1, \\ \\sum_{c=1}^C \\pi_c = 10≤πc​≤1, ∑c=1C​πc​=1 在实际使用时，往往以每个类c在数据集中的统计频率结果作为先验，带入极大似然估计可以得到，第c个类的多元高斯判别器参数为： μ^c=1Nc∑i:yi=cxi\\hat \\mu_c = \\frac{1}{N_c} \\sum_{i: y_i=c} \\bold x_iμ^​c​=Nc​1​∑i:yi​=c​xi​ Σ^c=1Nc∑i:yi=c(xi−μ^c)(xi−μ^c)T\\hat \\Sigma_c = \\frac{1}{N_c} \\sum_{i: y_i=c} (\\bold x_i - \\hat \\mu_c)(\\bold x_i - \\hat \\mu_c)^TΣ^c​=Nc​1​∑i:yi​=c​(xi​−μ^​c​)(xi​−μ^​c​)T 其中，NcN_cNc​是类别c的样本在数据集中出现的次数。 多个正态分布的高斯混合模型 上面的单一正态分布模型，看起来很美好，但是存在一个问题：我们假定了任何一个类别c的数据都只服从一个单一的正态分布 事实上，我们无法假设任何要拟合的连续数据都真的只服从一个正态分布。 实际很多情况下， 真实的数据是 多个不同正态分布的叠加。 因此，我哦们需要——高斯混合模型： 理论上来说，高斯混合模型可以模拟任意的分布函数。 为了能对数据进行 “混合高斯建模”，首先我们需要知道要建模的数据空间到底需要几个正态分布的类。 没错，所以建立高斯混合模型的第一步，是聚类（clustering）。 这就和上一节的生成分类器很不一样，前者我们已知（假设了）每个类别对应一个正态分布。而在这里是不同的，我们不能相信标签里的类别信息，只能将最终每个类别需要混合的正态分布的数量视为隐变量。 EM算法 估计高斯混合模型 我们依然可以写出MLE方法和MAP方法对混合高斯模型的对数似然函数： 最大的问题在于：我们不知道隐变量z（即真实正态分布的分类数量）的维度，因此参数的解也是不唯一的。 如果我们假设有k个维度（即每个分类用k个正态分布混合表示），那么就会有k!种不同的解。 把k也加入模型的参数中，我们可以用C4中提到变分EM算法来进行最大似然估计。 公式的推导很长，如何以后有机会的话我把它整理一下放到另一个链接里…… 这里我们简单回顾一下过程吧： 1、定义Q函数，步数t=0，定义一个初始先验。然后每一步迭代推导隐变量zi=kz_i= kzi​=k的条件概率 2、E步：计算t-1步中，将Q函数表示为多元正态分布的参数 3、M步：最大化Q函数值，结果作为下一次迭代的参数。 4、重复2-3步 下图是一个实际迭代过程的可视化举例： K-means算法 估计高斯混合模型 你们可能在数据分析或者数据挖掘课上学到过K-means算法，你们有没有好奇过，这个算法的原理到底是什么？ 天哪如果你只看上一节就敏锐地发现了其实K-means算法就是EM算法的近似变形的话那你可太NB（朴素贝叶斯）了，我上课的时候是一点没听懂，感觉这还是八竿子打不着的两个问题…… K-means算法其实是上述 混合高斯模型估计的 一个近似特例。 直观地说这么解释：它们在聚类的时候其实都是一开始未知类别的数量k，然后通过计算不断收敛的。 它做了两个假设： 每个要混合的正态分布的 协方差矩阵 已知 （假设它们都等于标准的方差） 每个要混合的类别的 先验 已知（假设它们都相等，即为1/k） 因此，在K-means算法中，我们只需要计算样本相对于每个先验点的均值，并以此来迭代估计出每个类别的分布： 好处显而易见：计算非常方便快捷，能在最短的时间得到收敛的聚类。 但是，因为做出了协方差和先验的假设，最后得到的高斯分布就再也不能用于进行概率密度估计了。因此K-means相对于直接建模混合高斯分布有以下缺点： 无法计算某一个样本点属于某个高斯分布类的概率值 因此，也就无法生成新的样本点 综上，K-means算法在数据分析中非常常用，但是它不能作为一个生成模型来使用。遗憾退场。 隐马尔可夫模型 隐马尔可夫模型是关于 时序 的概率模型 。 为此，首先你需要知道 马尔可夫过程 和 马尔可夫链 的基本定义。我假定你已经学过了《随机过程》…… 什么，你没有学过？！这下可不好办了……咳咳…… 问题不大，你只需要先记好这两点： 马尔可夫过程 是基于时序的，它假定随机变量的每个取值都随 **时间 **而变。 齐次马尔可夫过程 假定 每一个时刻 随机变量的概率取值 只与 上一个时刻的随机变量有关。 你不足需要知道 齐次 是什么意思，总之在本篇的应用范围内，我们可以认为马尔可夫过程都是齐次的。 这样的一个过程中，状态组成的序列被称为马尔可夫链，因为每一个时刻都是可以和上一个时刻通过概率相连的。 而在隐马尔可夫模型中， 状态序列(state sequence)是隐藏的，从外部数据无法观测。这也就是为什么被称为隐马尔可夫链。 那么外部观测的是什么呢？我们只知道它和当前时刻的那个隐藏状态有关。我们把每个时刻中观测到的数据也组成一个序列，称为观测随机序列 (observation sequence ) 这样一定很不好理解，问题不大，我们来举一个例子： 骰子游戏 假设有三个不同的骰子：D6、D4、D8 （六面、四面和八面骰子）。 （——为什么没有D100？！——兄弟你走错片场了，这里玩的是DND） 每次投掷前，先等概率地随机从三个骰子中选取一个。 然后投掷选到的骰子，骰子随机地产生一个数字。 如果我们每次记录下每次得到的数字，那么它们就组成一个观测序列。 隐藏序列是什么？是我们每一次投的是哪一个骰子。 加入我只告诉你前面几次投掷的结果是：1 6 3 5 2 7 3 5 2 4 ，要估计下一次投掷的结果，那么隐藏状态必然是猜测下一次骰的是哪一个骰子。 接下来我们就以这个“生成骰子游戏点数”的模型为例子，来介绍隐马尔可夫模型。 隐马尔可夫模型 有三个要素： 初始概率分布 π\\piπ：即第一次（0时刻）观测变量的概率分布 如骰子游戏中，第一次试验可能得到1-8中的一个数字，可以通过组合方法计算出初始分布。 比如，第一次试验产生数字1的概率=13(16+14+18)=1372=\\frac{1}{3}(\\frac{1}{6}+\\frac{1}{4}+\\frac{1}{8})=\\frac{13}{72}=31​(61​+41​+81​)=7213​ 状态转移矩阵AAA：每个隐藏状态在下一时刻可能转换为另一个隐藏状态的概率：A(i,j)=p(zt=j ∣ zt−1=i)A(i,j) = p(z_t = j\\ | \\ z_{t-1}=i)A(i,j)=p(zt​=j ∣ zt−1​=i) 在骰子游戏中，三个骰子被视为三个隐藏状态。 其中，每个状态在下一时刻都有等概率（1/3）转变为任意的三个状态（包括它自己） 观测概率矩阵 BBB：从每个已知隐藏状态到观测状态的概率 Bt(j)=p(xt ∣ zt=j)B_t(j) = p(\\bold x_t \\ | \\ z_t=j)Bt​(j)=p(xt​ ∣ zt​=j) 在骰子游戏中，_（如果t=0）_对于D4，每个观测状态（1-4）的概率是25%，其他三个骰子类似。 将马尔可夫过程的假设写成数学公式，就是： 1、齐次马尔可夫性假设 隐马尔可夫链 ttt**时刻的状态 只和 **t−1t-1t−1时刻的状态 有关， 与其他时刻的状态及观测无关，也与时刻ttt无关 ： p(z1:T)=p(z1)p(z2∣z1)p(z3∣z1,z2)....p(zT∣z1,z2,...,zT−1)=p(z1)p(z2∣z1)p(z3∣z2)....p(zT∣zT−1)p(z_{1:T}) = p(z_1)p(z_2|z_1)p(z_3|z_1,z_2) .... p(z_T|z_1,z_2,...,z_{T-1}) \\\\ =p(z_1)p(z_2|z_1)p(z_3|z_2)....p(z_T|z_{T-1})p(z1:T​)=p(z1​)p(z2​∣z1​)p(z3​∣z1​,z2​)....p(zT​∣z1​,z2​,...,zT−1​)=p(z1​)p(z2​∣z1​)p(z3​∣z2​)....p(zT​∣zT−1​) 2、 观测独立性假设 观测变量只和当前时刻的状态有关，与其他时刻的 观测和状态均无关 p(x1:T∣z1:T)=p(x1∣z1)p(x2∣z2)p(x3∣z3)....p(xT∣zT)p(x_{1:T}|z_{1:T}) = p(x_1|z_1)p(x_2|z_2)p(x_3|z_3)....p(x_T|z_T)p(x1:T​∣z1:T​)=p(x1​∣z1​)p(x2​∣z2​)p(x3​∣z3​)....p(xT​∣zT​) 在骰子游戏中，观测变量是离散的，因此模型参数可以使用多项分布或贝塔分布，如果观测变量是连续的（如语音或文本），可以使用上面几节介绍的多元高斯分布来建模。 但是 **状态空间 **在隐马尔可夫模型中 一定是离散的 根据任务的不同，隐马尔可夫模型要求解的问题可以被分为以下几类： 学习：已知观测序列x1:Tx_{1:T}x1:T​，学习模型参数θ\\thetaθ,使得生成该观测序列的概率p最大 解码（预测隐空间）：给定模型参数θ\\thetaθ和观测序列x1:Tx_{1:T}x1:T​，求最有可能的隐藏状态序列z1:Tz_{1:T}z1:T​ 计算（预测观测结果）：给定模型参数θ\\thetaθ和观测序列x1:Tx_{1:T}x1:T​，求该观测序列x1:Tx_{1:T}x1:T​生成的概率。按照是否已知整个序列的数据，还可以分为： 前向算法（在线学习）：已知中间观测序列1-t，求生成到该序列的概率：p(zt∣x1:t)p(z_{t}|x_{1:t})p(zt​∣x1:t​) 后向算法（离线学习）：已知整个观测序列1-T，求p(zt∣x1:T)p(z_{t}|x_{1:T})p(zt​∣x1:T​) 可观测隐变量学习 在骰子游戏这个例子中，虽然从给定的观测序列x1:Tx_{1:T}x1:T​中我们无法知道隐变量的分布**，但是我们知道隐变量定义为三种骰子的随机选取，它服从某种特定的规则。** 类似这种情况，我们仍为隐变量z仍然是可观测的，因此，可以直接利用极大似然估计法来估计隐马尔可夫模型参数 。 对于N个状态序列的离散变量，我们可以使用多项分布建模： 假设观测变量x服从多项分布，其中k表示隐藏状态，l表示观测到的序列： B(k,l)=p(xt=l∣zt=k,θ), K=1,2,..,k, l=1,2,...,LB(k,l) = p(x_t =l|z_t=k,\\theta),\\ K=1,2,..,k,\\ l=1,2,...,LB(k,l)=p(xt​=l∣zt​=k,θ), K=1,2,..,k, l=1,2,...,L 我们在C2章已经举过例子了，不如推导留给你们，这里是参数π\\piπ和 A 的估计结果： 而参数矩阵B则使用统计方法计算： 如果 观测变量x服从正态分布，那就如同前面几节的方法，如法炮制， 在计算观测概率B时，将分子分母替换为 均值 和 协方差 矩阵即可。 不可观测隐变量学习 那么，更多的情况下，我们确实无法观测隐变量z，或者无法得知其的生成规律，此时我们需要一种 非监督的学习方法 Baum-Welch算法 ：一种改进的EM算法，用来解决隐马尔可夫模型的参数估计。 老三样： Q函数： 这里因为符号t已经被用来标记马尔可夫过程的时刻，我们用old上表标记参数的上一次迭代。 E步：在这里我们需要计算数据集中已知的序列在当前参数下的生成概率，具体方法参见后面的“计算问题”小节。 M步： 计算问题 前向算法 ：从第0步开始，算出下一步的概率密度 我们可以发现，t时刻的隐藏状态依赖于t-1时刻的隐藏状态，利用马尔可夫链的特性，我们可以定义下面的两个转移函数，其中i，j都是某一个隐状态： ϕt(j)=p(xt∣zt=j,θ)\\phi_t(j) = p(x_t|z_t=j,\\theta)ϕt​(j)=p(xt​∣zt​=j,θ) ψ(i,j)=p(zt=j∣zt−1=i,θ)\\psi(i,j) = p(z_t=j|z_{t-1}=i,\\theta)ψ(i,j)=p(zt​=j∣zt−1​=i,θ) 定义t时刻的 信念状态 （belief state）： 于是我们得到：αt∝ϕt(j)(ΨTαt−1)\\alpha_t \\propto \\phi_t(j)(\\Psi^T\\alpha_{t-1})αt​∝ϕt​(j)(ΨTαt−1​)其中Ψ\\PsiΨ是所有转移函数ψ(i,j)\\psi(i,j)ψ(i,j)组成的矩阵，并由此可以从t=1开始计算信念状态，从而得到最中的概率密度链 后向算法： 与前向计算相反，使用 条件似然函数 从最终时间T（前提：我们知道整个序列）反向推导每个时间点的序列概率。 预测问题 假设我们已经知道了模型参数和观测得到的序列数据，现在，我们想要推断最有可能的隐状态序列： z∗=argmaxz1:Tp(z1:T∣x1:T)z* = argmax_{z_{1:T}}p(z_{1:T}|x_{1:T})z∗=argmaxz1:T​​p(z1:T​∣x1:T​) 我们可以从第一个时刻起，每个时刻取的隐状态z标记一个代价：即 前面所有步骤的计算概率是否最大： 这样，我们就将求隐状态序列转换为一个 最大（优）路径问题。 然后，我们可以使用计算机算法来求解，例如贪心算法。 下面介绍一个称为 维特比算法 的方法，它的思想是用 动态规划 解最大路径问题： 从终结点开始，由 后向前逐步求得结点 zT∗,zT−1∗,...,z1∗z^*_{T}, z^*_{T-1},...,z^*_{1}zT∗​,zT−1∗​,...,z1∗​，得到最优路径","tags":["生成模型","笔记"]},{"title":"【笔记】生成模型基础与应用 - 第3章","path":"/2023/11/27/生成模型基础与应用笔记-第3章/","content":"C3 主题模型：主题模型是一类用于文本分析的非监督学习方法，旨在从文本数据中发现隐藏的主题结构… C3 主题模型 Topic models：以非监督学习的方式对文本的隐含结构进行发现或生成的模型 主题模型是一类用于文本分析的非监督学习方法，旨在从文本数据中发现隐藏的主题结构。这些主题模型的目标是识别文本集合中的主题，而无需事先标记的主题标签或监督信息。主题模型最常见的应用之一是用于文本数据的主题建模，其中文档被看作是多个主题的混合，而每个主题又由一组词汇表示。 主题模型的发展： LSA PLSA LDA 2003 HDP 2005 单词向量空间 将文档中每个单词的出现的频数（或加权的频数）表示为向量 可以事先定义 有效单词的 语料库，只提取文档中有效单词的出现频率生成向量（也可以视为不在语料库中的单词其权值为0），所有的语料库中的有效单词用集合W表示 一个文档就表示成：d=(fw1,fw2,....,fwN)d=(f_{w_1},f_{w_2},....,f_{w_N})d=(fw1​​,fw2​​,....,fwN​​)，每个fif_ifi​都是有效单词iii出现的次数 一个文档集中的所有文档（设一共有N个文档组成了集合D）组成了一个向量集合，即 单词向量空间 对于两个不同的文档di, djd_i,\\ d_jdi​, dj​，可以使用它们之间的数学度量，来表示文本之间的语义相似度： 计算方法可以为 文档向量的 内积 或 标准化内积 单词向量空间的表示方法： 1、单词-文本矩阵：将单词在每个文档的出现频率向量作为列向量，组成的矩阵。通常为稀疏矩阵 单词-文本矩阵的例子： - 计算机处理稀疏矩阵，非常浪费算力，因此，需要一个等价的数据结构来表示相同给信息，于是提出了下面这种表示方法。 2、单词频率-逆文本频率（TF-IDF）： 统计单词wiw_iwi​在文本dkd_kdk​中出现的权值 ： TF−IDF(wi,dk)=f(wi,dk)len(dk)logNdf(wi)TF-IDF(w_i,d_k) = \\frac{f(w_i,d_k)}{len(d_k)}log\\frac{N}{df(w_i)}TF−IDF(wi​,dk​)=len(dk​)f(wi​,dk​)​logdf(wi​)N​ - tf(w,b)：单词w在文本b中的出现频数 - len(b)：文本b中的单词总数 - df(w)：整个文本集合中，含有单词w的文本数 - N：整个文本集合的大小（含有的文本数量） 单词向量空间的优缺点优点：模型简单、计算效率高缺点：内积计算的相似度不一定能准确表达两个文档间的相似度 话题向量空间 定义话题：假设所有的文本中一共含有K个话题 假设每个话题都是一个M维度的向量ttt，ttt由语料库（集合W）中的M个单词组成。 t=(w1,w2,...,wM)Tt = (w_1,w_2,...,w_M)^T t=(w1​,w2​,...,wM​)T 这样，一共K个话题就组成了一个话题向量空间，记为T=[t1,t2,...,tk]=[w11 w12... wMK]T = [t_1,t_2,...,t_k]=[w_{11}\\ w_{12} ... \\ w_{MK}]T=[t1​,t2​,...,tk​]=[w11​ w12​... wMK​]【单词-话题矩阵】 根据单词向量空间的定义，可以推导出： 假设一篇文本xxx在单词向量空间中被表示为：x=(fw1,fw2,....,fwN)x = (f_{w_1},f_{w_2},....,f_{w_N})x=(fw1​​,fw2​​,....,fwN​​)，而在话题空间中被表示为：$$y = g(x) = (f_{t_1},f_{t_2},…,f_{t_k})$$ 为了方便，我们将话题的出现频率 记为ft1=y1,ft2=y2...f_{t_1} = y_1, f_{t_2} = y_2...ft1​​=y1​,ft2​​=y2​...这样，一个文档x，它可以表示为所有话题和单词的线性组合： x=t1y1+t2y2+...tkykx = t_1y_1 + t_2y_2 + ... t_ky_k x=t1​y1​+t2​y2​+...tk​yk​ 如果我们定义话题-文本矩阵Y，即将每个话题表示的文本yyy组合起来，则由上述的线性组合关系，对整个文档集合D，即可由话题向量空间的 话题矩阵T 来表示。 因此，单词-文本矩阵、单词-话题矩阵和话题-文本矩阵具有如下的关系： XMN=TMKYKNX_{MN} = T_{MK}Y_{KN} XMN​=TMK​YKN​ 潜在语义分析即是 将文本在 单词向量空间的表示 通过 线性变换 转换为** 在话题向量空间中的表示**的方法 期望最大化算法 在讲潜在语义分析之前，必须要了解一下什么是贝叶斯概率模型的潜变量，以及对潜变量模型的估计算法。期望最大化算法（Exception Maximization Algorithm，简称EM算法）是一种启发式的迭代算法，用于对含有隐变量的概率模型 的参数做 极大似然估计 假设我们有这样一个概率模型： 有三枚硬币A,B,CA,B,CA,B,C，抛掷它们正面朝上的概率不同，记为r,p,qr,p,qr,p,q 重复执行以下试验（Ex(X)表示对事件X做一次实验，1表示正面向上，0表示反面向上）： Ex(Y)=Ex(B) if Ex(A)==1 else Ex(C)Ex(Y)= Ex(B) \\ if \\ Ex(A)==1 \\ else \\ Ex(C) Ex(Y)=Ex(B) if Ex(A)==1 else Ex(C) 即抛硬币A，如果是正面抛硬币B，否则抛硬币C，你只能观测到最后一次抛掷硬币的结果。 那么如何估计三个参数r,p,qr,p,qr,p,q呢？ 因为这个随机事件包含两个概率模型的 复合。因此我们无法直接对结果建模，数学家们由此提出，能否加入“隐变量”ZZZ： 用它来表示试验中抛掷A硬币的中间结果（虽然我们并不能直接观测到），假设共抛了n次，第i次观测结果的值记作ziz_izi​ -假设我们只看最终结果Y，将参数表示为向量θ=(r,p,q)\\theta = (r,p,q)θ=(r,p,q)，则实际要观测的事件Y的似然可以表示为： P(Y∣θ)=P(Y,Z∣θ)=∑i=1nP(yi,zi∣θ)=∑i=1nP(zi∣θ)P(yi∣zi,θ)=rp∑(yi=1)(1−p)∑(yi=0)+(1−r)q∑(yi=1)(1−q)∑(yi=0)P(Y|\\theta) = P(Y,Z|\\theta)= \\sum_{i=1}^{n}P(y_i,z_i|\\theta) = \\sum_{i=1}^{n}P(z_i|\\theta)P(y_i|z_i,\\theta) \\\\= rp^{\\sum(y_i=1)}(1-p)^{\\sum(y_i=0)}+(1-r)q^{\\sum(y_i=1)}(1-q)^{\\sum(y_i=0)} P(Y∣θ)=P(Y,Z∣θ)=i=1∑n​P(yi​,zi​∣θ)=i=1∑n​P(zi​∣θ)P(yi​∣zi​,θ)=rp∑(yi​=1)(1−p)∑(yi​=0)+(1−r)q∑(yi​=1)(1−q)∑(yi​=0) 直接使用MLE估计这个似然函数？不行，包含三个未知变量，且需要求log和的极大值，算不出解析解。 既然没有准确的解析解，数学家于是提出了用迭代法逼近最大值的求解办法，这就是EM算法。我们先定义好两种情况的似然函数，既然要求极大似然，我们对似然函数直接取log，定义对数似然函数： 不将隐变量Z视为随机变量的情况【称为：不完全数据】 LY(θ)=log[∏i=1nP(yi∣θ)]=∑i=1nlog[∑zi∈ Ex(Z)P(yi,zi∣θ)]L_Y(\\theta) = log[\\prod_{i=1}^nP(y_i|\\theta)] = \\sum_{i=1}^nlog[\\sum_{z_i \\in \\ Ex(Z)}P(y_i,z_i|\\theta)] LY​(θ)=log[i=1∏n​P(yi​∣θ)]=i=1∑n​log[zi​∈ Ex(Z)∑​P(yi​,zi​∣θ)] 将隐变量Z视为随机变量的情况【称为：完全数据】 LY,Z(θ)=log[∏i=1nP(yi,zi∣θ)]=∑i=1nlog[P(yi,zi∣θ)]L_{Y,Z}(\\theta) = log[\\prod_{i=1}^nP(y_i,z_i|\\theta)] = \\sum_{i=1}^nlog[P(y_i,z_i|\\theta)] LY,Z​(θ)=log[i=1∏n​P(yi​,zi​∣θ)]=i=1∑n​log[P(yi​,zi​∣θ)] 我们假设θ\\thetaθ从一个初始值θ0\\theta_0θ0​开始，每次迭代的时候更新（下标+1），那么我们希望 在已知观测结果Y的情况下，让完全数据的对数似然函数LY,Z(θ)L_{Y,Z}(\\theta)LY,Z​(θ)尽可能取得最大的期望，由此定义Q函数： Q(θ,θt)=E[LY,Z(θ)∣Y,θt]=∑zi∈ Ex(Z)P(Z∣Y,θt)log[P(Y,Z∣θ)]Q(\\theta,\\theta_t) = E[L_{Y,Z}(\\theta)|Y,\\theta_t] = \\sum_{z_i \\in \\ Ex(Z)}{P(Z|Y,\\theta_t)log[P(Y,Z|\\theta)]} Q(θ,θt​)=E[LY,Z​(θ)∣Y,θt​]=zi​∈ Ex(Z)∑​P(Z∣Y,θt​)log[P(Y,Z∣θ)] 其中，θt\\theta_tθt​是第t步更新的参数，Q是关于θ\\thetaθ的函数，让它最大，则需要对θ\\thetaθ求偏导，并取导数为0。此时θt\\theta_tθt​取一个新值，即： \\theta_{t+1} = argmax_\\theta{Q(\\theta,\\theta_t)} $$【MLE】 这是采用极大似然估计的方法，如果你说要考虑先验概率（即MAP方法），那么每次更新是需要加上先验函数的对数值： \\theta_{t+1} = argmax_\\theta{Q(\\theta,\\theta_t)} + logP(\\theta) 【MAP】 &gt; 现在我们试着对刚才的问题应用EM算法： &gt; - 先假设$\\theta^{(0)} = (0.5,0.5,0.5)$（为了下标不打架，我们用上括号标记步数t） &gt; - 则$P(Z|Y,\\theta_t)$表示： &gt; - 假设第$i$次的最终**观测结果是**$y_i(=1 \\ or\\ 0)$**，这个结果是由 抛硬币B**$(z_i=1)$** 还是 抛硬币C**$(z_i=0)$** 得到的概率，将其记为**$p_i$**，**我们可用直接以伯努利实验的结果计算： &gt; $p_i = \\frac{P(y_i,z_i=1)}{P(y_i,z_i=0)+P(y_i,z_i=1)}=\\frac{rp}{rp+(1-r)q}(y_i=1) \\ or \\ \\frac{r(1-p)}{r(1-p)+(1-r)(1-q)}(y_i=0)$**【对于每一步i】** &gt; - **然后求其关于Z的期望，即：** &gt; $$Q(\\theta) = \\sum_{i=1}^{n}p_i\\log[P(y_i=1,Z|\\theta)] + (1-p_i)\\log[P(y_i=0,Z|\\theta)] 注意：pip_ipi​与每一步要更新的θt\\theta_tθt​有关，因此其表示为的r,p,qr,p,qr,p,q**也需要每一步更新。**而log[P(Y,Z∣θ)]log[P(Y,Z|\\theta)]log[P(Y,Z∣θ)]则直接表示为上述的似然函数，注意它与要逼近的θt\\theta_tθt​无关，可以直接表示为r,p,qr,p,qr,p,q的式子，故： Q(θ,θt)=∑i=1npi(t−1)log⁡[rpyi(1−p)1−yi]+(1−pi(t−1))log⁡[(1−r)qyi(1−q)1−yi]Q(\\theta,\\theta_t)=\\sum_{i=1}^{n}p_i^{(t-1)}\\log[rp^{y_i}(1-p)^{1-y_i}] + (1-p_i^{(t-1)})\\log[(1-r)q^{y_i}(1-q)^{1-y_i}] Q(θ,θt​)=i=1∑n​pi(t−1)​log[rpyi​(1−p)1−yi​]+(1−pi(t−1)​)log[(1−r)qyi​(1−q)1−yi​] 对该函数的r,p,qr,p,qr,p,q分别求偏导，即可得到： r(t+1)=∑i=1npi(t)nr^{(t+1)} = \\frac{\\sum_{i=1}^{n}p_i^{(t)}}{n} r(t+1)=n∑i=1n​pi(t)​​ p(t+1)=∑i=1npi(t)yi∑i=1npi(t)p^{(t+1)} = \\frac{\\sum_{i=1}^{n}p_i^{(t)}y_i}{\\sum_{i=1}^{n}p_i^{(t)}} p(t+1)=∑i=1n​pi(t)​∑i=1n​pi(t)​yi​​ q(t+1)=∑i=1n(1−pi(t))yi∑i=1n(1−pi(t))q^{(t+1)} = \\frac{\\sum_{i=1}^{n}(1-p_i^{(t)})y_i}{\\sum_{i=1}^{n}(1-p_i^{(t)})} q(t+1)=∑i=1n​(1−pi(t)​)∑i=1n​(1−pi(t)​)yi​​ 问题： 为什么EM算法能近似实现对观测数据 的极大似然估计？ 答：可以通过数学证明：可以通过L(θ)L(\\theta)L(θ)函数构造Q函数，当作为中间函数时，可以证明在迭代过程中极大对数似然函数L(θ)L(\\theta)L(θ)的下界单调增加，即：L(θt+1)≥L(θt)L(\\theta_{t+1}) \\geq L(\\theta_t)L(θt+1​)≥L(θt​) 证明就省略啦~ 概率潜在语义分析（PLSA） 是一种利用概率生成模型 对 文本集合进行话题分析的 无监督学习方法 假设： 每个文本由一个话题的分布决定 每个话题由一个单词的分布决定 因此，从给定文本的表示到生成单词，就是一个概率模型，其中话题是隐变量 举个例子：假设一个离散空间中，【话题】和其对应的单词的概率分布如下： 【教育】 = {大学：0.5，老师：0.3，课程：0.2 } 【经济】 = {市场：0.4，企业：0.2，金融：0.4} 【交通】 = {高铁：0.5，汽车：0.2，飞机：0.3} 而对于一个挖了空位的文本，其每个位置对应话题的概率分布如下： D(xx场景下如何看待xx和xx的关系) = {教育：0.5，经济：0.3，交通：0.2} 那么，生成 “大学场景下如何看待大学与企业的关系”的概率 是： [P(t=教育)P(w=大学∣t=教育)]2×P(t=经济)P(w=企业∣t=经济)=0.00375[P( t = 教育 )P(w = 大学|t = 教育) ]^2× \\\\P(t = 经济) P(w = 企业 |t = 经济 ) =0.00375[P(t=教育)P(w=大学∣t=教育)]2×P(t=经济)P(w=企业∣t=经济)=0.00375 假设文本的集合是D={d1,d2,...,dN}D=\\{d_1,d_2,...,d_N\\}D={d1​,d2​,...,dN​}单词的集合是：W={w1,w2,...,wM}W=\\{w_1,w_2,...,w_M\\}W={w1​,w2​,...,wM​}话题的集合是：Z={z1,z2,...,zK}Z=\\{z_1,z_2,...,z_K\\}Z={z1​,z2​,...,zK​}我们可以看到，在求文本的生成分布时，涉及到以下的条件概率：P(z∣d)P(z|d)P(z∣d): 已知文本d，生成话题z的概率，是一个多项分布P(w∣z)P(w|z)P(w∣z): 已知话题z，生成单词w的概率，也是一个多项分布而P(d)P(d)P(d)表示从所有文本集合中，随机选取一个文本的d的概率 现在我们希望知道，给定(单词，文本)对，它在这个空间中的生成的概率，即： P(X)=∏(w,d) ∈ W×DP(w,d)cnt(w,d)P(X) = \\prod_{(w,d)\\ \\in \\ W×D} P(w,d)^{cnt(w,d)} P(X)=(w,d) ∈ W×D∏​P(w,d)cnt(w,d) ，其中所有的(w,d)(w,d)(w,d)对应该有N×L（文本数×每个文本中要填空的单词数）个根据上面例子的思想，对于每个(w,d)(w,d)(w,d)对，其生成概率又可以用隐变量写为： P(w,d)=P(d)P(w∣d)=P(d)∑z ∈ ZP(w,z∣d)=P(d)∑z ∈ ZP(z∣d)P(w∣z)P(w,d) = P(d)P(w|d) = P(d) \\sum_{z\\ \\in \\ Z}P(w,z|d) = P(d) \\sum_{z \\ \\in \\ Z}P(z|d)P(w|z) P(w,d)=P(d)P(w∣d)=P(d)z ∈ Z∑​P(w,z∣d)=P(d)z ∈ Z∑​P(z∣d)P(w∣z) 于是我们可以建立模型，对P(z∣d)P(z|d)P(z∣d)和P(w∣z)P(w|z)P(w∣z)分别进行估计，得到了这样的单层隐变量网络，即PLSA：其中左侧的参数量是 NK，右侧为MK 现实中K远小于M，所以PLSA通过话题 对数据进行了更简洁地表示，减少了学习过程中过拟合的可能性 继续使用极大似然估计的方法，试图求出让P(X)最大时的参数，取对数似然（下面的公式推导中，我们省略了长度是O(NK+MK)O(NK+MK)O(NK+MK)的参数向量θ，不然就太长了太难读了QAQ）： LW,Z,D=log∏(w,d) ∈ W×DP(w,d)cnt(w,d)=log∏i=1M∏j=1NP(wi,dj)cnt(wi,dj)=∑i=1M∑j=1Ncnt(wi,dj)logP(wi,dj)=∑i=1M∑j=1Ncnt(wi,dj)[logP(dj)+log∑k=1KP(wi∣zk)P(zk∣dj)]L_{W,Z,D} = log\\prod_{(w,d)\\ \\in \\ W×D} P(w,d)^{cnt(w,d)} \\\\ = log \\prod_{i=1}^M \\prod_{j=1}^NP(w_i,d_j)^{cnt(w_i,d_j)} \\\\ = \\sum_{i=1}^M\\sum_{j=1}^Ncnt(w_i,d_j)logP(w_i,d_j) \\\\ = \\sum_{i=1}^M\\sum_{j=1}^Ncnt(w_i,d_j)[logP(d_j) + log \\sum_{k=1}^KP(w_i|z_k)P(z_k|d_j)] LW,Z,D​=log(w,d) ∈ W×D∏​P(w,d)cnt(w,d)=logi=1∏M​j=1∏N​P(wi​,dj​)cnt(wi​,dj​)=i=1∑M​j=1∑N​cnt(wi​,dj​)logP(wi​,dj​)=i=1∑M​j=1∑N​cnt(wi​,dj​)[logP(dj​)+logk=1∑K​P(wi​∣zk​)P(zk​∣dj​)] 仍然是非常难计算，因此要使用上一节介绍的EM估计算法，为此我们计算Q函数。上面的似然函数LW,Z,DL_{W,Z,D}LW,Z,D​已经是一个【完全数据】，隐藏变量Z被视为与W,D并列的随机变量。因此Q函数是每个(w,d)(w,d)(w,d)对为条件下，LW,Z,DL_{W,Z,D}LW,Z,D​的期望，即： Q=P(zk∣wi,dj)[∑k=1K∑i=1M∑j=1Ncnt(wi,dj)logP(wi,dj,zk)]Q = P(z_k|w_i,d_j)[\\sum_{k=1}^K\\sum_{i=1}^M\\sum_{j=1}^Ncnt(w_i,d_j)logP(w_i,d_j,z_k)] Q=P(zk​∣wi​,dj​)[k=1∑K​i=1∑M​j=1∑N​cnt(wi​,dj​)logP(wi​,dj​,zk​)] 其中，$$P(w_i,d_j,z_k) = P(d_j)P(w_i|z_k)P(z_k|d_j)$$ 就是按照“链式法则”生成一堆单词文本的联合概率，而P(zk∣wi,dj)P(z_k|w_i,d_j)P(zk​∣wi​,dj​)则是隐藏变量Z不被视为条件的分布概率【不完全数据】，即： P(zk∣wi,dj)=P(wi∣zk)P(zk∣dj)∑k=1KP(wi∣zk)P(zk∣dj)P(z_k|w_i,d_j) = \\frac{P(w_i|z_k)P(z_k|d_j)}{\\sum_{k=1}^KP(w_i|z_k)P(z_k|d_j)} P(zk​∣wi​,dj​)=∑k=1K​P(wi​∣zk​)P(zk​∣dj​)P(wi​∣zk​)P(zk​∣dj​)​ 这样，我们可以发现，要每一步更新的参数其实是P(wi∣zk)P(w_i|z_k)P(wi​∣zk​)和P(zk∣dj)P(z_k|d_j)P(zk​∣dj​) 因为我们已经有一个数据集，P(dj)P(d_j)P(dj​)的值（即先验）可以直接由统计方法估计出来，cnt函数（即哪些对实际在数据集中同时出现了也是可以统计的已知量）。因此，EM近似的步骤为： 对于每个下标对(i,k)和(k,j)，分别求P(wi∣zk)P(w_i|z_k)P(wi​∣zk​)和P(zk∣dj)P(z_k|d_j)P(zk​∣dj​)的偏导数（第一步时为每个概率取初始值（如均匀分布），只要满足概率和为1即可） 然后将导数为0的值求出来，更新(i,k)和(k,j)对应参数向量的元素 取 k=k+1，求下一步的不完全数据，直到收敛 在用统计估计P(dj)P(d_j)P(dj​)后，局部最优解的结果经过推导是：PLSA的方法仍然胜在离散空间，计算复杂度低，可以快速迭代，但存在一个很重要的缺点：因为所学习的参数不仅依赖于单词库W，还依赖于文档数据集D，所有其可以生成其所在数据集的文档的模型，但却不能生成新文档的模型 潜在狄利克雷分布（LDA） 在对PLSA建模时，我们发现一个规律： 文本由话题的一个多项分布表示 ， 话题也由单词的一个多项分布表示 以防你忘了我们在C1中介绍的多项分布，补充一下完整定义： 这不巧了嘛，我们在C2中提到，多项分布的贝叶斯生成模型，可以用 狄利克雷分布 作为先验。因此，可以假设 话题分布 和 单词分布 的先验都是 狄利克雷分布： 对于每个文本dmd_mdm​，假设d中包含的单词组成了向量dm=wm (m=1,2,...,M)d_m = \\bold w_m \\ (m=1,2,...,M)dm​=wm​ (m=1,2,...,M),认为生成该文本话题的概率P(z∣wm)=θm∼Dir(α)P(z|\\bold w_m) = \\theta_m \\sim Dir(\\alpha)P(z∣wm​)=θm​∼Dir(α) θm\\theta_mθm​参数向量有K个值，每个值表示文本dmd_mdm​能生成对应话题z1,z2,...,zkz_1,z_2,...,z_kz1​,z2​,...,zk​的概率 因此，超参数α\\alphaα也是一个K维向量 注意，在这里我们为了对每个单词写出一个话题分布，将文本的定义改成了单词组成的向量。因此代表文本集中的文本数量的常量不再是NNN，而是MMM。用Ni(i=1,2,...,M)N_i(i=1,2,...,M)Ni​(i=1,2,...,M)来表示一个文本中的单词总数。而下面的部分中，我们假设整个单词库中的单词总数是VVV 对于每个话题zk (k=1,2,...,K)z_k\\ (k=1,2,...,K)zk​ (k=1,2,...,K)，认为生成相应单词的概率P(w∣zk)=ϕk∼Dir(β)P(w|z_k) = \\phi_k \\sim Dir(\\beta)P(w∣zk​)=ϕk​∼Dir(β) 而对于每个单词wm[n]∈wm (n=1,2,...,Nm)w_m[n] \\in \\bold w_m \\ (n=1,2,...,N_m)wm​[n]∈wm​ (n=1,2,...,Nm​)，其对应的话题和单词都链式地服从多项分布： zm[n]∼Mult(θm)z_m[n] \\sim Mult(\\theta_m)zm​[n]∼Mult(θm​)：先随机根据概率生成一个话题序列 wm[n]∼Mult(ϕzm[n])w_m[n] \\sim Mult(\\phi_{z_m[n]})wm​[n]∼Mult(ϕzm​[n]​)，再对每个话题，随机生成一个单词序列，共生成m个 在这个定义下，我们可以直接得到所有先验的表示，因此，可以使用最大后验概率估计MAP来求解参数，记住在这里w是观测量，z是隐变量，因此后验是已知观测量算隐变量的概率：文本w的后验=P(θ,z∣w,α,β)=P(w,z,θ,ϕ∣α,β)P(w∣α,β)文本w的后验=P(\\bold{\\theta}, \\bold z|\\bold w, \\alpha,\\beta) = \\frac{P(\\bold w,\\bold z,\\theta,\\phi|\\alpha,\\beta)}{P(\\bold w|\\alpha,\\beta)}文本w的后验=P(θ,z∣w,α,β)=P(w∣α,β)P(w,z,θ,ϕ∣α,β)​然后，就像PLSA方法一样，根据向量的嵌套定义把概率计算拆成每个元素概率的乘积，式子很长，直接贴一下结果吧：自然，想对这种玩意求解析最大值，和自杀没什么区别。不过，既然这次我们估计的是后验概率分布，概率论中可以以 变分推断 的方法来处理 变分推断 所谓变分推断，是取一个用隐变量z的分布q(z)q(z)q(z)来近似后验概率的条件分布p(z∣x)p(z|x)p(z∣x)的方法。为了比较二者的分布相似度，使用KL散度来计量： 如果能找到与p(z∣x)p(z|x)p(z∣x)在KL散度意义下最近的分布 q^(z)\\hat q(z)q^​(z)，则可以用这个分布近似后验概率分布 带入KL散度的公式： 则，如果想要KL散度更小，必有 log p(x)≥Eq[log p(x,z)]−Eq[log q(z)]log\\ p(x) \\geq E_q[log\\ p(x,z)] - E_q[log\\ q(z)] log p(x)≥Eq​[log p(x,z)]−Eq​[log q(z)] 因为先验x的分布log p(x)log\\ p(x)log p(x)可以被视为常量，因此右侧关于q分布的期望式越大，KL散度就越小。数学上把右侧称为证据下界。因此，下面问题就变为求证据下界的最大化。还有一个假设，是对于隐变量z（向量），假设分布q(z)q(z)q(z)对 z 的所有分量都是独立的，即： q(z)=q(z1)q(z2),...,q(zm)q(z) = q(z_1)q(z_2),...,q(z_m) q(z)=q(z1​)q(z2​),...,q(zm​) 称其为平均场。 现在让我们回到LDA模型的最大后验概率估计，现在我们有了变分推断算法，可以定义“文本w的后验” 的证据下界： L(r,t,α,ϕ)=Eq[log p(z,w)]−Eq[log q(z)]=Eq[log p(θ,z,w∣α,ϕ)]−Eq[log q(θ,z∣r,t)]L(r,t,\\alpha,\\phi) = E_q[log\\ p(\\bold z, \\bold w)] -E_q[log\\ q(\\bold z)]\\\\ = E_q[log\\ p(\\theta,\\bold z, \\bold w| \\alpha,\\phi)] -E_q[log\\ q(\\theta,\\bold z|r,t)] L(r,t,α,ϕ)=Eq​[log p(z,w)]−Eq​[log q(z)]=Eq​[log p(θ,z,w∣α,ϕ)]−Eq​[log q(θ,z∣r,t)] 其中，向量r和t是变分参数，r来估计隐变量z在单词向量中的分布参数θ\\thetaθ，t来估计话题向量中的分布参数(z1,z2,...,zn)(z_1,z_2,...,z_n)(z1​,z2​,...,zn​)有了平均场假设，就可以对每个文本分来计算，得到所有文本的证据下界： L′(r,t,α,ϕ)=∑m=1MEq[log p(θm,zm,wm∣αm,ϕm)]−Eq[log q(θm,zm∣rm,tm)]L&#x27;(r,t,\\alpha,\\phi) =\\sum_{m=1}^M{ E_q[log\\ p(\\theta_m,\\bold z_m, \\bold w_m| \\alpha_m,\\phi_m)] -E_q[log\\ q(\\theta_m,\\bold z_m|r_m,t_m)]} L′(r,t,α,ϕ)=m=1∑M​Eq​[log p(θm​,zm​,wm​∣αm​,ϕm​)]−Eq​[log q(θm​,zm​∣rm​,tm​)] 此时我们发现它也是一个 离散的期望最大化估计 问题了，可以使用第二节提到的 EM算法来迭代更新参数，此时有四个参数向量，两个是为了变分推断引入的，另外两个则为模型建立的狄利克雷分布参数。 注意，实际上狄利克雷分布的参数是α、β\\alpha 、\\betaα、β，不过因为话题的分布参数ϕ\\phiϕ可以直接由参数β\\betaβ根据话题数 k 得到，因此这里简化一下模型，直接估计参数向量ϕ\\phiϕ 这种方法被综合称为 **变分EM算法** LDA和PLSA的比较 相同点：都将 话题建模为单词的多项分布，文本建模为话题的多项分布 不同点： PLSA没有使用先验分布（ 或者说假设先验分布是均匀分布 ），使用MLE估计。 而LDA假设了狄利克雷分布作为先验分布，且使用MAP估计。 有先验分布的好处和C2中提到的一样，可以防止过拟合问题","tags":["生成模型","笔记"]},{"title":"【笔记】生成模型基础与应用 - 第2章","path":"/2023/11/27/生成模型基础与应用笔记-第2章/","content":"C2 离散数据的生成模型 C2 离散数据的生成模型 生成模型的目的：学习联合概率分布 P(X,Y)结合第一节课的知识，接下来我们会介绍一些比较传统的生成模型，它们不会涉及DL和神经网络，但是是后续生成模型的数学基础。 贝叶斯概念学习（以离散模型举例） 只提供正向的样本，让模型学习正向的特征，然后判断输入是否属于要训练的类别（概念） 与二分类任务的原理相同，但是训练二分类模型会提供 both 正向样本和负向样本 **以学习一个猜数字的模型为例：**问题：有一个由数字组成的数据集D\\mathcal {D}D，问如何学习一个概念C（C是未知的，描述数据集D\\mathcal {D}D中数字应该服从的分布），这里的数字都在0-100之间。 假设空间：人为地提出“假设”，用来猜测概念C。根据某一个概念假设（如“偶数”），所有可能生成的元素组成h的集合。比如“偶数”，则对应的假设数据集h={2,4,6,...,100}h=\\{2,4,6,...,100\\}h={2,4,6,...,100}。所有可以提出假设组成假设空间 版本空间：即所以“有效的”假设组成的空间。即满足：数据集D中的所有元素在假设的集合h中，这些假设组成的空间。 注意：一般来说，数据集D中包含的元素数量越多，版本空间就会越小，因为会有很多假设连数据集中的采样数据都过不了关，就被筛除了。 似然（likehood）：从h中独立采样N次，从假设能生成 数据集D中数据的概率 P(D∣h)P(\\mathcal {D}|h)P(D∣h) P(D∣h)=[1∣h∣]NP(\\mathcal {D}|h) = [\\frac{1}{|h|}]^NP(D∣h)=[∣h∣1​]N, 这里|h|表示h的集合大小 考虑假设1：h=“偶数”，假设2：h=“2的幂” 若 D = {16}，则假设1的概率P(D|h) = 1/6，则假设2的概率P(D|h) = 1/50 若 D = {2,8,16,64} 此时，假设2 的 似然概率是 假设1 的4812.5倍 奥卡姆剃刀原则：选择似然相近的假设时，假设h中包含的数据数量越少越好 先验概率：当提出假设时，为其中的数据赋予先验概率P(h)P(h)P(h) 。当提出的假设在数据集中对应的实际意义不太自然时，为其赋予低的先验概率 比如，D = {50,60,70,90}，给与两个假设，分别包含数据 67 和 210 如果数据集描述的是自然数，那么包含210的假设拥有更高的P(h) ，因为210和D中数据都是10的倍数 如果数据集描述的人的体重数据(kg)，那么包含67的假设拥有更高的P(h)，因为210在不太可能是符合概念的数据 后验：判断版本空间中的概念谁更接近要学习的概念。根据贝叶斯公式，后验概率与 先验概率和似然的乘积成正比：$$P(h|\\mathcal {D}) \\propto P(\\mathcal {D}|h) P(h)$$，后验概率最大的假设可以被认为是最优假设。 如果用H标记假设空间中所有假设i组成的集合，那么将数据集D与每种假设的联合概率求和，就是后验概率的基数了，即： P(h∣D)=P(D∣h)P(h)∑i∈HP(D,i)P(h|\\mathcal {D}) = \\frac{P(\\mathcal {D}|h)P(h)}{\\sum_{i \\in H}P(\\mathcal {D},i)} P(h∣D)=∑i∈H​P(D,i)P(D∣h)P(h)​ 注意：预先定义的假设空间不一定是完备的，真·概念C不一定在你的空间中，如果这样（现实中大多数情况也是如此），找到的最优假设只能说是最接近C的。 接下来我们尝试直接用数学推导出求最大后验概率的公式： 最大后验概率估计 MAP 我们要求一个h，其会使得后验概率最大，数学中我们用argmax函数来描述使得一个函数取最大值时，自变量的值，因此： h^map=argmaxh(P(h∣D))=argmaxh(P(D∣h)P(h)∑i∈HP(D,i))\\hat h^{map} = argmax_h(P(h|D)) = argmax_h(\\frac{P(\\mathcal {D}|h)P(h)}{\\sum_{i \\in H}P(\\mathcal {D},i)}) h^map=argmaxh​(P(h∣D))=argmaxh​(∑i∈H​P(D,i)P(D∣h)P(h)​) 分母对所有假设的联合概率求和，这与h无关，因此我们可以认为：h^map=argmaxhP(D∣h)P(h)=argmaxh[log([1∣h∣]N)+log(P(h))]\\hat{h}^{map} = argmax_h P(\\mathcal {D}|h)P(h) = argmax_h [log([\\frac{1}{|h|}]^N) + log(P(h))]h^map=argmaxh​P(D∣h)P(h)=argmaxh​[log([∣h∣1​]N)+log(P(h))] 右边我们取对数，在不破坏函数的单调性的同时将乘积的概率变为加法，然后我们会发现： 常量N会影响最大函数的取值 当N越大，即数据足够多时，P(D∣h)P(\\mathcal {D}|h)P(D∣h)即log([1∣h∣]N)log([\\frac{1}{|h|}]^N)log([∣h∣1​]N)的对数增长速率远大于P(h)P(h)P(h) 因此在计算出最大概率的h时，后者影响越来越小，直到可以忽略不计，因此，在实际建模中，我们常常只估计似然P(D∣h)P(\\mathcal {D}|h)P(D∣h)，我们将这种近似的方法称为：极大似然估计 MLE : \\hat{h}^{mle}\\ \\~= \\ argmax_h \\ P(\\mathcal {D}|h) = argmax_h \\ log([\\frac{1}{|h|}]^N) 于是，当N足够大时，将每个H空间中的假设h代入上式，使得函数取值最大的h可以被视为最优假设。 生成新数据：已知最优假设h后，便可以直接以该估计作为条件，求出生成各种元素（x）的概率： p(xˉ∣D)=∑h∈HP(h∣D)P(xˉ∣h)≈P(xˉ∣h^map)p(\\bar x| \\mathcal {D} ) = \\sum_{h\\in H}{P(h|\\mathcal {D})P(\\bar x|h)} \\approx P(\\bar x|\\hat{h}^{map}) p(xˉ∣D)=h∈H∑​P(h∣D)P(xˉ∣h)≈P(xˉ∣h^map) 以上是一个离散分布的贝叶斯概念模型学习的例子，根据概率论，这个方法也可以推广到的连续随机变量的情况，比如服从二项分布的概率模型。相应地，我们需要将计算似然和概率估计的概率P(X)改为概率密度p(x)，而将求和计算转为积分。下面会介绍两个连续分布的贝叶斯概念模型 Beta二项分布生成模型 现在来看连续随机变量的情况，以抛硬币的二项分布为例： 抛一个硬币N次，记录有N0N_0N0​次正面朝上，N1N_1N1​次反面朝上 假设每次抛硬币的结果为随机变量x，x服从二项分布，正面朝上记为1，反面朝上记为0 那么，二项分布的参数θ\\thetaθ（正面朝上的概率）在[0,1]之间，是一个连续变量，也是我们要估计的 注意：我们的目标是估计θ\\thetaθ的分布，而不是直接求出一个结果（不然直接频率作为概率不就完了，不行，你这不贝叶斯啊）。 因为有很多可能的θ\\thetaθ，都可以使得我们的抛硬币实验得到上述的结果。 我们现在已知N次独立实验的结果，N0N_0N0​次正面朝上，N1N_1N1​次反面朝上就是“数据集”的数据，因此可以写出已知采样与我们实际要求的“抛硬币哪一面向上”分布的似然： likehood：$$P(\\mathcal {D} | \\theta) = \\theta{N_1}(1-\\theta){N_0}$$ 记住我们要求后验概率，它等价于先验概率和似然的乘积。现在的问题在于：我们怎么知道先验概率p(θ)p(\\theta)p(θ)？我们可以用贝塔分布来作为我们的先验概率模型，之所以是它的理由如下： 贝塔分布常用于估计[0,1]分布上，缺少足够先验样本的分布 贝塔分布有一个重要的特性：共轭先验，即将其带入贝叶斯模型，得到的后验概率的函数形式与先验相同。而如果我们将独立实验的结果N0N_0N0​或N1N_1N1​次作为随机变量计算先验，其服从伯努利分布，似然函数的形式也与贝塔分布的先验相同。 我们假设先验服从贝塔分布：p(θ)∼Beta(θ∣a,b)p(\\theta) \\sim Beta(\\theta|a,b)p(θ)∼Beta(θ∣a,b)于是我们现在可以计算后验： P(θ∣D)∝[θN1(1−θ)N0][θa−1(1−θ)b−1]=[θN1+a−1(1−θ)N0+b−1]=Beta(θ∣N1+a,N0+b)P(\\theta|\\mathcal {D}) \\propto [\\theta^{N_1}(1-\\theta)^{N_0}][ \\theta^{a-1}(1-\\theta)^{b-1}] = [\\theta^{N_1+a-1}(1-\\theta)^{N_0+b-1}]\\\\ = Beta(\\theta|N_1+a,N_0+b) P(θ∣D)∝[θN1​(1−θ)N0​][θa−1(1−θ)b−1]=[θN1​+a−1(1−θ)N0​+b−1]=Beta(θ∣N1​+a,N0​+b) 现在，我们只需估计后验函数取尽可能大值时，参数θ的分布，根据贝塔分布的特性， Beta(a,b) 分布的随机变量取最大值时，参数为a−1a+b−2\\frac{a-1}{a+b-2}a+b−2a−1​ 得到最大后验概率估计：θ^map=a−1+N1a+b−2+N\\hat \\theta^{map} = \\frac{a-1+N_1}{a+b-2+N}θ^map=a+b−2+Na−1+N1​​，根据共轭先验的性质，我们要估计的θ\\thetaθ分布也是一个贝塔分布，其具体形状与参数a，b有关。 如果我们使用极大似然估计，即不考虑先验，则结果退化为：θ^mle=N1N\\hat\\theta^{mle} = \\frac{N_1}{N}θ^mle=NN1​​，即直接用频率估计概率的结果，如果我们取上述贝塔分布的a=b=1（均匀分布，认为先验的结果硬币正面朝上的概率就是五五开的），那么也会得到这个结果。 使用MAP的结果，我们可以生成下一次抛硬币正面朝上的结果估计： P(xˉ=1∣D)=∫01P(xˉ=1∣θ)P(θ∣D)dθ=∫01θBeta(θ∣a+N1,b+N0)dθ=E[θ∣D]=a+N1a+b+NP(\\bar x=1|\\mathcal D) = \\int_0^1P(\\bar x =1 |\\theta)P(\\theta|\\mathcal D)d\\theta = \\int_0^1\\theta Beta(\\theta|a+N_1,b+N_0) d\\theta \\\\= E[\\theta|D] = \\frac{a+N_1}{a+b+N} P(xˉ=1∣D)=∫01​P(xˉ=1∣θ)P(θ∣D)dθ=∫01​θBeta(θ∣a+N1​,b+N0​)dθ=E[θ∣D]=a+b+Na+N1​​ 可以发现，结果正好是我们的贝塔分布模型的均值。有趣的是，只要我们使用MAP的生成结果，即使取a=b=1的均匀分布，也会使得估计正面朝上的概率中，分子至少为1，不会出现 _因为实验数据集中没有正面朝上，就将其判定为不可能事件 _的情况。因此，这种方法也被称为**”（拉普拉斯）+1平滑“** 狄利克雷多项分布生成模型 再来看一个复杂一点的情况：随机变量有K个可能的取值，比如说，估计一个有k面的骰子，每个面朝上的概率。我们假设做了N次实验，每个面朝上的次数分别为：N1,N2,...NkN_1,N_2,...N_kN1​,N2​,...Nk​和伯努利实验类似，可以这样定义： 似然函数： p(D∣θ)=θ1N1θ2N2...θkNkp(\\mathcal D|\\theta) = \\theta_1^{N_1}\\theta_2^{N_2}...\\theta_k^{N_k} p(D∣θ)=θ1N1​​θ2N2​​...θkNk​​ 先验函数，将之前贝塔分布的共轭先验推广到多个变量，这就是狄利克雷多项分布： p(θ)=Dir(θ∣α)p(\\theta) = Dir(\\theta|\\alpha) p(θ)=Dir(θ∣α) 后验计算： 最大后验概率估计： θ^k=Nk+αk−1N+∑k=1Kαk−K\\hat \\theta_k = \\frac{N_k+\\alpha_k-1}{N+\\sum^K_{k=1}\\alpha_k - K} θ^k​=N+∑k=1K​αk​−KNk​+αk​−1​ 同样的，如果使用最大似然估计，会退化为：θ^k=NkN\\hat \\theta_k = \\frac{N_k}{N}θ^k​=NNk​​ 生成一个骰子点数，其结果是j点的概率： P(xˉ=j∣D)=E(θj∣D)=αj+Nj∑k=1Kαk+NP(\\bar x=j|\\mathcal D) = E(\\theta_j|\\mathcal D) = \\frac{\\alpha_j+N_j}{\\sum^K_{k=1}\\alpha_k+N} P(xˉ=j∣D)=E(θj​∣D)=∑k=1K​αk​+Nαj​+Nj​​ 朴素贝叶斯分类器（NBC） 在第一章，我们说了 生成模型 包含 分类模型的所有功能。比如下面这个分类问题： 任务：将若干个由离散数据组成的向量进行分类。设变量 x=(x1,x2,...xD)Tx = (x_1,x_2,...x_D)^Tx=(x1​,x2​,...xD​)T是长度为D的特征向量，每个元素（特征）有KKK个可能的取值，希望将所有这样的向量分为C类： {1,2,…C} ，用 Y=c 来表示结果的类标号。于是我们想要知道，给定向量x和模型参数向量θ，分类的概率：Py=P(Y=c∣x,θ)P_y=P(Y=c|x,\\theta)Py​=P(Y=c∣x,θ)，只要对所有的类别c计算这个概率，取最大值作为最终分类。 我们尝试用贝叶斯生成模型的方法来解决这个问题。这涉及到刚才两节二项分布和狄利克雷多项分布方法的综合。 首先，我们考虑给定类别c，能生成特征向量x的概率： Px=P(x∣Y=c,θ)P_x = P(x|Y=c,\\theta)Px​=P(x∣Y=c,θ) 则由贝叶斯公式，要求的概率可以表示为： Py=P(Y=c∣x,θ)=P(x,Y=c∣θ)P(x∣θ)=P(x,Y=c∣θ)∑i∈C[P(x,Y=i∣θ)]=P(x∣Y=c,θ)P(Y=c∣θ)∑i∈C[P(x∣Y=i,θ)P(Y=i∣θ)]∝P(Y=c∣θ)PxP_y=P(Y=c|x,\\theta) = \\frac{P(x,Y=c|\\theta)}{P(x|\\theta)} = \\frac{P(x,Y=c|\\theta)}{\\sum_{i \\in C}[P(x,Y=i|\\theta)]} = \\frac{P(x|Y=c,\\theta)P(Y=c|\\theta)}{\\sum_{i \\in C}[P(x|Y=i,\\theta)P(Y=i|\\theta)]} \\propto {P(Y=c|\\theta)}P_x Py​=P(Y=c∣x,θ)=P(x∣θ)P(x,Y=c∣θ)​=∑i∈C​[P(x,Y=i∣θ)]P(x,Y=c∣θ)​=∑i∈C​[P(x∣Y=i,θ)P(Y=i∣θ)]P(x∣Y=c,θ)P(Y=c∣θ)​∝P(Y=c∣θ)Px​ 可以发现，因为现在问题空间包含两个随机变量x，Y，对于类别的变量Y，式子中也推出了先验概率。 P(Y=c∣θ)P(Y=c|\\theta)P(Y=c∣θ)就是分类先验（在训练的数据集中，最终被每一类被分入了多少个向量）。 这会使得模型的参数向量θ中还要包含分类先验的参数。如果将它们和向量x的参数混为一谈，这将导致无法更新参数。为了能将向量内的每个特征对应的参数拆分出来计算，朴素贝叶斯提出一个重要的假设： 给定同一个类别的标签，每一个特征的条件都是独立的 注意：真实世界中，这个假设是很难成立的，但是依然该方法依然可以用来估计分类器 则我们可以将特征向量在模型中的参数用θ表示，而分类先验的参数在下文将用π\\piπ来表示，它们将被分别估计。先来看特征向量： Px=P(x∣Y=c,θ)=Πi=1DP(xi∣Y=c,θic)=Πi=1DTiP_x = P(x|Y=c,\\theta) = \\Pi_{i=1}^{D}P(x_i|Y=c,\\theta_{ic}) = \\Pi_{i=1}^{D} T_i Px​=P(x∣Y=c,θ)=Πi=1D​P(xi​∣Y=c,θic​)=Πi=1D​Ti​ 我们用TiT_iTi​来标记以 向量中的每个特征 为随机变量的 分布。对于特征的定义不同，这个分布会表示成不同的形式： 如果特征是0-1编码的：值不是0就是1，那么TiT_iTi​是伯努利分布，参数θic\\theta_{ic}θic​要表示 第i个特征的值会使得整个向量有多大可能性落在分类c中 如果每个特征有KKK个可能的取值（原始的问题），那么TiT_iTi​是多项分布，参数θic\\theta_{ic}θic​要表示为一组向量。 如果特征是连续的实数来表示的，那么TiT_iTi​将是一个连续的高斯分布，参数也要表示为&lt;均值，方差&gt;的一组值。 回到问题的开始，为了下面的推导写的比较简单，我们将问题简化为只要0-1编码的两种特征 （它已经很复杂了QAQ，再加上K个类别我都不敢想……） 然后我们来看看如何使用贝叶斯学习来进行训练：假设训练数据集中包含了N个**&lt;向量x, 分类标签Y&gt;**的数据对，每个对用&lt;xi,Yi&gt;(i=1,2,...,n)&lt;x_i,Y_i&gt;(i=1,2,...,n)&lt;xi​,Yi​&gt;(i=1,2,...,n)来表示，那么分类的先验表示为：$$P(Y_i|\\pi) = \\Pi\\ \\pi_c^{if(Y_i =c)}$$ [...]if(Yi=c)[...]^{if(Y_i =c)}[...]if(Yi​=c)表示计数函数，即只有第i个分类标签为c时，才保留参数πc\\pi_cπc​【没错，还是统计方法】 注意这里作为条件概率的pi和下面的θ都是向量，而右侧式子表示取向量中的哪一个有效值（用符号π\\piπ来区分 分类的参数向量 和 特征的参数向量θ\\thetaθ） 同样，特征的先验计算为： P(xi∣Yi,θ)=Πj=1DP(xi[j] ∣Yi,θj)=Πj=1D(Π P(xi[j] ∣θjc)if(Yi=c))P(x_i|Y_i,\\theta) = \\Pi_{j=1}^DP(x_i[j]\\ |Y_i,\\theta_j) = \\Pi_{j=1}^D(\\Pi\\ P(x_i[j]\\ |\\theta_{jc})^{if(Y_i=c)}) P(xi​∣Yi​,θ)=Πj=1D​P(xi​[j] ∣Yi​,θj​)=Πj=1D​(Π P(xi​[j] ∣θjc​)if(Yi​=c)) 于是，推导整个数据集的后验概率函数，即将所有数据的结果相乘起来： P(D∣θ)=Πi=1N[P(Yi∣θ)P(xi∣Yi,θ)]P(\\mathcal D|\\theta) = \\Pi_{i=1}^N[P(Y_i|\\theta)P(x_i|Y_i,\\theta)] P(D∣θ)=Πi=1N​[P(Yi​∣θ)P(xi​∣Yi​,θ)] 尝试使用极大似然估计来直接计算参数（MLE），直接对两边取对数，得到： log[P(D∣θ)]=∑c=1CNc+∑j=1D(∑c=1C(∑Yi=clogP(xi[j]∣θjc)))log[P(\\mathcal D|\\theta)] = \\sum_{c=1}^C N_c + \\sum_{j=1}^D(\\sum_{c=1}^C(\\sum_{Y_i=c}logP(x_i[j]|\\theta_{jc}))) log[P(D∣θ)]=c=1∑C​Nc​+j=1∑D​(c=1∑C​(Yi​=c∑​logP(xi​[j]∣θjc​))) （其中xi[j]x_i[j]xi​[j]表示向量xix_ixi​中的第jjj个值）上面的式子看着非常复杂，其实本质上与上一节的二项分布估计相同： 分类的先验估计就是：从数据集N中统计出分类c出现次数NcN_cNc​，并计算比例： πc=NcN\\pi_{c} = \\frac{N_{c}}{N}πc​=NNc​​ 特征的似然参数估计就是：从每个已知类别的出现次数中，统计出是当前特征的比例，即： θjc=NjcNc\\theta_{jc} = \\frac{N_{jc}}{N_c}θjc​=Nc​Njc​​ （不是0-1编码的，NjcN_{jc}Njc​就是一个嵌套的向量了） 这个结果，与抛硬币和骰子一样，因为先验是完全按照统计结果的，容易出现过拟合的问题。另一种方法，可以使用刚才的贝叶斯方法，对pi和θ两个参数的分布假设先验： :::warning 假设P(Yi∣π)P(Y_i|\\pi)P(Yi​∣π)是一个狄利克雷分布，参数是α=(α0,α1,...αC)\\alpha = (\\alpha_0,\\alpha_1,...\\alpha_C)α=(α0​,α1​,...αC​) 假设每个 P(xi[j] ∣θjc)P(x_i[j]\\ |\\theta_{jc})P(xi​[j] ∣θjc​)都是一个贝塔分布，参数是(β0,β1)(\\beta_0,\\beta_1)(β0​,β1​) ::: 然后计算后验，形式是一样的，只不过丑陋的计数函数被替换成了先验分布的均值：代入最大后验概率估计MAP的估计结果是：如果有一条新的数据需要分类，那么对于每个类别c=1,2,...Cc=1,2,...Cc=1,2,...C，将MAP估计的参数带入P(Y=c∣xˉ,θ)P(Y=c|\\bar x,\\theta)P(Y=c∣xˉ,θ)，得到使得概率最大的c就是要求的分类。可以看出，朴素贝叶斯模型虽然推导起来比较复杂（其实也只是复杂在数据维度比较多，人脑CPU容易干烧，交给计算机来还是很快的），但是计算复杂度很低，所以在小场景中使用广泛。同时，我们也可以用这个模型生成新的数据：和二项分布生成模型一样，在生成新的数据时，代入模型的参数就是分布的均值：","tags":["生成模型","笔记"]},{"title":"【笔记】生成模型基础与应用 - 第1章","path":"/2023/10/28/生成模型基础与应用笔记-第1章/","content":"C1 概率论和统计基础 C1 概率论和统计基础 什么是概率？ 频数的概率解释：频率估计概率（小学就学过的） 贝叶斯概率解释：概率是某样事件的“不确定度”，是人们在多大程度上相信某件事情将会发生（似然likehood） 因为贝叶斯概率能适用更多更广泛的概率事件（例如：从未发生但可能的事件），因此以下讨论的概率偶都基于贝叶斯概率 事件、随机变量和概率分布： 事件：自然语言描述的具有随机性的事件 随机变量：可能从有限 或 可数的无限 集合X中随机取值的变量 概率的表示：P(事件A) = P(随机变量X=某值a) 【如果我们定义当X取a时事件A发生】 概率分布：表述随机变量X的 取值 的 概率规律 的集函数 离散随机变量要点回顾 事件合并概率： P(A∪B)=P(A)+P(B)−P(A∩B)P(A ∪ B) = P(A) + P(B) - P(A ∩ B)P(A∪B)=P(A)+P(B)−P(A∩B) 如果A和B是互斥事件，则：P(A∪B)=P(A)+P(B)P(A ∪ B) = P(A) + P(B)P(A∪B)=P(A)+P(B) 两个随机变量的联合概率： P(A,B)=P(A)P(B∣A)=P(B)P(A∣B)P(A,B) = P(A)P(B|A) = P(B)P(A|B)P(A,B)=P(A)P(B∣A)=P(B)P(A∣B) 联合概率分布的链式法则： P(X1,X2,...,XD)=P(X1)P(X2∣X1)P(X3∣X1,X2)...P(XD∣X1,X2...XD)P(X_1,X_2,...,X_D)=P(X_1)P(X_2|X_1)P(X_3|X_1,X_2)...P(X_D|X_1,X_2...X_D)P(X1​,X2​,...,XD​)=P(X1​)P(X2​∣X1​)P(X3​∣X1​,X2​)...P(XD​∣X1​,X2​...XD​) 两个随机变量的条件概率： P(B∣A)=P(B,A)P(A)=P(B)P(A∣B)P(A) ,P(A)&gt;0P(B|A) = \\frac{P(B,A)}{P(A)}=\\frac{P(B)P(A|B)}{P(A)} \\ ,P(A) &gt; 0P(B∣A)=P(A)P(B,A)​=P(A)P(B)P(A∣B)​ ,P(A)&gt;0 贝叶斯条件概率公式 重要名称： 先验：已知的，B事件发生的概率 似然：在B的条件下A发生的概率 与 A发生的概率 之比：P(A∣B)P(A)\\frac{P(A|B)}{P(A)}P(A)P(A∣B)​ 后验：在A的条件下，B发生的概率，即P(B|A) 两个随机变量的条件独立性： 如果两个变量的联合概率可以被拆分为各自概率的乘积，则称两个变量是 （无条件）独立的 P(X,Y)=P(X)P(Y)P(X,Y) = P(X)P(Y)P(X,Y)=P(X)P(Y) 而如果给定随机变量Z，在Z的条件概率下满足上述条件，则称两个变量X,Y是 条件独立的 P(X,Y∣Z)=P(X∣Z)P(Y∣Z)P(X,Y|Z) = P(X|Z)P(Y|Z)P(X,Y∣Z)=P(X∣Z)P(Y∣Z) 连续随机变量要点回顾 累计分布函数（cdf）：F(x) = P(X &lt;= x)当连续变量的取值小于x时，总计的概率 概率密度函数（pdf）：p(x) = F(x) dx 是累计分布函数的微分 计算连续随机变量在a,b区间上的概率：F(b) - F(a)，或是在p(x)上积分 连续分布的数学量：均值E、方差σ^2、中位数…… 协方差：对于两个随机变量X,Y，衡量它们的线性相关性： cov[X,Y]=E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y)cov[X,Y] = E[(X-E(X))(Y-E(Y))] = E(XY) - E(X)E(Y)cov[X,Y]=E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y) 相关系数：将协方差标准化后的数学量： corr[X,Y]=R=cov[X,Y]σ2(X)σ2(Y)corr[X,Y] = R= \\frac{cov[X,Y]}{\\sqrt{\\sigma^2(X) \\sigma^2(Y)}}corr[X,Y]=R=σ2(X)σ2(Y)​cov[X,Y]​ 独立的两个变量，它们不相关 但是，不相关的两个变量，可能互相独立 如果两个符合高斯分布的变量不相关，则它们一定独立 对于多元随机变量的联合分布，协方差和相关系数的计算将变为矩阵的形式： 常见概率分布回顾 【离散的】 经验分布（eCDF）：描述从抽样中得到的概率分布，经验分布的概率密度函数即为所有抽样的结果之和，其中抽样被定义为 狄利克雷函数：即抽样的结果只有0或者1. 二项分布：重复n次独立的 伯努利事件 实验，获得其中一种结果k次的概率分布 伯努利事件：某个事件只有两种可能的结果（布尔随机变量），其中一种的概率为p，另一个为1-p 经典的例子是抛硬币 多项分布：重复n次独立的 多重伯努利事件 实验，获得 每种结果的次数 的概率分布 多重伯努利事件：某个事件可能有k种不同的结果，并且每种结果具有固定的概率 经典的例子是投一个k面的骰子 【连续的】 均匀分布：在一个区间或域上，随机变量的取值为固定值 对于一维变量，随机分布的概率密度函数为：Unif(x)=1b−a(a≤x≤b)Unif(x) = \\frac{1}{b-a} (a \\leq x \\leq b )Unif(x)=b−a1​(a≤x≤b) 正态分布（高斯分布）：多个相互独立的随机变量之和 的分布 会趋近于这个分布，因此它被广泛使用 正态分布的概率密度函数和符合该分布的随机变量的均值和方差有关 N(x∣μ,σ2)=12πσ2e−12σ2(x−μ)2N(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}N(x∣μ,σ2)=2πσ2​1​e−2σ21​(x−μ)2 可以拓展为多元变量的正态分布，将均值修改为多元变量的数学期望，方差修改为多元变量的协方差 泊松分布：对二项分布的连续近似，在二项分布的实验次数n很大，单次概率p很小时，二项分布可被近似为泊松分布。 P(x∣λ)=e−λλxx!P(x|\\lambda) = e^{-\\lambda}\\frac{\\lambda^x}{x!}P(x∣λ)=e−λx!λx​, 其方差和均值都是 λ Student t 分布：基于正态分布，为了增强其抗干扰性而提出的分布，加入一个参数v： 拉普拉斯分布（双指数分布）：在均值的两侧，呈现对称分布规律的一种 指数分布 变种 Lap(x∣μ,b)=12be−∣x−μ∣bLap(x|\\mu,b) = \\frac{1}{2b}e^{-\\frac{|x-\\mu|}{b}}Lap(x∣μ,b)=2b1​e−b∣x−μ∣​，其均值为 μ\\muμ，方差为 2b22b^22b2 伽马分布：对正实数域上的随机变量建模的分布，是多个独立同分布的指数分布变量 和 的分布 Gamma(x∣a,b)=baγ(a)xa−1e−xbGamma(x|a,b) = \\frac{b^a}{\\gamma(a)}x^{a-1}e^{-xb}Gamma(x∣a,b)=γ(a)ba​xa−1e−xb，其中γ(a)是伽马函数：\\gamma(a) = \\int_0^\\inf t^{a-1}e^{-t}dt 参数a被称为shape，b被称为rate，该分布均值为 ab\\frac{a}{b}ba​，方差为ab2\\frac{a}{b^2}b2a​ 贝塔分布：对[0,1]区间上取值的随机变量建模的分布 Beta(x,∣a,b)=1β(a,b)xa−1(1−x)b−1Beta(x,|a,b) = \\frac{1}{\\beta(a,b)}x^{a-1}(1-x)^{b-1}Beta(x,∣a,b)=β(a,b)1​xa−1(1−x)b−1,其中 β(a,b)\\beta(a,b)β(a,b)是贝塔函数，它只是为了使得这个分布的概率密度积分等于1才加上的。 狄利克雷分布：将贝塔分布拓展到多元变量的泛化 【分布的变换】 若分布Y可以由服从分布X的随机变量，将每个取值用离散或连续的函数f变换得到，那么分布Y的均值和方差会遵循以下公式 线性变换： 通用变换： 离散变量： py(y)=∑x:f(x)=ypx(x)p_y(y)=\\sum_{x:f(x)=y}p_x(x)py​(y)=∑x:f(x)=y​px​(x) 连续变量： py(y)=px(x)∣dxdy∣p_y(y)=p_x(x)|\\frac{dx}{dy}|py​(y)=px​(x)∣dydx​∣ 其他重要概念 大数定律：随着样本规模的增加，样本均值对总体均值的估计越准确。 中心不变定理：多个随机变量样本的均值分布（随机变量和的分布）将近似于高斯分布。 蒙特卡洛近似：如果某随机变量X的分布未知，但可以对其进行抽样来实验，则可以使用经验分布来近似X的分布： 衡量两个分布的相似度（距离）：KL散度 先补充信息论的知识：信息熵 信息熵可以描述随机变量X在分布P上的不确定性的程度：H(X)=−∑Kk=1p(X=k)log2p(X=k)H(X) = -\\sum_K^{k=1}p(X=k)log_2p(X=k)H(X)=−∑Kk=1​p(X=k)log2​p(X=k) 均匀分布的信息熵最大 交叉熵：将服从分布P的变量转换到分布Q，需要提供额外信息（bits）的量，其中p和q代表P和Q的概率密度函数 H(p,q)=−∑kpklogqkH(p,q) = -\\sum_{k}p_klogq_kH(p,q)=−∑k​pk​logqk​ KL散度：描述两个分布的概率密度函数p和q的相似度： KL(p∣∣q)=∑k=1Kpklogpkqk=−H(p)+H(p,q)KL(p||q) = \\sum_{k=1}^Kp_klog\\frac{p_k}{q_k}=-H(p)+H(p,q)KL(p∣∣q)=∑k=1K​pk​logqk​pk​​=−H(p)+H(p,q) 互信息度：衡量两个分布的变量之间互相依赖的程度： II(X;Y)=KL(p(X,Y)∣∣p(X)p(Y)) =∑x∑yp(x,y)logp(x,y)p(x)p(y)=H(X)−H(X∣Y)=H(Y)−H(Y∣X)II(X;Y) = KL(p(X,Y)||p(X)p(Y))\\ = \\sum_x\\sum_yp(x,y)log\\frac{p(x,y)}{p(x)p(y)}\\\\=H(X)-H(X|Y)=H(Y)-H(Y|X)II(X;Y)=KL(p(X,Y)∣∣p(X)p(Y)) =∑x​∑y​p(x,y)logp(x)p(y)p(x,y)​=H(X)−H(X∣Y)=H(Y)−H(Y∣X)","tags":["生成模型","笔记"]},{"title":"Test Article","path":"/2023/10/28/Test-Article/","content":"Hello world! Hi there! 如果你看到这个，说明这个博客框架似乎运行正常。 This marks the blog is running well. 下面的部分用于测试hexo和stellar主题扩展的相关功能，请忽略其中的内容。 表情 行内文本修饰 美化文本格式 这是 你知道的太多了 隐藏标签 这是 下划线 标签 这是 着重号 标签 这是 波浪线 标签 这是 删除线 标签 这是 X2 上标标签 这是 X2 下标标签 这是 键盘样式 标签，试一试：Ctrl + D mark 标记 这是彩色标记：默认 红 橙 黄 绿 青 蓝 紫 这是底色标记：浅 深 这是特殊标记： 警告 错误 一共 12 种颜色。 tag标签 和mark类似，但是可以高亮和添加链接，使用属性color指定颜色，不指定为随机 Stellar Hexo MyGitHub 使用标签插入图片 这将比使用默认markdown格式插入图片要好 src: 图片地址 description: 图片描述 width:和padding:可以对不同的尺寸做适应 bg:可以添加背景颜色，使用bg:var(–card)使得背景颜色适配全局配色 download: 设置为true可以增加一个下载图片的按钮，但这只会从src中下载图片，如果（对于大图）图片的下载地址和src的预览地址不同，可以将下载地址写在这里 点击放大功能：在任意 image 标签中增加 fancybox:true 参数即可为特定图片开启缩放功能 1&#123;% image src [description] [download:bool/string] [width:px] [padding:px] [bg:hex] %&#125; 点击下载你可以获得另一张来自Apple的图片 文本美化块 美化的引用 适合居中且醒目的引用：这句话不是我说的 ——鲁迅 支持自定义引号：话题001 诗词文本展示 游山西村陆游莫笑农家腊酒浑，丰年留客足鸡豚。山重水复疑无路，柳暗花明又一村。箫鼓追随春社近，衣冠简朴古风存。从今若许闲乘月，拄杖无时夜叩门。诗词节选 备注块 正式内容中，第一个空格前面的是标题，后面的是正文。如果标题中需要显示空格，请使用&amp;nbsp;代替。 无色备注块：和代码块一样的展示方式 你&nbsp;标题&nbsp;中有&nbsp;空&nbsp;格正文部分可以随便空格。 彩色备注块：使用color:&lt;color&gt;设置背景颜色，颜色标签和行内文本一致 可用颜色：color = red、orange、yellow、green、cyan、blue、purple、light、dark、warning、error 只使用标题的备注块，使得文本默认加粗： 这个只有标题，没有正文。 外链卡片 12345&#123;% link href [title] [icon:src] [desc:true/false] %&#125;href: 链接title: 可选，手动设置标题（为空时会自动抓取页面标题）icon: 可选，手动设置图标（为空时会自动抓取页面图标）desc: 可选，是否显示摘要描述，为true时将会显示页面描述 https://www.bilibili.com/video/BV1ic411R77s/Nickbit的bilibili投稿 mermaid代码语言绘图 需要安装插件，目前还没有支持 使用前需要在 Markdown 文件开头加入 123---mermaid: true--- 然后使用代码块，在语言中指定mermaid即可 frame插入一个移动设备UI框架 可用插选择参数img（图片）和video（视频） iframe插入视频 静态时间线 2021 年 6 月BT夏季版banner2021 年 2 月过年放假 友情链接 要现在source/_data/links.yml中加入链接的静态数据： 123456789101112&#x27;链接组名称&#x27;: - title: 某某某 url: https:// screenshot: avatar: description: - title: 某某某2 url: https:// screenshot: avatar: description: ... 然后这样写： 1&#123;% friends 链接组名称 %&#125; 一个链接组可以有多个链接，它们会自适应排布到页面。 网站卡片链接 同样要先在source/_data/links.yml中加入链接的静态数据，写法和友链一样。 写法是： 1&#123;% sites 分组名 %&#125; 616.sb纯净的音游下载站BEMANICNBEMANICN 论坛BestdoriBanG Dream! 资源库和社区 github card卡片 填写github仓库的名称后缀即可 容器标签 支持更加丰富的分块文本，note等标签是由容器标签简化实现的。 容器标签需要多行来写 标准容器块 这是标题这是容器块中的内容插入一个链接：#172 可折叠标题的容器块 容器块中可用嵌套其他块，包括另一个容器块。使用child属性设置嵌入块的属性 open属性设置块是否默认打开 默认打开的代码折叠框代码块 平铺折叠列表块 #1这是答案1#2这是答案2#3这是答案3 分栏tab容器 方便地切换展示的内容，也可以嵌套 图片代码块表格12let x = 123print(&quot;hello world&quot;) a b c a1 b1 c1 a2 b2 c2 轮播容器 适用于轮播图片，默认一张图片是 50% 宽度，通过设置 width:min 设置为 25% 宽度，width:max 设置为 100% 宽度。 添加参考引用列表，效果如下： references: title: ‘使用 Stellar 主题的博客’ url: https://xaoxuu.com/wiki/stellar/examples/ title: ‘Hexo Stellar 和 Next 主题支持 KaTex 公式与 Markdown 复杂表格’ url: https://www.panoshu.top/blog/2c3f9e38/"},{"title":"2022影视游艺总结","path":"/2022/09/27/2022影视游艺总结/","content":"名称 类型 评分（满分10分+附加分2分） 备注 三角符文（Deltarune第二章） 游戏 10+2 JOJO的奇妙冒险 石之海（上部） 动漫 9+1 师父《Sifu》 游戏 8 《Elden Ring》艾尔登法环 游戏 9+0.5 史丹利的寓言 超级豪华版《The Stanley Parable Ultra Deluxe》 游戏 9+1.5 瞬息全宇宙《Everything.Everywhere.All.At.Once》 电影/剧集 9+1.5 爱，死亡和机器人 第三季《Love.Death.and.Robots.S03》 电影/剧集 9 林中小女巫《Little Witch in the Woods》 游戏 7.5+1 诡野西部《Weird West》 游戏 6 未完整体验 孤山难越《Insurmountable》 游戏 7 未完整体验 派对浪客诸葛孔明《Paripi Koumei》》 动漫 8+1 间谍过家家《Spy x Family》 动漫 9+0.5 迷失《Stray》 游戏 8.5+0.5 莉可丽丝《Lycoris Recoil》 动漫 8.5+1 哥布林弹球《Peglin》 游戏 7+1 独行月球 电影/剧集 5.5 Arcaea 4.0 游戏 8+1 咩咩启示录《Cult of the Lamb》 游戏 7+0.5 未完整体验 汉化日记 第三季 动漫 7.5+1 JOJO的奇妙冒险 石之海 Part.2（下部） 动漫 8+0.5","tags":["总结","评分表"]},{"title":"Nickbit's 2021影视游艺总结","path":"/2021/12/28/2021影视游艺总结/","content":"名称 类型 体验时间（估计） 体验地点 评分（满分10分+附加分1分） 备注 Alba: A Wildlife Adventure 游戏 2021.1 Home 7+1 虽然玩法、剧情都很简单，甚至说低幼，但仍让我感到十分放松。 咒术回战 动漫 2021.1 Home 9+1 奇蛋物语 动漫 2021.1 Home+寝室 8+1 特别篇是6月份在寝室看的，说实话没看懂。 唐人街探案3 电影 2021.1 电影院 7 没啥感觉 你好，李焕英 电影 2021.1 电影院 8 刺杀小说家 电影 2021.2 Home 6 人潮汹涌 电影 2021.2 Home 8 有点感觉 刺客五六七（第三季） 动漫 2021.4 寝室 8+1 汉化日记（第二季） 动漫 2021.1 Home 8+1 《玩梗王者》 工作细胞（第二季） 动漫 2021.2 Home 8+1 我的三体 张北海传 动漫 2021.1 Home 9+1 前进四！ The Unfinished Swan未完成的天鹅 游戏 2021.2 Home 8+0.5 很神，但是很难说出来在哪里 Spiritfarer 灵魂摆渡人 游戏 2021.2 Home 8.5+1 死亡教育喜+1 戴森球计划 游戏 2021.2 Home 10+1 戴森球，YYDS！ 双人成行 游戏 2021.3 寝室 10+1 GOTY！ 节奏医生 游戏 2021.2 Home+寝室 10+1 一二三四五六七！ 核聚变2021北京站 展会 2021.5 亦创国际会展中心 9 人人人从从从众众众 寻找李白 话剧 2021.5 科学会堂 8.5 《中轴线》 书籍 2021.5 寝室 8 冲动消费的绘本竟成为今年唯一纸质阅读入账，堕落啊！ 爱，死亡和机器人(第二季) 剧集 2021.5 寝室 7+0.5 年底想来，只记得冰面了 灵笼（下半部分及终章） 动漫 2021.5 寝室 6.5+0.5 星露谷物语 游戏 2021.8 寝室 【未完整体验】 但是真的太上头了 霓虹深渊 游戏 2021.7 寝室 【未完整体验】 Lacuna 游戏 2021.9 寝室 7.5+0.5 音乐好听 小林家的龙女仆S 动漫 2021.10 寝室 9.5+1 小林用的python，托儿他爸说python是龙族的语言：懂了，python天下第一！（ 瑞克和莫蒂（第五季） 剧集 2021.10 寝室 10+1 越来越费脑子了 空洞骑士 游戏 2021.10 工位+寝室 【未完整体验】 前半部分是真滴劝退，但是City of Tears还是让我有信心打下去的 DELTARUNE (第二章) 游戏 2021.10 寝室 【未完整体验】 中文估计已经有了，但是ddl人估计要鸽到明年玩了（） 影子工厂 游戏 2021.10 寝室 9+1 瞰哥，我的瞰哥（The Great Wild Unknow 循环中） 失控玩家 电影 2021.10 电影院 7.5 沙丘 电影 2021.10 寝室 8 有空去看看原著 双城之战 剧集 2021.11 寝室 9+1 看之前没想到这么帅 Inscryption 邪恶冥刻 游戏 2021.11 寝室 10+1 看看人家怎么做卡牌游戏的！ 光明记忆：无限 游戏 2021.11 寝室 7 JOJO的奇妙冒险：石之海 动漫 2021.12 寝室 9+1","tags":["总结","评分表"]},{"title":"使用Spring和JPA构建REST规范的后端API项目","path":"/2021/09/19/使用Spring和JPA构建REST规范的后端API项目/","content":"关于如何使用Spring和JPA的入门教程 注意：该文档最初创建于Sep 19, 2021，可能存在已过时的内容，仅供参考 建立Spring项目 使用Spring Initializr（Spring提供的依赖包生成网站） Dependencies中加入以下依赖 Spring Web Spring Data JPA 一个数据库驱动（使用你喜欢的数据库驱动，比如MySQL Driver） 设置好以后，点击Generate Project，就可以下载到对应的zip格式的空项目包 在IDE中可以使用Maven管理项目包（建立Maven项目），只要把对应的包版本按照xml的格式写到项目根目录的pom.xml中就可以了 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; ...... &lt;dependencies&gt; &lt;dependency&gt; *添加你的依赖包* &lt;/dependency&gt; 配置程序主入口 一般来说，主入口文件都命名为XXXAplication.java，格式如下 12345678910import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class YourApplication &#123; public static void main(String... args) &#123; SpringApplication.run(YourApplication.class, args); &#125;&#125; 其中@SpringBootApplication标识服务端的入口类，SpringApplication.run(YourApplication.class, args)使用SpringBoot启动服务，一切配置都由其自动完成。 使用JPA链接模型类和数据库 JPA使用ORM（对象关系映射）来方便我们用代码链接数据库，简单来说，我们可以用一个类对应数据库中的一张表，类中的成员对应表中的属性，当类成员的值变化时，让JPA自动同步数据表中的值。这样的类一般称为模型类（MVC中的Model）或实体类，可以存放一些基础数据（比如用户类User） 语法说明 导入命名空间： 123import javax.persistence.Entity;import javax.persistence.GeneratedValue;import javax.persistence.Id; 1234@Entityclass User&#123;\t...&#125; @Entity 注解，写在类前面，声明JPA托管该类 123@Id @GeneratedValueprivate int id; @Id 和 @GeneratedValue：注解，写在类成员变量前面，声明该变量在数据表中所对应的属性是主关键字（PK） 其中@GeneratedValue注解向JPA声明主关键字的生成策略 123456@Id @GeneratedValueprivate long id;private String email; private String username; 对于非主关键字的属性，JPA会自动以类成员变量的命名，生成对应的属性名，无需再加入注解。 12345User() &#123;&#125;User(String email,String username) &#123;\tthis.email = eamil;\tthis.username = username;&#125; 有参构造函数用来创建没有指定id（主关键字）的实例 配置数据库的加载 JPA对类数据的操作，都是通过自定义一个新的接口实现的。 建立一个新接口，继承JpaRepository接口： 12345import org.springframework.data.jpa.repository.JpaRepository;interface UserRepository extends JpaRepository&lt;User, long&gt; &#123;\t...&#125; 其中，泛型参数&lt;User, long&gt;分别指JPA所链接的类的类型和其主关键字的类型. 接口类中可以自定义增删改查函数，如： 1234// 返回一个对象User findByUsername(String username);// 根据pageable参数返回分页对象（一页的内容）Page&lt;User&gt; findAllBy(Pageable pageable); 其中，findByXXXXX函数，Spring可以根据“XXXXX”的命名，自动对应模型类中同名（除了主键，不区分大小写）的数据变量，帮助你实现查找函数。即程序员只需声明该函数，并写对参数和返回类型。 findAllBy返回一个分页类型的所有数据成员。 然后，建立一个配置类，告诉Spring Boot加载接口对应数据类： 12345678910111213import ...@Configuration public class Initialize &#123; @Bean CommandLineRunner init(UserRepository userRepository, ...) &#123; return args -&gt; &#123; userRepository.save(new User()...) &#125;; &#125; 其中： @Configuration标识类是用来配置Spring Boot的加载项的 @Bean标识对象是一个Spring Bean CommandLineRunner接口：实现该接口的所有Bean都会在启动Spirng Boot时被加载。其中传入的参数即为JpaRepository类型的自定义接口对象（可以有多个，对应多个数据表），通过这些对象，即可以在服务加载时初始化数据库中内容。 在返回的匿名函数内用JpaRepository对象对数据表进行初始化操作： 使用xxxRepository.findByXXXX()函数，查找数据表的条目（前提：现在接口类中定义对应的函数） 使用xxxRepository.save()函数，修改并保存对应repository对应的模型类的成员数据，同时JPA会同步更新数据库的内容 注意：如果要操作模型类对象，使用其getter和setter函数，如 12345User user = userRepository.findByUsername(&quot;wht&quot;)String _info = user.getInfo();userRepository.save(new User().setUsername(&quot;admin&quot;) .setInfo(_info)) 在模型类中，注意封装好各个成员变量的getter和setter函数 实现HTTP控制模块 建立一个新类，用来处理前端请求，为了让Spring识别这个控制类，在类前加入注解@RestController 加入这些注解的目的是为了标记类为一个Spirng的Bean，Spring Boot帮我们配置好了扫描器，其会在启动时扫描全部带有@Component注解的类，将其注册为 Spring Bean。只有这样，Spring才会在有网络请求进入时，通过RequestMapping去索引 注册过的组件类 的相应函数。 @Component根据类的功能不同，其又分为以下三个子注解类型： @Controller：控制层，标注返回前端的控制组件 @Service：业务逻辑层，标注中间层的控制组件 @Repository：DAO层，标注数据库访问的组件 而@RestController是@Controller的一个升级版，其相当于@Controller + @ResponseBody，可以支持返回json格式的数据给前端。在REST模式设计的服务上，应该使用@RestController 在控制类中，在函数前加入注解@RequestMapping(&#123;url地址&#125;)，实现路由的作用，即用 URL请求的地址 对应 控制类调用相应的函数 根据REST规范，请求的类型可以分为 GET/POST/PUT/DELETE，因此可以直接使用以下子注解： @GetMapping(&quot;url&quot;) @PostMapping(&quot;url&quot;) @PutMapping(&quot;url&quot;) @DeleteMapping(&quot;url&quot;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// EmployeeController.javaimport java.util.List;// 导入RequestMapping使用的命名空间import org.springframework.web.bind.annotation.DeleteMapping;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.PutMapping;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RestController;@RestControllerclass EmployeeController &#123; // 引用接口服务，用来实现各种功能，记得要使用final静态变量 // 比如这里使用定义的JpaRepository接口对象，用来操作数据库 private final EmployeeRepository repository; EmployeeController(EmployeeRepository repository) &#123; this.repository = repository; &#125; // 如果用户请求GET，地址为/employees，调用该函数 @GetMapping(&quot;/employees&quot;) List&lt;Employee&gt; all() &#123; return repository.findAll(); &#125; // 如果用户请求GET，地址为/employees，调用该函数 @PostMapping(&quot;/employees&quot;) // @RequestBody 用于读取发来的请求中包含的参数数据，注意保持类型一致 Employee newEmployee(@RequestBody Employee newEmployee) &#123; return repository.save(newEmployee); &#125; @GetMapping(&quot;/employees/&#123;id&#125;&quot;) // @PathVariable 用于读取url地址中包含的参数数据，用&#123;&#125;包起参数名 Employee one(@PathVariable Long id) &#123; return repository.findById(id).orElseThrow(() -&gt; new EmployeeNotFoundException(id)); &#125; // 如果用户请求PUT（修改），地址为/employees/&#123;id&#125;，调用该函数 @PutMapping(&quot;/employees/&#123;id&#125;&quot;) Employee replaceEmployee(@RequestBody Employee newEmployee, @PathVariable Long id) &#123; return repository.findById(id) .map(employee -&gt; &#123; employee.setName(newEmployee.getName()); employee.setRole(newEmployee.getRole()); return repository.save(employee); &#125;) .orElseGet(() -&gt; &#123; newEmployee.setId(id); return repository.save(newEmployee); &#125;); &#125; // 如果用户请求DELETE（删除），地址为/employees/&#123;id&#125;，调用该函数 /employees/&#123;id&#125; @DeleteMapping(&quot;/employees/&#123;id&#125;&quot;) void deleteEmployee(@PathVariable Long id) &#123; repository.deleteById(id); &#125;&#125; 异常处理 在我们的后端服务中，一旦出现异常，光进行Java本身的异常处理是不够的，因为这样前端收不到对应的异常消息，没有办法知道接下来要干什么（前端：你这不是摆烂吗这是），因此Spring还提供ExceptionHandler反射机制，当后端产生异常时返回错误信息到前端。 主要步骤 先创建自己的异常类，继承RuntimeException，定义一种异常，如： XXXXException.java 1234567// EmployeeNotFoundException.javaclass EmployeeNotFoundException extends RuntimeException &#123; EmployeeNotFoundException(Long id) &#123; super(&quot;Could not find employee &quot; + id); &#125;&#125; 注意，这个异常类只抛出后端的错误信息，我们还需要让Spring返回前端信息 所以，在该类下再创建一个 XXXXAdvice.java 123456789101112131415161718192021// EmployeeNotFoundAdvice.java// 导入命名空间import org.springframework.http.HttpStatus;import org.springframework.web.bind.annotation.ControllerAdvice;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.bind.annotation.ResponseStatus;@ControllerAdviceclass EmployeeNotFoundAdvice &#123; @ResponseBody // 反射自定义异常类 @ExceptionHandler(EmployeeNotFoundException.class) // 注解，说明返回时向前端回复错误状态码（HttpStatus.NOT_FOUND 即 404） @ResponseStatus(HttpStatus.NOT_FOUND) String employeeNotFoundHandler(EmployeeNotFoundException ex) &#123;\t// 返回的错误信息 return ex.getMessage(); &#125;&#125; 定义XXXXAdvice类时，使用@ControllerAdvice注解，并且在其处理函数（通常命名为XXXXHandler）前，使用@ResponseBody、@ExceptionHandler、@ResponseStatus，它们的作用分别是： 注解 作用 @ResponseBody 说明返回给前端的消息采用json格式 @ExceptionHandler 说明这是一个处理异常的消息（参数填写对应异常类的反射） @ResponseStatus 说明返回的HTTP状态码（可以使用HttpStatus枚举） DTO（Data Transfer Object）支持 添加Spring HATEOAS 在pom.xml中加入以下依赖部分代码即可 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-hateoas&lt;/artifactId&gt;&lt;/dependency&gt; HATEOAS是Spring中用来实现链接（Link）的工具，链接的存在使得客户端可以动态发现其所能执行的动作。 链接的作用 让我们来看一个例子。这是一个普通的json返回包： 12345&#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;Bilbo Baggins&quot;, &quot;role&quot;: &quot;burglar&quot;,&#125; 这是一个带Links的json返回包： 12345678910111213&#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;Bilbo Baggins&quot;, &quot;role&quot;: &quot;burglar&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees/1&quot; &#125;, &quot;employees&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125;&#125; 返回的链接 “_links”部分将告诉前端，用户当前的操作（或是所在的位置）可以跳转到哪些可能的地址，因此前端可以根据这些链接为用户提供更方便的操作。 返回带链接的数据包 注意，以下代码可以简化，写这些的目的是为了更好的理解链接，如果要速成，请跳转到简化生成链接的步骤一节 以下是一个向前端返回带链接的数据的例子： 12345678910@GetMapping(&quot;/employees/&#123;id&#125;&quot;)EntityModel&lt;Employee&gt; one(@PathVariable Long id) &#123; Employee employee = repository.findById(id).orElseThrow( () -&gt; new EmployeeNotFoundException(id)); return EntityModel.of(employee, linkTo(methodOn(EmployeeController.class).one(id)).withSelfRel(), linkTo(methodOn(EmployeeController.class).all()).withRel(&quot;employees&quot;));&#125; 这个Mapping函数和之前的相比，有下面这些不同的地方： 返回值类型由自定义的模型类Employee变为了EntityModel&lt;Employee&gt;。 泛型EntityModel&lt;T&gt;是Spring HATEOAS设计的一种容器，其中不仅能存储数据，还可以存储一系列链接。 当你返回值时，不再只返回一个 Employee employee的模型对象，而是使用 1EntityModel.of(employee,&#123;links&#125;) 返回一个EntityModel类型（包括employee对象和若干个链接） 使用linkTo()方法生成链接： 参数传入一个methodOn()函数，参数为你要链接的方法所在的类的反射，一般为控制类，即XXXXController.class，然后用.点出要连接到的类的方法，如： linkTo(methodOn(EmployeeController.class).one(id)).withSelfRel() 即会加入EmployeeController&#123;&#125;.one(id)方法所在Mapping的URL，即 &quot;localhost:8080/employees/&#123;id&#125;&quot; 可以理解为这是一种反向Mapping，即从调用的方法查找到对应的路由 linkTo()返回一个Link对象，可以用其.withSelfRel()和.withRel(Sting)来修饰其在返回包中的Rel标签 Rel 是 relation 的简写，用来说明链接自身和链接之间的关系 .withSelfRel()返回的链接的Rel标签为“self”：&#123;&#125;，这说明返回的是请求的链接本身。一般来说带链接的返回包都要返回自己，其目的大概和类里面为啥要有this这个对象一样。 .withRel(Sting)返回的链接带有一个用String参数修饰的标签，即“xxxx”：&#123;&#125;，这个标签有助于前端根据标签的名称所以链接。 Spring HATEOAS的各种容器（Model） EntityModel&lt;&gt; 用来容纳单个模型对象及其链接 CollectionModel&lt;&gt;用来容纳多个模型对象及其链接，一个Collection可以包含多个Entity，写为：CollectionModel&lt;EntityModel&lt;T&gt;&gt; 将多个实体模型打包成Collection时，最好将每个模型对象都包装好链接，再为整体的Collection（可以是List、Map或是Dict之类）包装链接，示例如下： 12345678910111213141516@GetMapping(&quot;/employees&quot;)CollectionModel&lt;EntityModel&lt;Employee&gt;&gt; all() &#123;// 这一行写了很多东西，但主要的步骤是：// find所有employee的对象 -&gt; 转换为java8流对象（stream）-&gt; 用map函数把每个// employee的对象换为EntityModel对象，打包链接 -&gt; 将所有单体对象用collect函数// 重新打包进列表\tList&lt;EntityModel&lt;Employee&gt;&gt; employees = repository.findAll().stream() .map(employee -&gt; EntityModel.of(employee, linkTo(methodOn(EmployeeController.class).one(employee.getId())).withSelfRel(), linkTo(methodOn(EmployeeController.class).all()).withRel(&quot;employees&quot;))) .collect(Collectors.toList());//返回CollectionModel对象，打包整体的链接 return CollectionModel.of(employees, linkTo(methodOn(EmployeeController.class).all()).withSelfRel());&#125; ​\t该代码最后返回的json包格式如下： 12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;_embedded&quot;: &#123; &quot;employeeList&quot;: [ &#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;Bilbo Baggins&quot;, &quot;role&quot;: &quot;burglar&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees/1&quot; &#125;, &quot;employees&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125; &#125;, &#123; &quot;id&quot;: 2, &quot;name&quot;: &quot;Frodo Baggins&quot;, &quot;role&quot;: &quot;thief&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees/2&quot; &#125;, &quot;employees&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125; &#125; ] &#125;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125;&#125; 这就是一个非常符合REST规范的资源包了。 PagedModel&lt;&gt;也用来容纳多个模型对象及其链接，并且支持分页，生成PageModel必须使用Page&lt;&gt;类型的泛型对象，在生成PageModel时提供一个Pageable 的分页对象，用来记录分页信息： 1234567891011// PageModel.of()的函数重载类型：// 不传入Link参数，所有对象自动返回空Linkpublic static &lt;T&gt; PagedModel&lt;T&gt; model = PageModel.of (Collection&lt;T&gt; content, @Nullable PagedModel.PageMetadata metadata);// 传入一组Link参数，每个Link与content一一对应public static &lt;T&gt; PagedModel&lt;T&gt; model = PageModel.of (Collection&lt;T&gt; content, @Nullable PagedModel.PageMetadata metadata, Link... links); // 传入一个Link迭代对象，每个Link与content一一对应public static &lt;T&gt; PagedModel&lt;T&gt; model = PageModel.of (Collection&lt;T&gt; content, @Nullable PagedModel.PageMetadata metadata, Iterable&lt;Link&gt;); 带有分页信息的返回包格式如下，可以看到除了CollectionModel的信息外，还另外增加一个”Page“字段，存储分页信息： 123456789101112131415161718192021222324252627282930313233&#123; &quot;_embedded&quot;: &#123; &quot;userDtoList&quot;: [ &#123; &quot;email&quot;: &quot;admin@frogsoft.com&quot;, &quot;username&quot;: &quot;admin&quot;, &quot;roles&quot;: [ &quot;ROLE_USER&quot;, &quot;ROLE_ADMIN&quot; ], &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://127.0.0.1:8080/v1/users/admin&quot; &#125;, &quot;allUsers&quot;: &#123; &quot;href&quot;: &quot;http://127.0.0.1:8080/v1/users/?page=0&amp;size=10&quot; &#125; &#125; &#125; ] &#125;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://127.0.0.1:8080/v1/users?page=0&amp;size=10&quot; &#125; &#125;, &quot;page&quot;: &#123; &quot;size&quot;: 10, &quot;totalElements&quot;: 1, &quot;totalPages&quot;: 1, &quot;number&quot;: 0 &#125;&#125; 可以使用Spring提供的PagedResourcesAssembler类，简化PagedModel&lt;&gt;的生成， 在阅读以下代码之前，先阅读下一节关于RepresentationModelAssembler接口的介绍。 12345678910111213141516// 使用单个EntityModel的RepresentationModelAssembler接口生成PageModel的函数。public PagedModel&lt;EntityModel&lt;T&gt;&gt; getPageModelT(Pageable pageable) &#123; // 获取实体类型T对应的PagedResourcesAssembler类型 private final PagedResourcesAssembler&lt;T&gt; pagedResourcesAssembler; // 获取实体类型T的RepresentationModelAssembler（自行实现） private final XXXXModelAssembler tModelAssembler; // 将多个实体类型T打包为Page&lt;T&gt;类型（可以使用前面提到的JpaRepository中的findAllBy()实现） Page&lt;T&gt; contents = /* find the contents and packed in Page*/ // 调用.toModel返回分页模型，参数分别为打包的Page&lt;T&gt;类型 和 单个T类型对应的RepresentationModelAssembler对象 return pagedResourcesAssembler.toModel(contents, tModelAssembler); &#125; 简化生成链接的步骤 为了减少代码复用，不要在每次创建link时都重复写代码，我们可以写一个函数，将模型对象（比如 Employee）转换为对应的EntityModel&lt;T&gt;对象。这样当我们需要创建link时，只需简单的调用这个方法。 幸运的是，Spring HATEOAS帮你提供了RepresentationModelAssembler接口，通过这个接口，你可以快速创建转换类，而不用自己搭建框架。 实现RepresentationModelAssembler接口的方法如下： 123456789101112131415161718//EmployeeModelAssembler.javaimport static org.springframework.hateoas.server.mvc.WebMvcLinkBuilder.*;import org.springframework.hateoas.EntityModel;import org.springframework.hateoas.server.RepresentationModelAssembler;import org.springframework.stereotype.Component;@Componentclass EmployeeModelAssembler implements RepresentationModelAssembler&lt;Employee, EntityModel&lt;Employee&gt;&gt; &#123; @Override public EntityModel&lt;Employee&gt; toModel(Employee employee) &#123; return EntityModel.of(employee, linkTo(methodOn(EmployeeController.class).one(employee.getId())).withSelfRel(), linkTo(methodOn(EmployeeController.class).all()).withRel(&quot;employees&quot;)); &#125;&#125; 为你的接口实现类起一个 XXXModelAssembler的名字；加入Spring的注入注解@Component；实现接口的泛型参数&lt;Employee, EntityModel&lt;Employee&gt;&gt;表明其是将模型对象XXX转换为EntityModel&lt;XXX&gt; 该接口只需要实现（重载）一个toModel函数，其内容和之前直接在控制器类的函数中创建链接的代码一模一样。 使用该接口对象的方法和repository接口对象的方法类似，只需在控制器中声明其静态引用： 1private final EmployeeModelAssembler assembler; 别忘了在有参构造函数中也为其添加参数，然后只要在返回函数中调用toModel函数即可： 1234567@GetMapping(&quot;/employees/&#123;id&#125;&quot;)EntityModel&lt;Employee&gt; one(@PathVariable Long id) &#123; ... return assembler.toModel(employee);&#125; 之前打包Collection的代码也可以简化为： 123456789@GetMapping(&quot;/employees&quot;)CollectionModel&lt;EntityModel&lt;Employee&gt;&gt; all() &#123; List&lt;EntityModel&lt;Employee&gt;&gt; employees = repository.findAll().stream() .map(assembler::toModel) .collect(Collectors.toList()); return CollectionModel.of(employees, linkTo(methodOn(EmployeeController.class).all()).withSelfRel());&#125; 代码简洁了很多呢！ 使用ResponseEntity返回数据 另一个让你的代码符合REST规范的要点是永远返回合适的Respone 之前返回的json包中的HTTP状态码需要我们自行写入和处理，ResponseEntity则帮我们解决了这个问题，同时支持多种格式的返回包数据。 更新上面的POST函数，以便使用ResponseEntity类来处理返回值： 123456789@PostMapping(&quot;/employees&quot;)ResponseEntity&lt;?&gt; newEmployee(@RequestBody Employee newEmployee) &#123; EntityModel&lt;Employee&gt; entityModel = assembler.toModel(repository.save(newEmployee)); return ResponseEntity // .created(entityModel.getRequiredLink(IanaLinkRelations.SELF).toUri()) // .body(entityModel);&#125; 和不使用ResponseEntity相比，返回值类型变为ResponseEntity&lt;EntityModel&lt;Employee&gt;&gt;，也就是说，要在EntityModel外层，再包装一层ResponseEntity类型的泛型。在不至于混淆的情况下，可以使用ResponseEntity&lt;?&gt; 自动推导类型。 使用ResponseEntity.created()会返回HTTP 201 状态码，表示创建Employee的操作成功。当返回这个内容时，前端通常希望能同时获得该操作所对应的URL地址（Location），以便进行下一步操作。由于我们已经在返回包里打包了Link，因此添加响应头非常方便，代码中.created()的参数：entityModel.getRequiredLink(IanaLinkRelations.SELF).toUri()即是将Link的Self部分写入返回包的头部，最后得到的返回数据如下： 1234567891011121314151617181920&lt; Location: http://localhost:8080/employees/3&lt; Content-Type: application/hal+json;charset=UTF-8&lt; Transfer-Encoding: chunked&lt; Date: Fri, 10 Aug 2018 19:44:43 GMT&lt;&#123; &quot;id&quot;: 3, &quot;firstName&quot;: &quot;Samwise&quot;, &quot;lastName&quot;: &quot;Gamgee&quot;, &quot;role&quot;: &quot;gardener&quot;, &quot;name&quot;: &quot;Samwise Gamgee&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees/3&quot; &#125;, &quot;employees&quot;: &#123; &quot;href&quot;: &quot;http://localhost:8080/employees&quot; &#125; &#125;&#125; ResponseEntity.created()只返回一个特定的状态（201）， 通常情况下，我们使用ResponseEntity.status()返回一个自定义状态码的数据包，状态码可以用HttpStatus枚举表示。 HTTP状态码 HttpStatus枚举 含义 100 HttpStatus.CONTINUE 继续 102 HttpStatus.PROCESSING 处理中 200 HttpStatus.OK 正常 201 HttpStatus.CREATED 已创建 202 HttpStatus.ACCEPTED 已接受 204 HttpStatus.NO_CONTENT 已无内容 302 HttpStatus.FOUND 已找到 400 HttpStatus.BAD_REQUEST 请求错误 401 HttpStatus.UNAUTHORIZED 未授权 403 HttpStatus.FORBIDDEN 禁止访问 404 HttpStatus.NOT_FOUND 无法找到 408 HttpStatus.REQUEST_TIMEOUT 请求超时 429 HttpStatus.TOO_MANY_REQUESTS 请求过多 500 HttpStatus.INTERNAL_SERVER_ERROR 内部服务错误 502 HttpStatus.BAD_GATEWAY 网关错误 503 HttpStatus.SERVICE_UNAVAILABLE 服务不可用 504 HttpStatus.GATEWAY_TIMEOUT 网关超时 1ResponseEntity.status(/* http状态码 */HttpStatus.XXX).body(model); 其他一些常用的状态，ResponseEntity也为我们封装好了以下常用函数： 123456// ok：返回状态码200ResponseEntity.ok().body(model);// badRequest：返回状态码400ResponseEntity.badRequest().body(model);// notFound：返回状态码404ResponseEntity.notFound().body(model); Controller、Service、Repository 三层结构 之前我们提到，Spring用来注册Bean的控制接口@Component根据名称不同又分为三种： @Controller：控制层，标注返回前端的控制组件 @Service：业务逻辑层，标注中间层的控制组件 @Repository：DAO层，标注数据库访问的组件 之所以这么设计，是由编写代码中分层规范决定的，通常Controller层最接近前端，Repository(DAO)层最接近后端（数据库），这样的结构最利于代码解耦化，也利于每个部分的代码更加简洁，接下来我们就来整理项目以适应三层结构。 @Controller控制层 在项目结构中建立一个Controller包，用来处理控制层。 控制层是最接近前端的部分，所以要适应可能随时改变的前端需求。因此最好做好版本控制，可以将Controller包命名为/Controller.v&#123;版本号&#125;。当需要修改需求时，可以保留旧版本，重新建立新版本的Controller包，以实现向后兼容。 控制层要实现的内容： 根据设计好的API，建立若干控制类：XXXXController，放置于/Controller/api包下。 若API需要接受前端请求的数据比较复杂，需要单独用一个数据类存储，可以建立对应的数据类XXXXRequest，放置于/Controller/request包下。 控制类中要做的事情： 路由处理：用@RequestMapping索引（这个模块的）根地址（可以写在class的定义前），然后用@Get/Post/Put/DeleteMapping索引子地址 权限处理：判断访问请求是否具有特殊的权限： token鉴权：从请求中获取username，与授权系统中的列表匹配 Roles判断：函数前加入注解@RolesAllowed(&quot;身份名&quot;)，自动拒绝没有对应身份权限的用户的访问，判断用户的身份在登陆时由鉴权系统实现 处理请求数据： 读请求数据：函数参数中配置@RequestParam和@PathVariable，获得请求或URL中的数据 读写数据：调用Service层的接口对象 注意，对于数据的具体处理要在Service层实现，Controller层只简单处理和传输参数，并读取结果（Entity对象） 返回状态码，用ResponseEntity.status()实现 @Service服务层 服务层，即作为一个承上启下的中间部分，用来处理数据、打包数据。 服务层的基本结构： 建立若干服务接口，在/service文件夹下，如果功能细分需要多个功能模块，也可以建立子文件夹 服务接口，以XXXXService命名，接口的实现类以XXXXServiceImpl命名，使用接口的目的是为了多个类访问同一个数据仓库时方便重载和复用。 服务实现类中要做的事情： 读写数据：调用Repository层的接口对象，处理数据 读数据：使用findBy系列方法（在Repository层定义） 写数据：使用save方法 打包数据：将要返回的数据通过RepresentationModelAssembler接口对象（这也属于持久层）打包为Entity对象，返回给控制层 @Repository持久层 持久层，用来和数据库进行直接连接。在这里我们使用JPA来建立持久层，数据库连接变得十分方便，但要满足DTO数据模型的要求，因此持久层分为Repository和Dto两个平行层 持久层的基本结构： 建立若干仓库接口，以XXXXRepository命名，接口要继承自JpaRepository，以实现数据库连接，接口无需实现（Spring金牌服务，帮您实现！） 建立Dto文件夹，其包含三个部分： model文件夹：建立Dto数据类，Dto数据类的命名在基本数据实体类后加Dto即可（如UserDto） Dto数据类的成员变量内容和基本数据类一模一样，但无需写与数据库连接的注解，数据的连通由mapper类搞定 mapper文件夹：对于每个Dto类，建立一个Mapper类，以XXXXMapper命名。 mapper类实现一个函数，将基本数据实体类（如User）转化为Dto数据类（如UserDto） mapper函数的实现方法：new一个Dto对象，然后将使用setter函数配置所有成员即可。之所以在mapper中new对象，也是处于方便解耦和类嵌套之类的目的 assemler文件夹，对于每个Dto类，建立一个Assembler类，以XXXXModelAssembler命名。该类实现RepresentationModelAssembler接口。 注意，这里实现RepresentationModelAssembler接口时，传入的实体对象要是已经打包好的Dto类对象，不然我费这么大劲打包Dto干啥（感觉在说废话） Assembler类即主要实现toModel函数，实现将Dto数据打包为Entity实体对象，这样，服务层只需调用对应的接口实现类，即可以获得符合REST规范的返回数据包 对于基本数据实体类，建立一个\\Model文件夹存储，使用@Entity实现类与数据库的ORM连接。这些即为持久层所需处理的部分。","tags":["java","Spring"]},{"title":"关于","path":"/about/index.html","content":"About ME 关于我： AI Robotics LLM相关方向 EHD在读 写过小说 第二名完美主义者 独立游戏爱好者 混剪爱好者 （非常业余的）铁道迷 好吧，其实我只是喜欢线路图和标志设计 音游人 主持过两次COC跑团 已经发表的论文 …… …… …… 什么？！居然一篇论文也没有？！！ 天哪我怎么这么菜 一些个人主页 Github：#Nick-bit233 bilibili：@赛因斯没有坦 Steam：@Nick_bit 做过的一些有意思的东西 Link-Up Hero一个基于连连看+DBG Rougelike的Demo游戏Dreaming Butterfly《如梦之梦》水彩漫画2/3D视觉解谜游戏，2021网易雷火MiniGame夏令营 第一名Puzzle maze一个独立游戏《VVVVVV》的自制关卡 音游成绩 我正在研究如何实时的更新这个东西，如果你有建议，欢迎写在评论区~ Game Ranking Last Update Date Arcaea 11.78 23/12/04 Lanota 15.80 23/12/04 Chunithm（CN） 14.68 23/12/01 maimaiDX（CN） 11746 23/12/01 Phigros 14.61 23/12/04 osu! Standard 803pp 22/08 有用的网站收藏夹 实用工具 Complier Explorer在线编译器Vocal RemoverAI人声音轨分离图片背景消除在线抠图，去除背景B站封面提取抓取B站视频的封面Wolfram Alpha数学求解器和科学问题搜索Latex Live在线Latex编辑和图像生成器Tiki-Toki时间轴展示生成 寻找资源 Pexels免费摄影图片素材Panzoid免费的片头展示视频模板和生成器清华Tuna镜像站开源软件镜像Audio Library免费无版权音乐Story Blocks免费音效下载 不正经的部分 616.sb纯净的音游下载站BEMANICNBEMANICN 论坛BestdoriBanG Dream! 资源库和社区"},{"title":"友链","path":"/friends/index.html","content":"See also my friends! Geeks RiddNOAM CHI Artists Waldo Yang"},{"title":"笔记分类页","path":"/notes/index.html","content":"正在施工中…"}]